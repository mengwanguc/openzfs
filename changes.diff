diff --git a/.gitignore b/.gitignore
index 056bbb8f0..a120eac45 100644
--- a/.gitignore
+++ b/.gitignore
@@ -68,3 +68,5 @@ venv
 *.so
 *.so.debug
 *.so.full
+
+libzfs-test
\ No newline at end of file
diff --git a/README-mlec.md b/README-mlec.md
new file mode 100644
index 000000000..984745bcd
--- /dev/null
+++ b/README-mlec.md
@@ -0,0 +1,69 @@
+## Introduction
+There are two main folder to look at
+- `module`: this contains ZFS internal logics for MLEC repairs as well as error detections
+- `cmd|lib`: contains all the changes for ioctl calls from HDFS
+
+I will split the rest of the README according to the two components
+
+## Building
+To build this repo, we just need to run the scripts `zfs_reinstall.sh`. This script will
+1. Tear down all the existing ZFS pools in your system
+2. Remove the ZFS/SPL kernel module
+3. Remove the ZFS libs files
+4. Build ZFS with userspace and kernel debug symbols
+5. Enable ZFS/SPL kernel module
+6. Create a test RAID-Z pool with 2+1 config along with a test file
+Note that if step **1** says that ZFS pool is busy and cannot be destroyed, you would need to reboot your VM. That most likely means your currently running ZFS caused a kernel panic and thus cannot be properly unmounted.
+
+## `ioctl` calls
+We use `ioctl` for HDFS to
+- `mlec_easy_scrub`: get all the failure information from ZFS through ZAP (ZFS attribute processor)
+- `mlec_failed_chunks`: get all failed chunks in the zpool
+- `mlec_receive_data`: receive binary array from HDFS, and conduct repair
+
+### `mlec_easy_scrub` call chains and contents
+
+This ioctl call checks for the child vdev(physical disks) of a top-level vdev(RAIDZ/CP), and return an array of int that represent their current status. A non-zero number will indicate error.
+
+There are two **entry points** for this command, one from CMD (for debugging and impl), and one from `libzfs` C API
+- `cmd/zpool/zpool_main.c`: this is the main entry point for CMD easy scrub that can be triggered via `zpool easyscrub <poolname>` command
+- `lib/libzfs_core/libzfs_core.c`: this is the entry point for `libzfs` C API, and hence `libzfs-mlec-binding` Java binding
+
+The above two entry points are all handled within ZFS's ioctl handler class.
+- `module/zfs/zfs_ioctl.c`: this is where all the logics for `mlec_easy_scrub` lies, in function `zfs_ioc_pool_easy_scan`
+
+The main logic of `mlec_easy_scrub` is the loop over of the vdev children and the calling of `vdev_open`, which will return a non-zero code if the vdev cannot be opened.
+
+### `mlec_failed_chunks` call chains and contents
+
+If `mlec_easy_scrub` returns error code, we can then call `mlec_failed_chunks` to get the dnode number of the files that are impacted by the failed chunks.
+
+Same as `mlec_easy_scrub`, there are two entry points, one for CMD and one for `libzfs` C API. They are located in the same file as `mlec_easy_scrub`.
+
+Same as `mlec_easy_scrub`, the two entry points are handled in `module/zfs/zfs_ioctl.c` but in the function `zfs_ioctl_failed_chunks`
+
+The main logic of failed chunks is in `mlec_dump_objset`. Where we loop through all the dnodes in the zpool passed in from the entry points, and then return its attributes through `nvlist` that will be translated to `DnodeAttributes.java` in the Java binding project.
+
+Note: the `fsize` logic is still kind of buggy, due to SA handle setup is a very complicated process. I have currently commented it out, but we would need to enable this for the whole repair process to work correctly.
+
+### `mlec_receive_data` call chains and contents.
+
+This ioctl call takes a dnode number, and a binary array from the C API, and override the ARC buffer of the RAIDZ columns.
+
+Different from `mlec_easy_scrub`, there is no point in providing a CMD interface to a data-plane API. Therefore, the entry point for the call only resides in `lib/libzfs_core/libzfs_core.c`
+
+The main logic of `mlec_receive_data` is basically as follows
+1. We acquire the `dnode` struct from the input through ZAP
+2. We copy the input binary array into ZFS data structure through `abd_alloc_for_io` and `abd_copy_from_buf` 
+3. We then initiate a ZIO through `zio_root` and `zio_ioctl`, which will write the data through internal logics that I will talk about in the following sections.
+
+## Internal Logics
+Although most of the logics are done in the ioctl handler, we had to make a few changes to ZFS internal logics so that we can have special MLEC repair code paths.
+
+### MLEC specific repair data
+Normal ZIO requires setups that are complex and unnecessary for MLEC. Since for MLEC we are only interested in getting the received data directly into column ARC buffer, we developed a specialized ZIO pipeline.
+
+1. We first have added a new ZIO type in `include/sys/fs/zfs.h` called `ZIO_TYPE_MLEC_WRITE_DATA`
+2. We added in `module/zfs/zio.c`'s function `zio_ioctl` a special `if` case for `ZIO_TYPE_MLEC_WRITE_DATA`
+3. We added custom adb logic in `module/zfs/zio.c`'s function `zio_vdev_io_start` to handle ZIO that are MLEC repair
+4. We added custom logic in `module/zfs/vdev_raidz.c` to handle MLEC data physical write in `vdev_raidz_io_start`
diff --git a/README.md b/README.md
index 331889560..216dad045 100644
--- a/README.md
+++ b/README.md
@@ -1,35 +1,53 @@
-![img](https://openzfs.github.io/openzfs-docs/_static/img/logo/480px-Open-ZFS-Secondary-Logo-Colour-halfsize.png)
-
-OpenZFS is an advanced file system and volume manager which was originally
-developed for Solaris and is now maintained by the OpenZFS community.
-This repository contains the code for running OpenZFS on Linux and FreeBSD.
-
-[![codecov](https://codecov.io/gh/openzfs/zfs/branch/master/graph/badge.svg)](https://codecov.io/gh/openzfs/zfs)
-[![coverity](https://scan.coverity.com/projects/1973/badge.svg)](https://scan.coverity.com/projects/openzfs-zfs)
-
-# Official Resources
-
-  * [Documentation](https://openzfs.github.io/openzfs-docs/) - for using and developing this repo
-  * [ZoL Site](https://zfsonlinux.org) - Linux release info & links
-  * [Mailing lists](https://openzfs.github.io/openzfs-docs/Project%20and%20Community/Mailing%20Lists.html)
-  * [OpenZFS site](https://openzfs.org/) - for conference videos and info on other platforms (illumos, OSX, Windows, etc)
-
-# Installation
-
-Full documentation for installing OpenZFS on your favorite operating system can
-be found at the [Getting Started Page](https://openzfs.github.io/openzfs-docs/Getting%20Started/index.html).
-
-# Contribute & Develop
-
-We have a separate document with [contribution guidelines](./.github/CONTRIBUTING.md).
-
-We have a [Code of Conduct](./CODE_OF_CONDUCT.md).
-
-# Release
-
-OpenZFS is released under a CDDL license.
-For more details see the NOTICE, LICENSE and COPYRIGHT files; `UCRL-CODE-235197`
-
-# Supported Kernels
-  * The `META` file contains the officially recognized supported Linux kernel versions.
-  * Supported FreeBSD versions are any supported branches and releases starting from 12.2-RELEASE.
+# About this repo
+
+This repository is modified based on the release version of OpenZFS 2.1. This is part of MLEC (Multi-level Erasure Coding) project that aims to implement MLEC in a real system. In the end, we hope to build HDFS on top of OpenZFS, with MLEC implemented.
+
+# Initial Installation
+
+First, reserve a node from Chameleon following this [guide](https://chameleoncloud.readthedocs.io/en/latest/technical/reservations.html), and launch an [instances](https://chameleoncloud.readthedocs.io/en/latest/technical/baremetal.html). The installation generally works for Ubuntu 20.04 and 22.04.
+
+Then, clone the repository, and switch to the branch with MLEC implementation. 
+```
+git clone https://github.com/zhynwng/zfs-MLEC.git
+git checkout zfs-2.1-myraid
+```
+
+Run the initialization script, which will install all necessary packages and set up the environment. Press Y for all options during package installation.
+```
+cd zfs-MLEC
+./zfs_init.sh
+```
+
+Then reboot the machine. Make sure all changes are saved before you reboot. 
+```
+sudo reboot
+```
+
+After the reboot, you can run the full installation script. This script will set up system configuration to build ZFS from source.
+```
+./zfs_full_install.sh
+```
+
+You can see whether zfs is successfully installed by trying the command "zfs list". This the expected result:
+```
+cc@node2:~/zfs-MLEC$ zfs list
+no datasets available
+```
+
+# Later install and uninstall
+
+After the first installation, you can using the more light-weight installation script. This script omits some of the steps to make the build process faster
+```
+./zfs_install.sh
+```
+
+You can remove ZFS from the system by the following script
+```
+zfs_uninstall.sh
+```
+
+There are also testing scripts to test the general functionality of ZFS erasure coding systems (one is for general repair test, the other is to test the new easyscrub method). Feel free to explore and change these two scripts
+```
+./zfs_test.sh
+./scrub_tesh.sh
+```
\ No newline at end of file
diff --git a/README_2.md b/README_2.md
new file mode 100644
index 000000000..331889560
--- /dev/null
+++ b/README_2.md
@@ -0,0 +1,35 @@
+![img](https://openzfs.github.io/openzfs-docs/_static/img/logo/480px-Open-ZFS-Secondary-Logo-Colour-halfsize.png)
+
+OpenZFS is an advanced file system and volume manager which was originally
+developed for Solaris and is now maintained by the OpenZFS community.
+This repository contains the code for running OpenZFS on Linux and FreeBSD.
+
+[![codecov](https://codecov.io/gh/openzfs/zfs/branch/master/graph/badge.svg)](https://codecov.io/gh/openzfs/zfs)
+[![coverity](https://scan.coverity.com/projects/1973/badge.svg)](https://scan.coverity.com/projects/openzfs-zfs)
+
+# Official Resources
+
+  * [Documentation](https://openzfs.github.io/openzfs-docs/) - for using and developing this repo
+  * [ZoL Site](https://zfsonlinux.org) - Linux release info & links
+  * [Mailing lists](https://openzfs.github.io/openzfs-docs/Project%20and%20Community/Mailing%20Lists.html)
+  * [OpenZFS site](https://openzfs.org/) - for conference videos and info on other platforms (illumos, OSX, Windows, etc)
+
+# Installation
+
+Full documentation for installing OpenZFS on your favorite operating system can
+be found at the [Getting Started Page](https://openzfs.github.io/openzfs-docs/Getting%20Started/index.html).
+
+# Contribute & Develop
+
+We have a separate document with [contribution guidelines](./.github/CONTRIBUTING.md).
+
+We have a [Code of Conduct](./CODE_OF_CONDUCT.md).
+
+# Release
+
+OpenZFS is released under a CDDL license.
+For more details see the NOTICE, LICENSE and COPYRIGHT files; `UCRL-CODE-235197`
+
+# Supported Kernels
+  * The `META` file contains the officially recognized supported Linux kernel versions.
+  * Supported FreeBSD versions are any supported branches and releases starting from 12.2-RELEASE.
diff --git a/TEST b/TEST
deleted file mode 100644
index 376d6eb69..000000000
--- a/TEST
+++ /dev/null
@@ -1,50 +0,0 @@
-#!/bin/sh
-
-### prepare
-#TEST_PREPARE_WATCHDOG="yes"
-#TEST_PREPARE_SHARES="yes"
-
-### ztest
-#TEST_ZTEST_SKIP="yes"
-#TEST_ZTEST_TIMEOUT=1800
-#TEST_ZTEST_DIR="/var/tmp/"
-#TEST_ZTEST_OPTIONS="-V"
-#TEST_ZTEST_CORE_DIR="/mnt/zloop"
-
-### zimport
-#TEST_ZIMPORT_SKIP="yes"
-#TEST_ZIMPORT_DIR="/var/tmp/zimport"
-#TEST_ZIMPORT_VERSIONS="master installed"
-#TEST_ZIMPORT_POOLS="zol-0.6.1 zol-0.6.2 master installed"
-#TEST_ZIMPORT_OPTIONS="-c"
-
-### xfstests
-#TEST_XFSTESTS_SKIP="yes"
-#TEST_XFSTESTS_URL="https://github.com/behlendorf/xfstests/archive/"
-#TEST_XFSTESTS_VER="zfs.tar.gz"
-#TEST_XFSTESTS_POOL="tank"
-#TEST_XFSTESTS_FS="xfstests"
-#TEST_XFSTESTS_VDEV="/var/tmp/vdev"
-#TEST_XFSTESTS_OPTIONS=""
-
-### zfs-tests.sh
-#TEST_ZFSTESTS_SKIP="yes"
-#TEST_ZFSTESTS_DIR="/mnt/"
-#TEST_ZFSTESTS_DISKS="vdb vdc vdd"
-#TEST_ZFSTESTS_DISKSIZE="8G"
-#TEST_ZFSTESTS_ITERS="1"
-#TEST_ZFSTESTS_OPTIONS="-vx"
-#TEST_ZFSTESTS_RUNFILE="linux.run"
-#TEST_ZFSTESTS_TAGS="functional"
-
-### zfsstress
-#TEST_ZFSSTRESS_SKIP="yes"
-#TEST_ZFSSTRESS_URL="https://github.com/nedbass/zfsstress/archive/"
-#TEST_ZFSSTRESS_VER="master.tar.gz"
-#TEST_ZFSSTRESS_RUNTIME=300
-#TEST_ZFSSTRESS_POOL="tank"
-#TEST_ZFSSTRESS_FS="fish"
-#TEST_ZFSSTRESS_FSOPT="-o overlay=on"
-#TEST_ZFSSTRESS_VDEV="/var/tmp/vdev"
-#TEST_ZFSSTRESS_DIR="/$TEST_ZFSSTRESS_POOL/$TEST_ZFSSTRESS_FS"
-#TEST_ZFSSTRESS_OPTIONS=""
diff --git a/bw_limit.sh b/bw_limit.sh
new file mode 100755
index 000000000..da82dbf3e
--- /dev/null
+++ b/bw_limit.sh
@@ -0,0 +1,8 @@
+echo cfq > /sys/module/zfs/parameters/zfs_vdev_scheduler
+echo "7:21 10485760" > /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device
+echo "7:22 10485760" > /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device
+echo "7:23 10485760" > /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device
+echo "7:24 10485760" > /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device
+dd if=/dev/zero of=/rampool/file1 bs=128M count=1
+dd if=/dev/zero of=/rampool/file1 bs=128M count=1
+dd if=/dev/zero of=/rampool/file1 bs=128M count=1
\ No newline at end of file
diff --git a/cmd/zdb/zdb.c b/cmd/zdb/zdb.c
index bcd520de3..0a77cbb30 100644
--- a/cmd/zdb/zdb.c
+++ b/cmd/zdb/zdb.c
@@ -3378,13 +3378,6 @@ dump_object(objset_t *os, uint64_t object, int verbosity,
 	CTASSERT(sizeof (asize) >= NN_NUMBUF_SZ);
 	CTASSERT(sizeof (bonus_size) >= NN_NUMBUF_SZ);
 
-	if (*print_header) {
-		(void) printf("\n%10s  %3s  %5s  %5s  %5s  %6s  %5s  %6s  %s\n",
-		    "Object", "lvl", "iblk", "dblk", "dsize", "dnsize",
-		    "lsize", "%full", "type");
-		*print_header = 0;
-	}
-
 	if (object == 0) {
 		dn = DMU_META_DNODE(os);
 		dmu_object_info_from_dnode(dn, &doi);
@@ -3413,6 +3406,15 @@ dump_object(objset_t *os, uint64_t object, int verbosity,
 			bsize = db->db_size;
 			dn = DB_DNODE((dmu_buf_impl_t *)db);
 		}
+
+		// printf("dump_object() <objset_id:dnode_id> <%ld:%ld>\n", dmu_objset_id(os), dn->dn_object);
+	}
+
+	if (*print_header) {
+		(void) printf("\n%10s  %3s  %5s  %5s  %5s  %6s  %5s  %6s  %s\n",
+		    "Object", "lvl", "iblk", "dblk", "dsize", "dnsize",
+		    "lsize", "%full", "type");
+		*print_header = 0;
 	}
 
 	/*
@@ -4551,6 +4553,7 @@ dump_path_impl(objset_t *os, uint64_t obj, char *name, uint64_t *retobj)
 	err = zap_lookup(os, obj, name, 8, 1, &child_obj);
 
 	(void) strlcat(curpath, name, sizeof (curpath));
+	// printf("Current dump path %s, child obj %ld\n", curpath, child_obj);
 
 	if (err != 0) {
 		(void) fprintf(stderr, "failed to lookup %s: %s\n",
@@ -4560,12 +4563,14 @@ dump_path_impl(objset_t *os, uint64_t obj, char *name, uint64_t *retobj)
 
 	child_obj = ZFS_DIRENT_OBJ(child_obj);
 	err = sa_buf_hold(os, child_obj, FTAG, &db);
+
 	if (err != 0) {
 		(void) fprintf(stderr,
 		    "failed to get SA dbuf for obj %llu: %s\n",
 		    (u_longlong_t)child_obj, strerror(err));
 		return (EINVAL);
 	}
+
 	dmu_object_info_from_db(db, &doi);
 	sa_buf_rele(db, FTAG);
 
@@ -8778,6 +8783,7 @@ retry_lookup:
 		flagbits['A'] = ZOR_FLAG_ALL_TYPES;
 
 		if (argc > 0 && dump_opt['d']) {
+			printf("Dump option [d]");
 			zopt_object_args = argc;
 			zopt_object_ranges = calloc(zopt_object_args,
 			    sizeof (zopt_object_range_t));
diff --git a/cmd/zpool/zpool_main.c b/cmd/zpool/zpool_main.c
index a06af9aec..7c1ea7351 100644
--- a/cmd/zpool/zpool_main.c
+++ b/cmd/zpool/zpool_main.c
@@ -106,6 +106,9 @@ static int zpool_do_split(int, char **);
 
 static int zpool_do_initialize(int, char **);
 static int zpool_do_scrub(int, char **);
+static int zpool_do_easy_scrub(int, char **);
+static int zpool_do_get_all_dnode(int, char **);
+static int zpool_do_get_failed_chunks(int, char**);
 static int zpool_do_resilver(int, char **);
 static int zpool_do_trim(int, char **);
 
@@ -168,6 +171,7 @@ typedef enum {
 	HELP_REMOVE,
 	HELP_INITIALIZE,
 	HELP_SCRUB,
+	HELP_EASYSCRUB,
 	HELP_RESILVER,
 	HELP_TRIM,
 	HELP_STATUS,
@@ -269,6 +273,11 @@ typedef struct zpool_command {
 	zpool_help_t	usage;
 } zpool_command_t;
 
+typedef struct failed_chunks_cbdata {
+	int64_t objset_id;
+	int64_t object_id;
+} failed_chunks_cbdata_t;
+
 /*
  * Master command table.  Each ZFS command has a name, associated function, and
  * usage message.  The usage messages need to be internationalized, so we have
@@ -308,6 +317,9 @@ static zpool_command_t command_table[] = {
 	{ "initialize",	zpool_do_initialize,	HELP_INITIALIZE		},
 	{ "resilver",	zpool_do_resilver,	HELP_RESILVER		},
 	{ "scrub",	zpool_do_scrub,		HELP_SCRUB		},
+	{ "easyscrub", zpool_do_easy_scrub, HELP_EASYSCRUB},
+	{ "getalldnode", zpool_do_get_all_dnode, HELP_EASYSCRUB},
+	{ "getfailedchunks", zpool_do_get_failed_chunks, HELP_EASYSCRUB},
 	{ "trim",	zpool_do_trim,		HELP_TRIM		},
 	{ NULL },
 	{ "import",	zpool_do_import,	HELP_IMPORT		},
@@ -396,6 +408,8 @@ get_usage(zpool_help_t idx)
 		    "[<device> ...]\n"));
 	case HELP_SCRUB:
 		return (gettext("\tscrub [-s | -p] [-w] <pool> ...\n"));
+	case HELP_EASYSCRUB:
+		return (gettext("\teasyscrub <pool> ...\n"));
 	case HELP_RESILVER:
 		return (gettext("\tresilver <pool> ...\n"));
 	case HELP_TRIM:
@@ -7282,6 +7296,25 @@ scrub_callback(zpool_handle_t *zhp, void *data)
 	return (err != 0);
 }
 
+static int 
+easy_scrub_callback(zpool_handle_t *zhp, void *data) 
+{
+	return zpool_easy_scan(zhp);
+}
+
+static int 
+get_all_dnode_callback(zpool_handle_t *zhp, void *data) 
+{
+	return zpool_get_all_dnode(zhp);
+}
+
+static int
+get_failed_chunks_callback(zpool_handle_t *zhp, void *data) 
+{
+	failed_chunks_cbdata_t *tup = data;
+	return zpool_get_failed_chunks(zhp, tup->objset_id, tup->object_id);
+}
+
 static int
 wait_callback(zpool_handle_t *zhp, void *data)
 {
@@ -7360,6 +7393,61 @@ zpool_do_scrub(int argc, char **argv)
 	return (error);
 }
 
+int
+zpool_do_get_all_dnode(int argc, char **argv) {
+	scrub_cbdata_t cb;
+	int error;
+
+	cb.cb_type = POOL_SCAN_SCRUB;
+	cb.cb_scrub_cmd = POOL_SCRUB_NORMAL;
+	
+	argc -= optind;
+	argv += optind;
+
+	error = for_each_pool(argc, argv, B_TRUE, NULL, B_FALSE,
+	    get_all_dnode_callback, &cb);
+	
+	return (error);
+}
+
+int
+zpool_do_get_failed_chunks(int argc, char **argv) {
+	int error;
+
+	failed_chunks_cbdata_t cb;
+	// cb.objset_id = atoi(argv[2]);
+	// cb.object_id = atoi(argv[3]);
+
+	// // Hijack, so that for each does not recognize the two following arguments
+	// argc -= 2;
+
+	argc -= optind;
+	argv += optind;
+
+	error = for_each_pool(argc, argv, B_TRUE, NULL, B_FALSE,
+	    get_failed_chunks_callback, &cb);
+	
+	return (error);
+}
+
+/* Easy scrub for MLEC research */
+int
+zpool_do_easy_scrub(int argc, char **argv)
+{
+	scrub_cbdata_t cb;
+	int error;
+
+	cb.cb_type = POOL_SCAN_SCRUB;
+	cb.cb_scrub_cmd = POOL_SCRUB_NORMAL;
+	
+	argc -= optind;
+	argv += optind;
+
+	error = for_each_pool(argc, argv, B_TRUE, NULL, B_FALSE,
+	    easy_scrub_callback, &cb);
+	
+	return (error);
+}
 /*
  * zpool resilver <pool> ...
  *
diff --git a/cmd/zpool/zpool_vdev.c b/cmd/zpool/zpool_vdev.c
index 3d83da641..f178c6eed 100644
--- a/cmd/zpool/zpool_vdev.c
+++ b/cmd/zpool/zpool_vdev.c
@@ -1169,8 +1169,14 @@ get_parity(const char *type)
 	long parity = 0;
 	const char *p;
 
-	if (strncmp(type, VDEV_TYPE_RAIDZ, strlen(VDEV_TYPE_RAIDZ)) == 0) {
-		p = type + strlen(VDEV_TYPE_RAIDZ);
+	if (strncmp(type, VDEV_TYPE_RAIDZ, strlen(VDEV_TYPE_RAIDZ)) == 0 ||
+		strncmp(type, VDEV_TYPE_MY_RAIDZ, strlen(VDEV_TYPE_MY_RAIDZ)) == 0) {
+		
+		if (strncmp(type, VDEV_TYPE_RAIDZ, strlen(VDEV_TYPE_RAIDZ)) == 0) {
+			p = type + strlen(VDEV_TYPE_RAIDZ);
+		} else {
+			p = type + strlen(VDEV_TYPE_MY_RAIDZ);
+		}
 
 		if (*p == '\0') {
 			/* when unspecified default to single parity */
@@ -1225,8 +1231,11 @@ is_grouping(const char *type, int *mindev, int *maxdev)
 	int nparity;
 
 	if (strncmp(type, VDEV_TYPE_RAIDZ, strlen(VDEV_TYPE_RAIDZ)) == 0 ||
-	    strncmp(type, VDEV_TYPE_DRAID, strlen(VDEV_TYPE_DRAID)) == 0) {
+	    strncmp(type, VDEV_TYPE_DRAID, strlen(VDEV_TYPE_DRAID)) == 0 ||
+		strncmp(type, VDEV_TYPE_MY_RAIDZ, strlen(VDEV_TYPE_MY_RAIDZ)) == 0) {
 		nparity = get_parity(type);
+
+		printf("parity level %d\n", nparity);
 		if (nparity == 0)
 			return (NULL);
 		if (mindev != NULL)
@@ -1237,6 +1246,9 @@ is_grouping(const char *type, int *mindev, int *maxdev)
 		if (strncmp(type, VDEV_TYPE_RAIDZ,
 		    strlen(VDEV_TYPE_RAIDZ)) == 0) {
 			return (VDEV_TYPE_RAIDZ);
+		} else if (strncmp(type, VDEV_TYPE_MY_RAIDZ,
+		    strlen(VDEV_TYPE_MY_RAIDZ)) == 0) {
+			return (VDEV_TYPE_MY_RAIDZ);
 		} else {
 			return (VDEV_TYPE_DRAID);
 		}
@@ -1251,6 +1263,12 @@ is_grouping(const char *type, int *mindev, int *maxdev)
 		return (VDEV_TYPE_MIRROR);
 	}
 
+	if (strcmp(type, "my_mirror") == 0 ) {
+		if (mindev != NULL)
+			*mindev = 2;
+		return (VDEV_TYPE_MY_MIRROR);
+	}
+	
 	if (strcmp(type, "spare") == 0) {
 		if (mindev != NULL)
 			*mindev = 1;
@@ -1622,7 +1640,8 @@ construct_spec(nvlist_t *props, int argc, char **argv)
 					    ZPOOL_CONFIG_ALLOCATION_BIAS,
 					    VDEV_ALLOC_BIAS_DEDUP) == 0);
 				}
-				if (strcmp(type, VDEV_TYPE_RAIDZ) == 0) {
+				if (strcmp(type, VDEV_TYPE_RAIDZ) == 0 ||
+					strcmp(type, VDEV_TYPE_MY_RAIDZ) == 0) {
 					verify(nvlist_add_uint64(nv,
 					    ZPOOL_CONFIG_NPARITY,
 					    mindev - 1) == 0);
diff --git a/config/zfs-build.m4 b/config/zfs-build.m4
index 9390812cd..6290cbf51 100644
--- a/config/zfs-build.m4
+++ b/config/zfs-build.m4
@@ -14,7 +14,7 @@ AC_DEFUN([ZFS_AC_DEBUG_ENABLE], [
 	WITH_DEBUG="true"
 	AC_DEFINE(ZFS_DEBUG, 1, [zfs debugging enabled])
 
-	KERNEL_DEBUG_CFLAGS="-Werror"
+	# KERNEL_DEBUG_CFLAGS="-Werror -Wno-error=vla"
 	KERNEL_DEBUG_CPPFLAGS="-DDEBUG -UNDEBUG"
 ])
 
diff --git a/create_loopback.sh b/create_loopback.sh
new file mode 100755
index 000000000..04dfddc0d
--- /dev/null
+++ b/create_loopback.sh
@@ -0,0 +1,21 @@
+#!/bin/bash
+size=$1
+
+for i in {1..4}; do
+    # Create tmpfs
+        sudo mkdir -p /media/rdisk$i
+        sudo mount -t tmpfs -o size=${size} tmpfs /media/rdisk$i/
+        sudo truncate -s ${size} /media/rdisk$i/disk.img
+
+    # Install GPT table to the img
+    cfdisk /media/rdisk$i/disk.img
+
+    # Setup loopback device
+    loopback_num=$(($i+20))
+    losetup -P /dev/loop$loopback_num /media/rdisk$i/disk.img
+
+    # # Format FS and mount
+    # mkfs.ext4 /dev/loop$loopback_num
+    # sudo mkdir -p /rdisk/ram$i
+    # sudo mount /dev/loop$loopback_num /rdisk/ram$i
+done
diff --git a/default-net.xml b/default-net.xml
new file mode 100644
index 000000000..57e49e0d6
--- /dev/null
+++ b/default-net.xml
@@ -0,0 +1,9 @@
+<network>
+  <name>default</name>
+  <bridge name="virbr0" stp="on" delay="0"/>
+  <ip address="192.168.122.1" netmask="255.255.255.0">
+    <dhcp>
+      <range start="192.168.122.2" end="192.168.122.254"/>
+    </dhcp>
+  </ip>
+</network>
diff --git a/include/libzfs.h b/include/libzfs.h
index 214a188f9..521ddbb00 100644
--- a/include/libzfs.h
+++ b/include/libzfs.h
@@ -283,6 +283,9 @@ typedef struct trimflags {
  * Functions to manipulate pool and vdev state
  */
 extern int zpool_scan(zpool_handle_t *, pool_scan_func_t, pool_scrub_cmd_t);
+extern int zpool_easy_scan(zpool_handle_t *);
+extern int zpool_get_all_dnode(zpool_handle_t *);
+extern int zpool_get_failed_chunks(zpool_handle_t *, int64_t, int64_t);
 extern int zpool_initialize(zpool_handle_t *, pool_initialize_func_t,
     nvlist_t *);
 extern int zpool_initialize_wait(zpool_handle_t *, pool_initialize_func_t,
diff --git a/include/libzfs_core.h b/include/libzfs_core.h
index 34161a06f..f27a03270 100644
--- a/include/libzfs_core.h
+++ b/include/libzfs_core.h
@@ -137,6 +137,12 @@ int lzc_wait_fs(const char *, zfs_wait_activity_t, boolean_t *);
 
 int lzc_set_bootenv(const char *, const nvlist_t *);
 int lzc_get_bootenv(const char *, nvlist_t **);
+
+// MLEC test stuff
+int lzc_mlec_receive_data(const char *pool, nvlist_t *input);
+int lzc_mlec_get_failed_chunks(const char *pool, nvlist_t *input, nvlist_t **output);
+int lzc_mlec_get_all_dnode(const char *pool, nvlist_t *input, nvlist_t **output);
+
 #ifdef	__cplusplus
 }
 #endif
diff --git a/include/sys/fs/zfs.h b/include/sys/fs/zfs.h
index 84f5aee59..24d7305be 100644
--- a/include/sys/fs/zfs.h
+++ b/include/sys/fs/zfs.h
@@ -32,244 +32,251 @@
  * Copyright (c) 2022 Hewlett Packard Enterprise Development LP.
  */
 
-#ifndef	_SYS_FS_ZFS_H
-#define	_SYS_FS_ZFS_H
+#ifndef _SYS_FS_ZFS_H
+#define _SYS_FS_ZFS_H
 
 #include <sys/time.h>
 #include <sys/zio_priority.h>
 
-#ifdef	__cplusplus
-extern "C" {
+#ifdef __cplusplus
+extern "C"
+{
 #endif
 
-/*
- * Types and constants shared between userland and the kernel.
- */
+	/*
+	 * Types and constants shared between userland and the kernel.
+	 */
 
-/*
- * Each dataset can be one of the following types.  These constants can be
- * combined into masks that can be passed to various functions.
- */
-typedef enum {
-	ZFS_TYPE_FILESYSTEM	= (1 << 0),
-	ZFS_TYPE_SNAPSHOT	= (1 << 1),
-	ZFS_TYPE_VOLUME		= (1 << 2),
-	ZFS_TYPE_POOL		= (1 << 3),
-	ZFS_TYPE_BOOKMARK	= (1 << 4)
-} zfs_type_t;
+	/*
+	 * Each dataset can be one of the following types.  These constants can be
+	 * combined into masks that can be passed to various functions.
+	 */
+	typedef enum
+	{
+		ZFS_TYPE_FILESYSTEM = (1 << 0),
+		ZFS_TYPE_SNAPSHOT = (1 << 1),
+		ZFS_TYPE_VOLUME = (1 << 2),
+		ZFS_TYPE_POOL = (1 << 3),
+		ZFS_TYPE_BOOKMARK = (1 << 4)
+	} zfs_type_t;
 
-/*
- * NB: lzc_dataset_type should be updated whenever a new objset type is added,
- * if it represents a real type of a dataset that can be created from userland.
- */
-typedef enum dmu_objset_type {
-	DMU_OST_NONE,
-	DMU_OST_META,
-	DMU_OST_ZFS,
-	DMU_OST_ZVOL,
-	DMU_OST_OTHER,			/* For testing only! */
-	DMU_OST_ANY,			/* Be careful! */
-	DMU_OST_NUMTYPES
-} dmu_objset_type_t;
-
-#define	ZFS_TYPE_DATASET	\
+	/*
+	 * NB: lzc_dataset_type should be updated whenever a new objset type is added,
+	 * if it represents a real type of a dataset that can be created from userland.
+	 */
+	typedef enum dmu_objset_type
+	{
+		DMU_OST_NONE,
+		DMU_OST_META,
+		DMU_OST_ZFS,
+		DMU_OST_ZVOL,
+		DMU_OST_OTHER, /* For testing only! */
+		DMU_OST_ANY,   /* Be careful! */
+		DMU_OST_NUMTYPES
+	} dmu_objset_type_t;
+
+#define ZFS_TYPE_DATASET \
 	(ZFS_TYPE_FILESYSTEM | ZFS_TYPE_VOLUME | ZFS_TYPE_SNAPSHOT)
 
 /*
  * All of these include the terminating NUL byte.
  */
-#define	ZAP_MAXNAMELEN 256
-#define	ZAP_MAXVALUELEN (1024 * 8)
-#define	ZAP_OLDMAXVALUELEN 1024
-#define	ZFS_MAX_DATASET_NAME_LEN 256
+#define ZAP_MAXNAMELEN 256
+#define ZAP_MAXVALUELEN (1024 * 8)
+#define ZAP_OLDMAXVALUELEN 1024
+#define ZFS_MAX_DATASET_NAME_LEN 256
 
-/*
- * Dataset properties are identified by these constants and must be added to
- * the end of this list to ensure that external consumers are not affected
- * by the change. If you make any changes to this list, be sure to update
- * the property table in module/zcommon/zfs_prop.c.
- */
-typedef enum {
-	ZPROP_CONT = -2,
-	ZPROP_INVAL = -1,
-	ZFS_PROP_TYPE = 0,
-	ZFS_PROP_CREATION,
-	ZFS_PROP_USED,
-	ZFS_PROP_AVAILABLE,
-	ZFS_PROP_REFERENCED,
-	ZFS_PROP_COMPRESSRATIO,
-	ZFS_PROP_MOUNTED,
-	ZFS_PROP_ORIGIN,
-	ZFS_PROP_QUOTA,
-	ZFS_PROP_RESERVATION,
-	ZFS_PROP_VOLSIZE,
-	ZFS_PROP_VOLBLOCKSIZE,
-	ZFS_PROP_RECORDSIZE,
-	ZFS_PROP_MOUNTPOINT,
-	ZFS_PROP_SHARENFS,
-	ZFS_PROP_CHECKSUM,
-	ZFS_PROP_COMPRESSION,
-	ZFS_PROP_ATIME,
-	ZFS_PROP_DEVICES,
-	ZFS_PROP_EXEC,
-	ZFS_PROP_SETUID,
-	ZFS_PROP_READONLY,
-	ZFS_PROP_ZONED,
-	ZFS_PROP_SNAPDIR,
-	ZFS_PROP_ACLMODE,
-	ZFS_PROP_ACLINHERIT,
-	ZFS_PROP_CREATETXG,
-	ZFS_PROP_NAME,			/* not exposed to the user */
-	ZFS_PROP_CANMOUNT,
-	ZFS_PROP_ISCSIOPTIONS,		/* not exposed to the user */
-	ZFS_PROP_XATTR,
-	ZFS_PROP_NUMCLONES,		/* not exposed to the user */
-	ZFS_PROP_COPIES,
-	ZFS_PROP_VERSION,
-	ZFS_PROP_UTF8ONLY,
-	ZFS_PROP_NORMALIZE,
-	ZFS_PROP_CASE,
-	ZFS_PROP_VSCAN,
-	ZFS_PROP_NBMAND,
-	ZFS_PROP_SHARESMB,
-	ZFS_PROP_REFQUOTA,
-	ZFS_PROP_REFRESERVATION,
-	ZFS_PROP_GUID,
-	ZFS_PROP_PRIMARYCACHE,
-	ZFS_PROP_SECONDARYCACHE,
-	ZFS_PROP_USEDSNAP,
-	ZFS_PROP_USEDDS,
-	ZFS_PROP_USEDCHILD,
-	ZFS_PROP_USEDREFRESERV,
-	ZFS_PROP_USERACCOUNTING,	/* not exposed to the user */
-	ZFS_PROP_STMF_SHAREINFO,	/* not exposed to the user */
-	ZFS_PROP_DEFER_DESTROY,
-	ZFS_PROP_USERREFS,
-	ZFS_PROP_LOGBIAS,
-	ZFS_PROP_UNIQUE,		/* not exposed to the user */
-	ZFS_PROP_OBJSETID,
-	ZFS_PROP_DEDUP,
-	ZFS_PROP_MLSLABEL,
-	ZFS_PROP_SYNC,
-	ZFS_PROP_DNODESIZE,
-	ZFS_PROP_REFRATIO,
-	ZFS_PROP_WRITTEN,
-	ZFS_PROP_CLONES,
-	ZFS_PROP_LOGICALUSED,
-	ZFS_PROP_LOGICALREFERENCED,
-	ZFS_PROP_INCONSISTENT,		/* not exposed to the user */
-	ZFS_PROP_VOLMODE,
-	ZFS_PROP_FILESYSTEM_LIMIT,
-	ZFS_PROP_SNAPSHOT_LIMIT,
-	ZFS_PROP_FILESYSTEM_COUNT,
-	ZFS_PROP_SNAPSHOT_COUNT,
-	ZFS_PROP_SNAPDEV,
-	ZFS_PROP_ACLTYPE,
-	ZFS_PROP_SELINUX_CONTEXT,
-	ZFS_PROP_SELINUX_FSCONTEXT,
-	ZFS_PROP_SELINUX_DEFCONTEXT,
-	ZFS_PROP_SELINUX_ROOTCONTEXT,
-	ZFS_PROP_RELATIME,
-	ZFS_PROP_REDUNDANT_METADATA,
-	ZFS_PROP_OVERLAY,
-	ZFS_PROP_PREV_SNAP,
-	ZFS_PROP_RECEIVE_RESUME_TOKEN,
-	ZFS_PROP_ENCRYPTION,
-	ZFS_PROP_KEYLOCATION,
-	ZFS_PROP_KEYFORMAT,
-	ZFS_PROP_PBKDF2_SALT,
-	ZFS_PROP_PBKDF2_ITERS,
-	ZFS_PROP_ENCRYPTION_ROOT,
-	ZFS_PROP_KEY_GUID,
-	ZFS_PROP_KEYSTATUS,
-	ZFS_PROP_REMAPTXG,		/* obsolete - no longer used */
-	ZFS_PROP_SPECIAL_SMALL_BLOCKS,
-	ZFS_PROP_IVSET_GUID,		/* not exposed to the user */
-	ZFS_PROP_REDACTED,
-	ZFS_PROP_REDACT_SNAPS,
-	ZFS_NUM_PROPS
-} zfs_prop_t;
-
-typedef enum {
-	ZFS_PROP_USERUSED,
-	ZFS_PROP_USERQUOTA,
-	ZFS_PROP_GROUPUSED,
-	ZFS_PROP_GROUPQUOTA,
-	ZFS_PROP_USEROBJUSED,
-	ZFS_PROP_USEROBJQUOTA,
-	ZFS_PROP_GROUPOBJUSED,
-	ZFS_PROP_GROUPOBJQUOTA,
-	ZFS_PROP_PROJECTUSED,
-	ZFS_PROP_PROJECTQUOTA,
-	ZFS_PROP_PROJECTOBJUSED,
-	ZFS_PROP_PROJECTOBJQUOTA,
-	ZFS_NUM_USERQUOTA_PROPS
-} zfs_userquota_prop_t;
-
-extern const char *zfs_userquota_prop_prefixes[ZFS_NUM_USERQUOTA_PROPS];
+	/*
+	 * Dataset properties are identified by these constants and must be added to
+	 * the end of this list to ensure that external consumers are not affected
+	 * by the change. If you make any changes to this list, be sure to update
+	 * the property table in module/zcommon/zfs_prop.c.
+	 */
+	typedef enum
+	{
+		ZPROP_CONT = -2,
+		ZPROP_INVAL = -1,
+		ZFS_PROP_TYPE = 0,
+		ZFS_PROP_CREATION,
+		ZFS_PROP_USED,
+		ZFS_PROP_AVAILABLE,
+		ZFS_PROP_REFERENCED,
+		ZFS_PROP_COMPRESSRATIO,
+		ZFS_PROP_MOUNTED,
+		ZFS_PROP_ORIGIN,
+		ZFS_PROP_QUOTA,
+		ZFS_PROP_RESERVATION,
+		ZFS_PROP_VOLSIZE,
+		ZFS_PROP_VOLBLOCKSIZE,
+		ZFS_PROP_RECORDSIZE,
+		ZFS_PROP_MOUNTPOINT,
+		ZFS_PROP_SHARENFS,
+		ZFS_PROP_CHECKSUM,
+		ZFS_PROP_COMPRESSION,
+		ZFS_PROP_ATIME,
+		ZFS_PROP_DEVICES,
+		ZFS_PROP_EXEC,
+		ZFS_PROP_SETUID,
+		ZFS_PROP_READONLY,
+		ZFS_PROP_ZONED,
+		ZFS_PROP_SNAPDIR,
+		ZFS_PROP_ACLMODE,
+		ZFS_PROP_ACLINHERIT,
+		ZFS_PROP_CREATETXG,
+		ZFS_PROP_NAME, /* not exposed to the user */
+		ZFS_PROP_CANMOUNT,
+		ZFS_PROP_ISCSIOPTIONS, /* not exposed to the user */
+		ZFS_PROP_XATTR,
+		ZFS_PROP_NUMCLONES, /* not exposed to the user */
+		ZFS_PROP_COPIES,
+		ZFS_PROP_VERSION,
+		ZFS_PROP_UTF8ONLY,
+		ZFS_PROP_NORMALIZE,
+		ZFS_PROP_CASE,
+		ZFS_PROP_VSCAN,
+		ZFS_PROP_NBMAND,
+		ZFS_PROP_SHARESMB,
+		ZFS_PROP_REFQUOTA,
+		ZFS_PROP_REFRESERVATION,
+		ZFS_PROP_GUID,
+		ZFS_PROP_PRIMARYCACHE,
+		ZFS_PROP_SECONDARYCACHE,
+		ZFS_PROP_USEDSNAP,
+		ZFS_PROP_USEDDS,
+		ZFS_PROP_USEDCHILD,
+		ZFS_PROP_USEDREFRESERV,
+		ZFS_PROP_USERACCOUNTING, /* not exposed to the user */
+		ZFS_PROP_STMF_SHAREINFO, /* not exposed to the user */
+		ZFS_PROP_DEFER_DESTROY,
+		ZFS_PROP_USERREFS,
+		ZFS_PROP_LOGBIAS,
+		ZFS_PROP_UNIQUE, /* not exposed to the user */
+		ZFS_PROP_OBJSETID,
+		ZFS_PROP_DEDUP,
+		ZFS_PROP_MLSLABEL,
+		ZFS_PROP_SYNC,
+		ZFS_PROP_DNODESIZE,
+		ZFS_PROP_REFRATIO,
+		ZFS_PROP_WRITTEN,
+		ZFS_PROP_CLONES,
+		ZFS_PROP_LOGICALUSED,
+		ZFS_PROP_LOGICALREFERENCED,
+		ZFS_PROP_INCONSISTENT, /* not exposed to the user */
+		ZFS_PROP_VOLMODE,
+		ZFS_PROP_FILESYSTEM_LIMIT,
+		ZFS_PROP_SNAPSHOT_LIMIT,
+		ZFS_PROP_FILESYSTEM_COUNT,
+		ZFS_PROP_SNAPSHOT_COUNT,
+		ZFS_PROP_SNAPDEV,
+		ZFS_PROP_ACLTYPE,
+		ZFS_PROP_SELINUX_CONTEXT,
+		ZFS_PROP_SELINUX_FSCONTEXT,
+		ZFS_PROP_SELINUX_DEFCONTEXT,
+		ZFS_PROP_SELINUX_ROOTCONTEXT,
+		ZFS_PROP_RELATIME,
+		ZFS_PROP_REDUNDANT_METADATA,
+		ZFS_PROP_OVERLAY,
+		ZFS_PROP_PREV_SNAP,
+		ZFS_PROP_RECEIVE_RESUME_TOKEN,
+		ZFS_PROP_ENCRYPTION,
+		ZFS_PROP_KEYLOCATION,
+		ZFS_PROP_KEYFORMAT,
+		ZFS_PROP_PBKDF2_SALT,
+		ZFS_PROP_PBKDF2_ITERS,
+		ZFS_PROP_ENCRYPTION_ROOT,
+		ZFS_PROP_KEY_GUID,
+		ZFS_PROP_KEYSTATUS,
+		ZFS_PROP_REMAPTXG, /* obsolete - no longer used */
+		ZFS_PROP_SPECIAL_SMALL_BLOCKS,
+		ZFS_PROP_IVSET_GUID, /* not exposed to the user */
+		ZFS_PROP_REDACTED,
+		ZFS_PROP_REDACT_SNAPS,
+		ZFS_NUM_PROPS
+	} zfs_prop_t;
+
+	typedef enum
+	{
+		ZFS_PROP_USERUSED,
+		ZFS_PROP_USERQUOTA,
+		ZFS_PROP_GROUPUSED,
+		ZFS_PROP_GROUPQUOTA,
+		ZFS_PROP_USEROBJUSED,
+		ZFS_PROP_USEROBJQUOTA,
+		ZFS_PROP_GROUPOBJUSED,
+		ZFS_PROP_GROUPOBJQUOTA,
+		ZFS_PROP_PROJECTUSED,
+		ZFS_PROP_PROJECTQUOTA,
+		ZFS_PROP_PROJECTOBJUSED,
+		ZFS_PROP_PROJECTOBJQUOTA,
+		ZFS_NUM_USERQUOTA_PROPS
+	} zfs_userquota_prop_t;
+
+	extern const char *zfs_userquota_prop_prefixes[ZFS_NUM_USERQUOTA_PROPS];
 
-/*
- * Pool properties are identified by these constants and must be added to the
- * end of this list to ensure that external consumers are not affected
- * by the change.  Properties must be registered in zfs_prop_init().
- */
-typedef enum {
-	ZPOOL_PROP_INVAL = -1,
-	ZPOOL_PROP_NAME,
-	ZPOOL_PROP_SIZE,
-	ZPOOL_PROP_CAPACITY,
-	ZPOOL_PROP_ALTROOT,
-	ZPOOL_PROP_HEALTH,
-	ZPOOL_PROP_GUID,
-	ZPOOL_PROP_VERSION,
-	ZPOOL_PROP_BOOTFS,
-	ZPOOL_PROP_DELEGATION,
-	ZPOOL_PROP_AUTOREPLACE,
-	ZPOOL_PROP_CACHEFILE,
-	ZPOOL_PROP_FAILUREMODE,
-	ZPOOL_PROP_LISTSNAPS,
-	ZPOOL_PROP_AUTOEXPAND,
-	ZPOOL_PROP_DEDUPDITTO,
-	ZPOOL_PROP_DEDUPRATIO,
-	ZPOOL_PROP_FREE,
-	ZPOOL_PROP_ALLOCATED,
-	ZPOOL_PROP_READONLY,
-	ZPOOL_PROP_ASHIFT,
-	ZPOOL_PROP_COMMENT,
-	ZPOOL_PROP_EXPANDSZ,
-	ZPOOL_PROP_FREEING,
-	ZPOOL_PROP_FRAGMENTATION,
-	ZPOOL_PROP_LEAKED,
-	ZPOOL_PROP_MAXBLOCKSIZE,
-	ZPOOL_PROP_TNAME,
-	ZPOOL_PROP_MAXDNODESIZE,
-	ZPOOL_PROP_MULTIHOST,
-	ZPOOL_PROP_CHECKPOINT,
-	ZPOOL_PROP_LOAD_GUID,
-	ZPOOL_PROP_AUTOTRIM,
-	ZPOOL_PROP_COMPATIBILITY,
-	ZPOOL_NUM_PROPS
-} zpool_prop_t;
+	/*
+	 * Pool properties are identified by these constants and must be added to the
+	 * end of this list to ensure that external consumers are not affected
+	 * by the change.  Properties must be registered in zfs_prop_init().
+	 */
+	typedef enum
+	{
+		ZPOOL_PROP_INVAL = -1,
+		ZPOOL_PROP_NAME,
+		ZPOOL_PROP_SIZE,
+		ZPOOL_PROP_CAPACITY,
+		ZPOOL_PROP_ALTROOT,
+		ZPOOL_PROP_HEALTH,
+		ZPOOL_PROP_GUID,
+		ZPOOL_PROP_VERSION,
+		ZPOOL_PROP_BOOTFS,
+		ZPOOL_PROP_DELEGATION,
+		ZPOOL_PROP_AUTOREPLACE,
+		ZPOOL_PROP_CACHEFILE,
+		ZPOOL_PROP_FAILUREMODE,
+		ZPOOL_PROP_LISTSNAPS,
+		ZPOOL_PROP_AUTOEXPAND,
+		ZPOOL_PROP_DEDUPDITTO,
+		ZPOOL_PROP_DEDUPRATIO,
+		ZPOOL_PROP_FREE,
+		ZPOOL_PROP_ALLOCATED,
+		ZPOOL_PROP_READONLY,
+		ZPOOL_PROP_ASHIFT,
+		ZPOOL_PROP_COMMENT,
+		ZPOOL_PROP_EXPANDSZ,
+		ZPOOL_PROP_FREEING,
+		ZPOOL_PROP_FRAGMENTATION,
+		ZPOOL_PROP_LEAKED,
+		ZPOOL_PROP_MAXBLOCKSIZE,
+		ZPOOL_PROP_TNAME,
+		ZPOOL_PROP_MAXDNODESIZE,
+		ZPOOL_PROP_MULTIHOST,
+		ZPOOL_PROP_CHECKPOINT,
+		ZPOOL_PROP_LOAD_GUID,
+		ZPOOL_PROP_AUTOTRIM,
+		ZPOOL_PROP_COMPATIBILITY,
+		ZPOOL_NUM_PROPS
+	} zpool_prop_t;
 
 /* Small enough to not hog a whole line of printout in zpool(8). */
-#define	ZPROP_MAX_COMMENT	32
+#define ZPROP_MAX_COMMENT 32
 
-#define	ZPROP_VALUE		"value"
-#define	ZPROP_SOURCE		"source"
+#define ZPROP_VALUE "value"
+#define ZPROP_SOURCE "source"
 
-typedef enum {
-	ZPROP_SRC_NONE = 0x1,
-	ZPROP_SRC_DEFAULT = 0x2,
-	ZPROP_SRC_TEMPORARY = 0x4,
-	ZPROP_SRC_LOCAL = 0x8,
-	ZPROP_SRC_INHERITED = 0x10,
-	ZPROP_SRC_RECEIVED = 0x20
-} zprop_source_t;
+	typedef enum
+	{
+		ZPROP_SRC_NONE = 0x1,
+		ZPROP_SRC_DEFAULT = 0x2,
+		ZPROP_SRC_TEMPORARY = 0x4,
+		ZPROP_SRC_LOCAL = 0x8,
+		ZPROP_SRC_INHERITED = 0x10,
+		ZPROP_SRC_RECEIVED = 0x20
+	} zprop_source_t;
 
-#define	ZPROP_SRC_ALL	0x3f
+#define ZPROP_SRC_ALL 0x3f
 
-#define	ZPROP_SOURCE_VAL_RECVD	"$recvd"
-#define	ZPROP_N_MORE_ERRORS	"N_MORE_ERRORS"
+#define ZPROP_SOURCE_VAL_RECVD "$recvd"
+#define ZPROP_N_MORE_ERRORS "N_MORE_ERRORS"
 
 /*
  * Dataset flag implemented as a special entry in the props zap object
@@ -278,227 +285,243 @@ typedef enum {
  * just as it did in earlier versions, and thereafter, local properties are
  * preserved.
  */
-#define	ZPROP_HAS_RECVD		"$hasrecvd"
+#define ZPROP_HAS_RECVD "$hasrecvd"
 
-typedef enum {
-	ZPROP_ERR_NOCLEAR = 0x1, /* failure to clear existing props */
-	ZPROP_ERR_NORESTORE = 0x2 /* failure to restore props on error */
-} zprop_errflags_t;
+	typedef enum
+	{
+		ZPROP_ERR_NOCLEAR = 0x1,  /* failure to clear existing props */
+		ZPROP_ERR_NORESTORE = 0x2 /* failure to restore props on error */
+	} zprop_errflags_t;
 
-typedef int (*zprop_func)(int, void *);
+	typedef int (*zprop_func)(int, void *);
 
 /*
  * Properties to be set on the root file system of a new pool
  * are stuffed into their own nvlist, which is then included in
  * the properties nvlist with the pool properties.
  */
-#define	ZPOOL_ROOTFS_PROPS	"root-props-nvl"
+#define ZPOOL_ROOTFS_PROPS "root-props-nvl"
 
 /*
  * Length of 'written@' and 'written#'
  */
-#define	ZFS_WRITTEN_PROP_PREFIX_LEN	8
+#define ZFS_WRITTEN_PROP_PREFIX_LEN 8
 
-/*
- * Dataset property functions shared between libzfs and kernel.
- */
-const char *zfs_prop_default_string(zfs_prop_t);
-uint64_t zfs_prop_default_numeric(zfs_prop_t);
-boolean_t zfs_prop_readonly(zfs_prop_t);
-boolean_t zfs_prop_visible(zfs_prop_t prop);
-boolean_t zfs_prop_inheritable(zfs_prop_t);
-boolean_t zfs_prop_setonce(zfs_prop_t);
-boolean_t zfs_prop_encryption_key_param(zfs_prop_t);
-boolean_t zfs_prop_valid_keylocation(const char *, boolean_t);
-const char *zfs_prop_to_name(zfs_prop_t);
-zfs_prop_t zfs_name_to_prop(const char *);
-boolean_t zfs_prop_user(const char *);
-boolean_t zfs_prop_userquota(const char *);
-boolean_t zfs_prop_written(const char *);
-int zfs_prop_index_to_string(zfs_prop_t, uint64_t, const char **);
-int zfs_prop_string_to_index(zfs_prop_t, const char *, uint64_t *);
-uint64_t zfs_prop_random_value(zfs_prop_t, uint64_t seed);
-boolean_t zfs_prop_valid_for_type(int, zfs_type_t, boolean_t);
+	/*
+	 * Dataset property functions shared between libzfs and kernel.
+	 */
+	const char *zfs_prop_default_string(zfs_prop_t);
+	uint64_t zfs_prop_default_numeric(zfs_prop_t);
+	boolean_t zfs_prop_readonly(zfs_prop_t);
+	boolean_t zfs_prop_visible(zfs_prop_t prop);
+	boolean_t zfs_prop_inheritable(zfs_prop_t);
+	boolean_t zfs_prop_setonce(zfs_prop_t);
+	boolean_t zfs_prop_encryption_key_param(zfs_prop_t);
+	boolean_t zfs_prop_valid_keylocation(const char *, boolean_t);
+	const char *zfs_prop_to_name(zfs_prop_t);
+	zfs_prop_t zfs_name_to_prop(const char *);
+	boolean_t zfs_prop_user(const char *);
+	boolean_t zfs_prop_userquota(const char *);
+	boolean_t zfs_prop_written(const char *);
+	int zfs_prop_index_to_string(zfs_prop_t, uint64_t, const char **);
+	int zfs_prop_string_to_index(zfs_prop_t, const char *, uint64_t *);
+	uint64_t zfs_prop_random_value(zfs_prop_t, uint64_t seed);
+	boolean_t zfs_prop_valid_for_type(int, zfs_type_t, boolean_t);
 
-/*
- * Pool property functions shared between libzfs and kernel.
- */
-zpool_prop_t zpool_name_to_prop(const char *);
-const char *zpool_prop_to_name(zpool_prop_t);
-const char *zpool_prop_default_string(zpool_prop_t);
-uint64_t zpool_prop_default_numeric(zpool_prop_t);
-boolean_t zpool_prop_readonly(zpool_prop_t);
-boolean_t zpool_prop_setonce(zpool_prop_t);
-boolean_t zpool_prop_feature(const char *);
-boolean_t zpool_prop_unsupported(const char *);
-int zpool_prop_index_to_string(zpool_prop_t, uint64_t, const char **);
-int zpool_prop_string_to_index(zpool_prop_t, const char *, uint64_t *);
-uint64_t zpool_prop_random_value(zpool_prop_t, uint64_t seed);
+	/*
+	 * Pool property functions shared between libzfs and kernel.
+	 */
+	zpool_prop_t zpool_name_to_prop(const char *);
+	const char *zpool_prop_to_name(zpool_prop_t);
+	const char *zpool_prop_default_string(zpool_prop_t);
+	uint64_t zpool_prop_default_numeric(zpool_prop_t);
+	boolean_t zpool_prop_readonly(zpool_prop_t);
+	boolean_t zpool_prop_setonce(zpool_prop_t);
+	boolean_t zpool_prop_feature(const char *);
+	boolean_t zpool_prop_unsupported(const char *);
+	int zpool_prop_index_to_string(zpool_prop_t, uint64_t, const char **);
+	int zpool_prop_string_to_index(zpool_prop_t, const char *, uint64_t *);
+	uint64_t zpool_prop_random_value(zpool_prop_t, uint64_t seed);
 
-/*
- * Definitions for the Delegation.
- */
-typedef enum {
-	ZFS_DELEG_WHO_UNKNOWN = 0,
-	ZFS_DELEG_USER = 'u',
-	ZFS_DELEG_USER_SETS = 'U',
-	ZFS_DELEG_GROUP = 'g',
-	ZFS_DELEG_GROUP_SETS = 'G',
-	ZFS_DELEG_EVERYONE = 'e',
-	ZFS_DELEG_EVERYONE_SETS = 'E',
-	ZFS_DELEG_CREATE = 'c',
-	ZFS_DELEG_CREATE_SETS = 'C',
-	ZFS_DELEG_NAMED_SET = 's',
-	ZFS_DELEG_NAMED_SET_SETS = 'S'
-} zfs_deleg_who_type_t;
-
-typedef enum {
-	ZFS_DELEG_NONE = 0,
-	ZFS_DELEG_PERM_LOCAL = 1,
-	ZFS_DELEG_PERM_DESCENDENT = 2,
-	ZFS_DELEG_PERM_LOCALDESCENDENT = 3,
-	ZFS_DELEG_PERM_CREATE = 4
-} zfs_deleg_inherit_t;
-
-#define	ZFS_DELEG_PERM_UID	"uid"
-#define	ZFS_DELEG_PERM_GID	"gid"
-#define	ZFS_DELEG_PERM_GROUPS	"groups"
-
-#define	ZFS_MLSLABEL_DEFAULT	"none"
-
-#define	ZFS_SMB_ACL_SRC		"src"
-#define	ZFS_SMB_ACL_TARGET	"target"
-
-typedef enum {
-	ZFS_CANMOUNT_OFF = 0,
-	ZFS_CANMOUNT_ON = 1,
-	ZFS_CANMOUNT_NOAUTO = 2
-} zfs_canmount_type_t;
-
-typedef enum {
-	ZFS_LOGBIAS_LATENCY = 0,
-	ZFS_LOGBIAS_THROUGHPUT = 1
-} zfs_logbias_op_t;
-
-typedef enum zfs_share_op {
-	ZFS_SHARE_NFS = 0,
-	ZFS_UNSHARE_NFS = 1,
-	ZFS_SHARE_SMB = 2,
-	ZFS_UNSHARE_SMB = 3
-} zfs_share_op_t;
-
-typedef enum zfs_smb_acl_op {
-	ZFS_SMB_ACL_ADD,
-	ZFS_SMB_ACL_REMOVE,
-	ZFS_SMB_ACL_RENAME,
-	ZFS_SMB_ACL_PURGE
-} zfs_smb_acl_op_t;
-
-typedef enum zfs_cache_type {
-	ZFS_CACHE_NONE = 0,
-	ZFS_CACHE_METADATA = 1,
-	ZFS_CACHE_ALL = 2
-} zfs_cache_type_t;
-
-typedef enum {
-	ZFS_SYNC_STANDARD = 0,
-	ZFS_SYNC_ALWAYS = 1,
-	ZFS_SYNC_DISABLED = 2
-} zfs_sync_type_t;
-
-typedef enum {
-	ZFS_XATTR_OFF = 0,
-	ZFS_XATTR_DIR = 1,
-	ZFS_XATTR_SA = 2
-} zfs_xattr_type_t;
-
-typedef enum {
-	ZFS_DNSIZE_LEGACY = 0,
-	ZFS_DNSIZE_AUTO = 1,
-	ZFS_DNSIZE_1K = 1024,
-	ZFS_DNSIZE_2K = 2048,
-	ZFS_DNSIZE_4K = 4096,
-	ZFS_DNSIZE_8K = 8192,
-	ZFS_DNSIZE_16K = 16384
-} zfs_dnsize_type_t;
-
-typedef enum {
-	ZFS_REDUNDANT_METADATA_ALL,
-	ZFS_REDUNDANT_METADATA_MOST,
-	ZFS_REDUNDANT_METADATA_SOME,
-	ZFS_REDUNDANT_METADATA_NONE
-} zfs_redundant_metadata_type_t;
-
-typedef enum {
-	ZFS_VOLMODE_DEFAULT = 0,
-	ZFS_VOLMODE_GEOM = 1,
-	ZFS_VOLMODE_DEV = 2,
-	ZFS_VOLMODE_NONE = 3
-} zfs_volmode_t;
-
-typedef enum zfs_keystatus {
-	ZFS_KEYSTATUS_NONE = 0,
-	ZFS_KEYSTATUS_UNAVAILABLE,
-	ZFS_KEYSTATUS_AVAILABLE,
-} zfs_keystatus_t;
-
-typedef enum zfs_keyformat {
-	ZFS_KEYFORMAT_NONE = 0,
-	ZFS_KEYFORMAT_RAW,
-	ZFS_KEYFORMAT_HEX,
-	ZFS_KEYFORMAT_PASSPHRASE,
-	ZFS_KEYFORMAT_FORMATS
-} zfs_keyformat_t;
-
-typedef enum zfs_key_location {
-	ZFS_KEYLOCATION_NONE = 0,
-	ZFS_KEYLOCATION_PROMPT,
-	ZFS_KEYLOCATION_URI,
-	ZFS_KEYLOCATION_LOCATIONS
-} zfs_keylocation_t;
-
-#define	DEFAULT_PBKDF2_ITERATIONS 350000
-#define	MIN_PBKDF2_ITERATIONS 100000
+	/*
+	 * Definitions for the Delegation.
+	 */
+	typedef enum
+	{
+		ZFS_DELEG_WHO_UNKNOWN = 0,
+		ZFS_DELEG_USER = 'u',
+		ZFS_DELEG_USER_SETS = 'U',
+		ZFS_DELEG_GROUP = 'g',
+		ZFS_DELEG_GROUP_SETS = 'G',
+		ZFS_DELEG_EVERYONE = 'e',
+		ZFS_DELEG_EVERYONE_SETS = 'E',
+		ZFS_DELEG_CREATE = 'c',
+		ZFS_DELEG_CREATE_SETS = 'C',
+		ZFS_DELEG_NAMED_SET = 's',
+		ZFS_DELEG_NAMED_SET_SETS = 'S'
+	} zfs_deleg_who_type_t;
+
+	typedef enum
+	{
+		ZFS_DELEG_NONE = 0,
+		ZFS_DELEG_PERM_LOCAL = 1,
+		ZFS_DELEG_PERM_DESCENDENT = 2,
+		ZFS_DELEG_PERM_LOCALDESCENDENT = 3,
+		ZFS_DELEG_PERM_CREATE = 4
+	} zfs_deleg_inherit_t;
+
+#define ZFS_DELEG_PERM_UID "uid"
+#define ZFS_DELEG_PERM_GID "gid"
+#define ZFS_DELEG_PERM_GROUPS "groups"
+
+#define ZFS_MLSLABEL_DEFAULT "none"
+
+#define ZFS_SMB_ACL_SRC "src"
+#define ZFS_SMB_ACL_TARGET "target"
+
+	typedef enum
+	{
+		ZFS_CANMOUNT_OFF = 0,
+		ZFS_CANMOUNT_ON = 1,
+		ZFS_CANMOUNT_NOAUTO = 2
+	} zfs_canmount_type_t;
+
+	typedef enum
+	{
+		ZFS_LOGBIAS_LATENCY = 0,
+		ZFS_LOGBIAS_THROUGHPUT = 1
+	} zfs_logbias_op_t;
+
+	typedef enum zfs_share_op
+	{
+		ZFS_SHARE_NFS = 0,
+		ZFS_UNSHARE_NFS = 1,
+		ZFS_SHARE_SMB = 2,
+		ZFS_UNSHARE_SMB = 3
+	} zfs_share_op_t;
+
+	typedef enum zfs_smb_acl_op
+	{
+		ZFS_SMB_ACL_ADD,
+		ZFS_SMB_ACL_REMOVE,
+		ZFS_SMB_ACL_RENAME,
+		ZFS_SMB_ACL_PURGE
+	} zfs_smb_acl_op_t;
+
+	typedef enum zfs_cache_type
+	{
+		ZFS_CACHE_NONE = 0,
+		ZFS_CACHE_METADATA = 1,
+		ZFS_CACHE_ALL = 2
+	} zfs_cache_type_t;
+
+	typedef enum
+	{
+		ZFS_SYNC_STANDARD = 0,
+		ZFS_SYNC_ALWAYS = 1,
+		ZFS_SYNC_DISABLED = 2
+	} zfs_sync_type_t;
+
+	typedef enum
+	{
+		ZFS_XATTR_OFF = 0,
+		ZFS_XATTR_DIR = 1,
+		ZFS_XATTR_SA = 2
+	} zfs_xattr_type_t;
+
+	typedef enum
+	{
+		ZFS_DNSIZE_LEGACY = 0,
+		ZFS_DNSIZE_AUTO = 1,
+		ZFS_DNSIZE_1K = 1024,
+		ZFS_DNSIZE_2K = 2048,
+		ZFS_DNSIZE_4K = 4096,
+		ZFS_DNSIZE_8K = 8192,
+		ZFS_DNSIZE_16K = 16384
+	} zfs_dnsize_type_t;
+
+	typedef enum
+	{
+		ZFS_REDUNDANT_METADATA_ALL,
+		ZFS_REDUNDANT_METADATA_MOST,
+		ZFS_REDUNDANT_METADATA_SOME,
+		ZFS_REDUNDANT_METADATA_NONE
+	} zfs_redundant_metadata_type_t;
+
+	typedef enum
+	{
+		ZFS_VOLMODE_DEFAULT = 0,
+		ZFS_VOLMODE_GEOM = 1,
+		ZFS_VOLMODE_DEV = 2,
+		ZFS_VOLMODE_NONE = 3
+	} zfs_volmode_t;
+
+	typedef enum zfs_keystatus
+	{
+		ZFS_KEYSTATUS_NONE = 0,
+		ZFS_KEYSTATUS_UNAVAILABLE,
+		ZFS_KEYSTATUS_AVAILABLE,
+	} zfs_keystatus_t;
+
+	typedef enum zfs_keyformat
+	{
+		ZFS_KEYFORMAT_NONE = 0,
+		ZFS_KEYFORMAT_RAW,
+		ZFS_KEYFORMAT_HEX,
+		ZFS_KEYFORMAT_PASSPHRASE,
+		ZFS_KEYFORMAT_FORMATS
+	} zfs_keyformat_t;
+
+	typedef enum zfs_key_location
+	{
+		ZFS_KEYLOCATION_NONE = 0,
+		ZFS_KEYLOCATION_PROMPT,
+		ZFS_KEYLOCATION_URI,
+		ZFS_KEYLOCATION_LOCATIONS
+	} zfs_keylocation_t;
+
+#define DEFAULT_PBKDF2_ITERATIONS 350000
+#define MIN_PBKDF2_ITERATIONS 100000
 
 /*
  * On-disk version number.
  */
-#define	SPA_VERSION_1			1ULL
-#define	SPA_VERSION_2			2ULL
-#define	SPA_VERSION_3			3ULL
-#define	SPA_VERSION_4			4ULL
-#define	SPA_VERSION_5			5ULL
-#define	SPA_VERSION_6			6ULL
-#define	SPA_VERSION_7			7ULL
-#define	SPA_VERSION_8			8ULL
-#define	SPA_VERSION_9			9ULL
-#define	SPA_VERSION_10			10ULL
-#define	SPA_VERSION_11			11ULL
-#define	SPA_VERSION_12			12ULL
-#define	SPA_VERSION_13			13ULL
-#define	SPA_VERSION_14			14ULL
-#define	SPA_VERSION_15			15ULL
-#define	SPA_VERSION_16			16ULL
-#define	SPA_VERSION_17			17ULL
-#define	SPA_VERSION_18			18ULL
-#define	SPA_VERSION_19			19ULL
-#define	SPA_VERSION_20			20ULL
-#define	SPA_VERSION_21			21ULL
-#define	SPA_VERSION_22			22ULL
-#define	SPA_VERSION_23			23ULL
-#define	SPA_VERSION_24			24ULL
-#define	SPA_VERSION_25			25ULL
-#define	SPA_VERSION_26			26ULL
-#define	SPA_VERSION_27			27ULL
-#define	SPA_VERSION_28			28ULL
-#define	SPA_VERSION_5000		5000ULL
+#define SPA_VERSION_1 1ULL
+#define SPA_VERSION_2 2ULL
+#define SPA_VERSION_3 3ULL
+#define SPA_VERSION_4 4ULL
+#define SPA_VERSION_5 5ULL
+#define SPA_VERSION_6 6ULL
+#define SPA_VERSION_7 7ULL
+#define SPA_VERSION_8 8ULL
+#define SPA_VERSION_9 9ULL
+#define SPA_VERSION_10 10ULL
+#define SPA_VERSION_11 11ULL
+#define SPA_VERSION_12 12ULL
+#define SPA_VERSION_13 13ULL
+#define SPA_VERSION_14 14ULL
+#define SPA_VERSION_15 15ULL
+#define SPA_VERSION_16 16ULL
+#define SPA_VERSION_17 17ULL
+#define SPA_VERSION_18 18ULL
+#define SPA_VERSION_19 19ULL
+#define SPA_VERSION_20 20ULL
+#define SPA_VERSION_21 21ULL
+#define SPA_VERSION_22 22ULL
+#define SPA_VERSION_23 23ULL
+#define SPA_VERSION_24 24ULL
+#define SPA_VERSION_25 25ULL
+#define SPA_VERSION_26 26ULL
+#define SPA_VERSION_27 27ULL
+#define SPA_VERSION_28 28ULL
+#define SPA_VERSION_5000 5000ULL
 
 /*
  * The incrementing pool version number has been replaced by pool feature
  * flags.  For more details, see zfeature.c.
  */
-#define	SPA_VERSION			SPA_VERSION_5000
-#define	SPA_VERSION_STRING		"5000"
+#define SPA_VERSION SPA_VERSION_5000
+#define SPA_VERSION_STRING "5000"
 
 /*
  * Symbolic names for the changes that caused a SPA_VERSION switch.
@@ -511,92 +534,93 @@ typedef enum zfs_key_location {
  *       last synced uberblock.  Checking the in-flight version can
  *       be dangerous in some cases.
  */
-#define	SPA_VERSION_INITIAL		SPA_VERSION_1
-#define	SPA_VERSION_DITTO_BLOCKS	SPA_VERSION_2
-#define	SPA_VERSION_SPARES		SPA_VERSION_3
-#define	SPA_VERSION_RAIDZ2		SPA_VERSION_3
-#define	SPA_VERSION_BPOBJ_ACCOUNT	SPA_VERSION_3
-#define	SPA_VERSION_RAIDZ_DEFLATE	SPA_VERSION_3
-#define	SPA_VERSION_DNODE_BYTES		SPA_VERSION_3
-#define	SPA_VERSION_ZPOOL_HISTORY	SPA_VERSION_4
-#define	SPA_VERSION_GZIP_COMPRESSION	SPA_VERSION_5
-#define	SPA_VERSION_BOOTFS		SPA_VERSION_6
-#define	SPA_VERSION_SLOGS		SPA_VERSION_7
-#define	SPA_VERSION_DELEGATED_PERMS	SPA_VERSION_8
-#define	SPA_VERSION_FUID		SPA_VERSION_9
-#define	SPA_VERSION_REFRESERVATION	SPA_VERSION_9
-#define	SPA_VERSION_REFQUOTA		SPA_VERSION_9
-#define	SPA_VERSION_UNIQUE_ACCURATE	SPA_VERSION_9
-#define	SPA_VERSION_L2CACHE		SPA_VERSION_10
-#define	SPA_VERSION_NEXT_CLONES		SPA_VERSION_11
-#define	SPA_VERSION_ORIGIN		SPA_VERSION_11
-#define	SPA_VERSION_DSL_SCRUB		SPA_VERSION_11
-#define	SPA_VERSION_SNAP_PROPS		SPA_VERSION_12
-#define	SPA_VERSION_USED_BREAKDOWN	SPA_VERSION_13
-#define	SPA_VERSION_PASSTHROUGH_X	SPA_VERSION_14
-#define	SPA_VERSION_USERSPACE		SPA_VERSION_15
-#define	SPA_VERSION_STMF_PROP		SPA_VERSION_16
-#define	SPA_VERSION_RAIDZ3		SPA_VERSION_17
-#define	SPA_VERSION_USERREFS		SPA_VERSION_18
-#define	SPA_VERSION_HOLES		SPA_VERSION_19
-#define	SPA_VERSION_ZLE_COMPRESSION	SPA_VERSION_20
-#define	SPA_VERSION_DEDUP		SPA_VERSION_21
-#define	SPA_VERSION_RECVD_PROPS		SPA_VERSION_22
-#define	SPA_VERSION_SLIM_ZIL		SPA_VERSION_23
-#define	SPA_VERSION_SA			SPA_VERSION_24
-#define	SPA_VERSION_SCAN		SPA_VERSION_25
-#define	SPA_VERSION_DIR_CLONES		SPA_VERSION_26
-#define	SPA_VERSION_DEADLISTS		SPA_VERSION_26
-#define	SPA_VERSION_FAST_SNAP		SPA_VERSION_27
-#define	SPA_VERSION_MULTI_REPLACE	SPA_VERSION_28
-#define	SPA_VERSION_BEFORE_FEATURES	SPA_VERSION_28
-#define	SPA_VERSION_FEATURES		SPA_VERSION_5000
-
-#define	SPA_VERSION_IS_SUPPORTED(v) \
+#define SPA_VERSION_INITIAL SPA_VERSION_1
+#define SPA_VERSION_DITTO_BLOCKS SPA_VERSION_2
+#define SPA_VERSION_SPARES SPA_VERSION_3
+#define SPA_VERSION_RAIDZ2 SPA_VERSION_3
+#define SPA_VERSION_BPOBJ_ACCOUNT SPA_VERSION_3
+#define SPA_VERSION_RAIDZ_DEFLATE SPA_VERSION_3
+#define SPA_VERSION_DNODE_BYTES SPA_VERSION_3
+#define SPA_VERSION_ZPOOL_HISTORY SPA_VERSION_4
+#define SPA_VERSION_GZIP_COMPRESSION SPA_VERSION_5
+#define SPA_VERSION_BOOTFS SPA_VERSION_6
+#define SPA_VERSION_SLOGS SPA_VERSION_7
+#define SPA_VERSION_DELEGATED_PERMS SPA_VERSION_8
+#define SPA_VERSION_FUID SPA_VERSION_9
+#define SPA_VERSION_REFRESERVATION SPA_VERSION_9
+#define SPA_VERSION_REFQUOTA SPA_VERSION_9
+#define SPA_VERSION_UNIQUE_ACCURATE SPA_VERSION_9
+#define SPA_VERSION_L2CACHE SPA_VERSION_10
+#define SPA_VERSION_NEXT_CLONES SPA_VERSION_11
+#define SPA_VERSION_ORIGIN SPA_VERSION_11
+#define SPA_VERSION_DSL_SCRUB SPA_VERSION_11
+#define SPA_VERSION_SNAP_PROPS SPA_VERSION_12
+#define SPA_VERSION_USED_BREAKDOWN SPA_VERSION_13
+#define SPA_VERSION_PASSTHROUGH_X SPA_VERSION_14
+#define SPA_VERSION_USERSPACE SPA_VERSION_15
+#define SPA_VERSION_STMF_PROP SPA_VERSION_16
+#define SPA_VERSION_RAIDZ3 SPA_VERSION_17
+#define SPA_VERSION_USERREFS SPA_VERSION_18
+#define SPA_VERSION_HOLES SPA_VERSION_19
+#define SPA_VERSION_ZLE_COMPRESSION SPA_VERSION_20
+#define SPA_VERSION_DEDUP SPA_VERSION_21
+#define SPA_VERSION_RECVD_PROPS SPA_VERSION_22
+#define SPA_VERSION_SLIM_ZIL SPA_VERSION_23
+#define SPA_VERSION_SA SPA_VERSION_24
+#define SPA_VERSION_SCAN SPA_VERSION_25
+#define SPA_VERSION_DIR_CLONES SPA_VERSION_26
+#define SPA_VERSION_DEADLISTS SPA_VERSION_26
+#define SPA_VERSION_FAST_SNAP SPA_VERSION_27
+#define SPA_VERSION_MULTI_REPLACE SPA_VERSION_28
+#define SPA_VERSION_BEFORE_FEATURES SPA_VERSION_28
+#define SPA_VERSION_FEATURES SPA_VERSION_5000
+
+#define SPA_VERSION_IS_SUPPORTED(v)                                        \
 	(((v) >= SPA_VERSION_INITIAL && (v) <= SPA_VERSION_BEFORE_FEATURES) || \
-	((v) >= SPA_VERSION_FEATURES && (v) <= SPA_VERSION))
+	 ((v) >= SPA_VERSION_FEATURES && (v) <= SPA_VERSION))
 
 /*
  * ZPL version - rev'd whenever an incompatible on-disk format change
  * occurs.  This is independent of SPA/DMU/ZAP versioning.  You must
  * also update the version_table[] and help message in zfs_prop.c.
  */
-#define	ZPL_VERSION_1			1ULL
-#define	ZPL_VERSION_2			2ULL
-#define	ZPL_VERSION_3			3ULL
-#define	ZPL_VERSION_4			4ULL
-#define	ZPL_VERSION_5			5ULL
-#define	ZPL_VERSION			ZPL_VERSION_5
-#define	ZPL_VERSION_STRING		"5"
-
-#define	ZPL_VERSION_INITIAL		ZPL_VERSION_1
-#define	ZPL_VERSION_DIRENT_TYPE		ZPL_VERSION_2
-#define	ZPL_VERSION_FUID		ZPL_VERSION_3
-#define	ZPL_VERSION_NORMALIZATION	ZPL_VERSION_3
-#define	ZPL_VERSION_SYSATTR		ZPL_VERSION_3
-#define	ZPL_VERSION_USERSPACE		ZPL_VERSION_4
-#define	ZPL_VERSION_SA			ZPL_VERSION_5
+#define ZPL_VERSION_1 1ULL
+#define ZPL_VERSION_2 2ULL
+#define ZPL_VERSION_3 3ULL
+#define ZPL_VERSION_4 4ULL
+#define ZPL_VERSION_5 5ULL
+#define ZPL_VERSION ZPL_VERSION_5
+#define ZPL_VERSION_STRING "5"
+
+#define ZPL_VERSION_INITIAL ZPL_VERSION_1
+#define ZPL_VERSION_DIRENT_TYPE ZPL_VERSION_2
+#define ZPL_VERSION_FUID ZPL_VERSION_3
+#define ZPL_VERSION_NORMALIZATION ZPL_VERSION_3
+#define ZPL_VERSION_SYSATTR ZPL_VERSION_3
+#define ZPL_VERSION_USERSPACE ZPL_VERSION_4
+#define ZPL_VERSION_SA ZPL_VERSION_5
 
 /* Persistent L2ARC version */
-#define	L2ARC_PERSISTENT_VERSION_1	1ULL
-#define	L2ARC_PERSISTENT_VERSION	L2ARC_PERSISTENT_VERSION_1
-#define	L2ARC_PERSISTENT_VERSION_STRING	"1"
+#define L2ARC_PERSISTENT_VERSION_1 1ULL
+#define L2ARC_PERSISTENT_VERSION L2ARC_PERSISTENT_VERSION_1
+#define L2ARC_PERSISTENT_VERSION_STRING "1"
 
 /* Rewind policy information */
-#define	ZPOOL_NO_REWIND		1  /* No policy - default behavior */
-#define	ZPOOL_NEVER_REWIND	2  /* Do not search for best txg or rewind */
-#define	ZPOOL_TRY_REWIND	4  /* Search for best txg, but do not rewind */
-#define	ZPOOL_DO_REWIND		8  /* Rewind to best txg w/in deferred frees */
-#define	ZPOOL_EXTREME_REWIND	16 /* Allow extreme measures to find best txg */
-#define	ZPOOL_REWIND_MASK	28 /* All the possible rewind bits */
-#define	ZPOOL_REWIND_POLICIES	31 /* All the possible policy bits */
-
-typedef struct zpool_load_policy {
-	uint32_t	zlp_rewind;	/* rewind policy requested */
-	uint64_t	zlp_maxmeta;	/* max acceptable meta-data errors */
-	uint64_t	zlp_maxdata;	/* max acceptable data errors */
-	uint64_t	zlp_txg;	/* specific txg to load */
-} zpool_load_policy_t;
+#define ZPOOL_NO_REWIND 1		 /* No policy - default behavior */
+#define ZPOOL_NEVER_REWIND 2	 /* Do not search for best txg or rewind */
+#define ZPOOL_TRY_REWIND 4		 /* Search for best txg, but do not rewind */
+#define ZPOOL_DO_REWIND 8		 /* Rewind to best txg w/in deferred frees */
+#define ZPOOL_EXTREME_REWIND 16	 /* Allow extreme measures to find best txg */
+#define ZPOOL_REWIND_MASK 28	 /* All the possible rewind bits */
+#define ZPOOL_REWIND_POLICIES 31 /* All the possible policy bits */
+
+	typedef struct zpool_load_policy
+	{
+		uint32_t zlp_rewind;  /* rewind policy requested */
+		uint64_t zlp_maxmeta; /* max acceptable meta-data errors */
+		uint64_t zlp_maxdata; /* max acceptable data errors */
+		uint64_t zlp_txg;	  /* specific txg to load */
+	} zpool_load_policy_t;
 
 /*
  * The following are configuration names used in the nvlist describing a pool's
@@ -604,460 +628,477 @@ typedef struct zpool_load_policy {
  * (e.g. "org.openzfs:") to avoid conflicting names being developed
  * independently.
  */
-#define	ZPOOL_CONFIG_VERSION		"version"
-#define	ZPOOL_CONFIG_POOL_NAME		"name"
-#define	ZPOOL_CONFIG_POOL_STATE		"state"
-#define	ZPOOL_CONFIG_POOL_TXG		"txg"
-#define	ZPOOL_CONFIG_POOL_GUID		"pool_guid"
-#define	ZPOOL_CONFIG_CREATE_TXG		"create_txg"
-#define	ZPOOL_CONFIG_TOP_GUID		"top_guid"
-#define	ZPOOL_CONFIG_VDEV_TREE		"vdev_tree"
-#define	ZPOOL_CONFIG_TYPE		"type"
-#define	ZPOOL_CONFIG_CHILDREN		"children"
-#define	ZPOOL_CONFIG_ID			"id"
-#define	ZPOOL_CONFIG_GUID		"guid"
-#define	ZPOOL_CONFIG_INDIRECT_OBJECT	"com.delphix:indirect_object"
-#define	ZPOOL_CONFIG_INDIRECT_BIRTHS	"com.delphix:indirect_births"
-#define	ZPOOL_CONFIG_PREV_INDIRECT_VDEV	"com.delphix:prev_indirect_vdev"
-#define	ZPOOL_CONFIG_PATH		"path"
-#define	ZPOOL_CONFIG_DEVID		"devid"
-#define	ZPOOL_CONFIG_SPARE_ID		"spareid"
-#define	ZPOOL_CONFIG_METASLAB_ARRAY	"metaslab_array"
-#define	ZPOOL_CONFIG_METASLAB_SHIFT	"metaslab_shift"
-#define	ZPOOL_CONFIG_ASHIFT		"ashift"
-#define	ZPOOL_CONFIG_ASIZE		"asize"
-#define	ZPOOL_CONFIG_DTL		"DTL"
-#define	ZPOOL_CONFIG_SCAN_STATS		"scan_stats"	/* not stored on disk */
-#define	ZPOOL_CONFIG_REMOVAL_STATS	"removal_stats"	/* not stored on disk */
-#define	ZPOOL_CONFIG_CHECKPOINT_STATS	"checkpoint_stats" /* not on disk */
-#define	ZPOOL_CONFIG_VDEV_STATS		"vdev_stats"	/* not stored on disk */
-#define	ZPOOL_CONFIG_INDIRECT_SIZE	"indirect_size"	/* not stored on disk */
+#define ZPOOL_CONFIG_VERSION "version"
+#define ZPOOL_CONFIG_POOL_NAME "name"
+#define ZPOOL_CONFIG_POOL_STATE "state"
+#define ZPOOL_CONFIG_POOL_TXG "txg"
+#define ZPOOL_CONFIG_POOL_GUID "pool_guid"
+#define ZPOOL_CONFIG_CREATE_TXG "create_txg"
+#define ZPOOL_CONFIG_TOP_GUID "top_guid"
+#define ZPOOL_CONFIG_VDEV_TREE "vdev_tree"
+#define ZPOOL_CONFIG_TYPE "type"
+#define ZPOOL_CONFIG_CHILDREN "children"
+#define ZPOOL_CONFIG_ID "id"
+#define ZPOOL_CONFIG_GUID "guid"
+#define ZPOOL_CONFIG_INDIRECT_OBJECT "com.delphix:indirect_object"
+#define ZPOOL_CONFIG_INDIRECT_BIRTHS "com.delphix:indirect_births"
+#define ZPOOL_CONFIG_PREV_INDIRECT_VDEV "com.delphix:prev_indirect_vdev"
+#define ZPOOL_CONFIG_PATH "path"
+#define ZPOOL_CONFIG_DEVID "devid"
+#define ZPOOL_CONFIG_SPARE_ID "spareid"
+#define ZPOOL_CONFIG_METASLAB_ARRAY "metaslab_array"
+#define ZPOOL_CONFIG_METASLAB_SHIFT "metaslab_shift"
+#define ZPOOL_CONFIG_ASHIFT "ashift"
+#define ZPOOL_CONFIG_ASIZE "asize"
+#define ZPOOL_CONFIG_DTL "DTL"
+#define ZPOOL_CONFIG_SCAN_STATS "scan_stats"			 /* not stored on disk */
+#define ZPOOL_CONFIG_REMOVAL_STATS "removal_stats"		 /* not stored on disk */
+#define ZPOOL_CONFIG_CHECKPOINT_STATS "checkpoint_stats" /* not on disk */
+#define ZPOOL_CONFIG_VDEV_STATS "vdev_stats"			 /* not stored on disk */
+#define ZPOOL_CONFIG_INDIRECT_SIZE "indirect_size"		 /* not stored on disk */
 
 /* container nvlist of extended stats */
-#define	ZPOOL_CONFIG_VDEV_STATS_EX	"vdev_stats_ex"
+#define ZPOOL_CONFIG_VDEV_STATS_EX "vdev_stats_ex"
 
 /* Active queue read/write stats */
-#define	ZPOOL_CONFIG_VDEV_SYNC_R_ACTIVE_QUEUE	"vdev_sync_r_active_queue"
-#define	ZPOOL_CONFIG_VDEV_SYNC_W_ACTIVE_QUEUE	"vdev_sync_w_active_queue"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_R_ACTIVE_QUEUE	"vdev_async_r_active_queue"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_W_ACTIVE_QUEUE	"vdev_async_w_active_queue"
-#define	ZPOOL_CONFIG_VDEV_SCRUB_ACTIVE_QUEUE	"vdev_async_scrub_active_queue"
-#define	ZPOOL_CONFIG_VDEV_TRIM_ACTIVE_QUEUE	"vdev_async_trim_active_queue"
+#define ZPOOL_CONFIG_VDEV_SYNC_R_ACTIVE_QUEUE "vdev_sync_r_active_queue"
+#define ZPOOL_CONFIG_VDEV_SYNC_W_ACTIVE_QUEUE "vdev_sync_w_active_queue"
+#define ZPOOL_CONFIG_VDEV_ASYNC_R_ACTIVE_QUEUE "vdev_async_r_active_queue"
+#define ZPOOL_CONFIG_VDEV_ASYNC_W_ACTIVE_QUEUE "vdev_async_w_active_queue"
+#define ZPOOL_CONFIG_VDEV_SCRUB_ACTIVE_QUEUE "vdev_async_scrub_active_queue"
+#define ZPOOL_CONFIG_VDEV_TRIM_ACTIVE_QUEUE "vdev_async_trim_active_queue"
 
 /* Queue sizes */
-#define	ZPOOL_CONFIG_VDEV_SYNC_R_PEND_QUEUE	"vdev_sync_r_pend_queue"
-#define	ZPOOL_CONFIG_VDEV_SYNC_W_PEND_QUEUE	"vdev_sync_w_pend_queue"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_R_PEND_QUEUE	"vdev_async_r_pend_queue"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_W_PEND_QUEUE	"vdev_async_w_pend_queue"
-#define	ZPOOL_CONFIG_VDEV_SCRUB_PEND_QUEUE	"vdev_async_scrub_pend_queue"
-#define	ZPOOL_CONFIG_VDEV_TRIM_PEND_QUEUE	"vdev_async_trim_pend_queue"
+#define ZPOOL_CONFIG_VDEV_SYNC_R_PEND_QUEUE "vdev_sync_r_pend_queue"
+#define ZPOOL_CONFIG_VDEV_SYNC_W_PEND_QUEUE "vdev_sync_w_pend_queue"
+#define ZPOOL_CONFIG_VDEV_ASYNC_R_PEND_QUEUE "vdev_async_r_pend_queue"
+#define ZPOOL_CONFIG_VDEV_ASYNC_W_PEND_QUEUE "vdev_async_w_pend_queue"
+#define ZPOOL_CONFIG_VDEV_SCRUB_PEND_QUEUE "vdev_async_scrub_pend_queue"
+#define ZPOOL_CONFIG_VDEV_TRIM_PEND_QUEUE "vdev_async_trim_pend_queue"
 
 /* Latency read/write histogram stats */
-#define	ZPOOL_CONFIG_VDEV_TOT_R_LAT_HISTO	"vdev_tot_r_lat_histo"
-#define	ZPOOL_CONFIG_VDEV_TOT_W_LAT_HISTO	"vdev_tot_w_lat_histo"
-#define	ZPOOL_CONFIG_VDEV_DISK_R_LAT_HISTO	"vdev_disk_r_lat_histo"
-#define	ZPOOL_CONFIG_VDEV_DISK_W_LAT_HISTO	"vdev_disk_w_lat_histo"
-#define	ZPOOL_CONFIG_VDEV_SYNC_R_LAT_HISTO	"vdev_sync_r_lat_histo"
-#define	ZPOOL_CONFIG_VDEV_SYNC_W_LAT_HISTO	"vdev_sync_w_lat_histo"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_R_LAT_HISTO	"vdev_async_r_lat_histo"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_W_LAT_HISTO	"vdev_async_w_lat_histo"
-#define	ZPOOL_CONFIG_VDEV_SCRUB_LAT_HISTO	"vdev_scrub_histo"
-#define	ZPOOL_CONFIG_VDEV_TRIM_LAT_HISTO	"vdev_trim_histo"
+#define ZPOOL_CONFIG_VDEV_TOT_R_LAT_HISTO "vdev_tot_r_lat_histo"
+#define ZPOOL_CONFIG_VDEV_TOT_W_LAT_HISTO "vdev_tot_w_lat_histo"
+#define ZPOOL_CONFIG_VDEV_DISK_R_LAT_HISTO "vdev_disk_r_lat_histo"
+#define ZPOOL_CONFIG_VDEV_DISK_W_LAT_HISTO "vdev_disk_w_lat_histo"
+#define ZPOOL_CONFIG_VDEV_SYNC_R_LAT_HISTO "vdev_sync_r_lat_histo"
+#define ZPOOL_CONFIG_VDEV_SYNC_W_LAT_HISTO "vdev_sync_w_lat_histo"
+#define ZPOOL_CONFIG_VDEV_ASYNC_R_LAT_HISTO "vdev_async_r_lat_histo"
+#define ZPOOL_CONFIG_VDEV_ASYNC_W_LAT_HISTO "vdev_async_w_lat_histo"
+#define ZPOOL_CONFIG_VDEV_SCRUB_LAT_HISTO "vdev_scrub_histo"
+#define ZPOOL_CONFIG_VDEV_TRIM_LAT_HISTO "vdev_trim_histo"
 
 /* Request size histograms */
-#define	ZPOOL_CONFIG_VDEV_SYNC_IND_R_HISTO	"vdev_sync_ind_r_histo"
-#define	ZPOOL_CONFIG_VDEV_SYNC_IND_W_HISTO	"vdev_sync_ind_w_histo"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_IND_R_HISTO	"vdev_async_ind_r_histo"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_IND_W_HISTO	"vdev_async_ind_w_histo"
-#define	ZPOOL_CONFIG_VDEV_IND_SCRUB_HISTO	"vdev_ind_scrub_histo"
-#define	ZPOOL_CONFIG_VDEV_IND_TRIM_HISTO	"vdev_ind_trim_histo"
-#define	ZPOOL_CONFIG_VDEV_SYNC_AGG_R_HISTO	"vdev_sync_agg_r_histo"
-#define	ZPOOL_CONFIG_VDEV_SYNC_AGG_W_HISTO	"vdev_sync_agg_w_histo"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_AGG_R_HISTO	"vdev_async_agg_r_histo"
-#define	ZPOOL_CONFIG_VDEV_ASYNC_AGG_W_HISTO	"vdev_async_agg_w_histo"
-#define	ZPOOL_CONFIG_VDEV_AGG_SCRUB_HISTO	"vdev_agg_scrub_histo"
-#define	ZPOOL_CONFIG_VDEV_AGG_TRIM_HISTO	"vdev_agg_trim_histo"
+#define ZPOOL_CONFIG_VDEV_SYNC_IND_R_HISTO "vdev_sync_ind_r_histo"
+#define ZPOOL_CONFIG_VDEV_SYNC_IND_W_HISTO "vdev_sync_ind_w_histo"
+#define ZPOOL_CONFIG_VDEV_ASYNC_IND_R_HISTO "vdev_async_ind_r_histo"
+#define ZPOOL_CONFIG_VDEV_ASYNC_IND_W_HISTO "vdev_async_ind_w_histo"
+#define ZPOOL_CONFIG_VDEV_IND_SCRUB_HISTO "vdev_ind_scrub_histo"
+#define ZPOOL_CONFIG_VDEV_IND_TRIM_HISTO "vdev_ind_trim_histo"
+#define ZPOOL_CONFIG_VDEV_SYNC_AGG_R_HISTO "vdev_sync_agg_r_histo"
+#define ZPOOL_CONFIG_VDEV_SYNC_AGG_W_HISTO "vdev_sync_agg_w_histo"
+#define ZPOOL_CONFIG_VDEV_ASYNC_AGG_R_HISTO "vdev_async_agg_r_histo"
+#define ZPOOL_CONFIG_VDEV_ASYNC_AGG_W_HISTO "vdev_async_agg_w_histo"
+#define ZPOOL_CONFIG_VDEV_AGG_SCRUB_HISTO "vdev_agg_scrub_histo"
+#define ZPOOL_CONFIG_VDEV_AGG_TRIM_HISTO "vdev_agg_trim_histo"
 
 /* Number of slow IOs */
-#define	ZPOOL_CONFIG_VDEV_SLOW_IOS		"vdev_slow_ios"
+#define ZPOOL_CONFIG_VDEV_SLOW_IOS "vdev_slow_ios"
 
 /* vdev enclosure sysfs path */
-#define	ZPOOL_CONFIG_VDEV_ENC_SYSFS_PATH	"vdev_enc_sysfs_path"
-
-#define	ZPOOL_CONFIG_WHOLE_DISK		"whole_disk"
-#define	ZPOOL_CONFIG_ERRCOUNT		"error_count"
-#define	ZPOOL_CONFIG_NOT_PRESENT	"not_present"
-#define	ZPOOL_CONFIG_SPARES		"spares"
-#define	ZPOOL_CONFIG_IS_SPARE		"is_spare"
-#define	ZPOOL_CONFIG_NPARITY		"nparity"
-#define	ZPOOL_CONFIG_HOSTID		"hostid"
-#define	ZPOOL_CONFIG_HOSTNAME		"hostname"
-#define	ZPOOL_CONFIG_LOADED_TIME	"initial_load_time"
-#define	ZPOOL_CONFIG_UNSPARE		"unspare"
-#define	ZPOOL_CONFIG_PHYS_PATH		"phys_path"
-#define	ZPOOL_CONFIG_IS_LOG		"is_log"
-#define	ZPOOL_CONFIG_L2CACHE		"l2cache"
-#define	ZPOOL_CONFIG_HOLE_ARRAY		"hole_array"
-#define	ZPOOL_CONFIG_VDEV_CHILDREN	"vdev_children"
-#define	ZPOOL_CONFIG_IS_HOLE		"is_hole"
-#define	ZPOOL_CONFIG_DDT_HISTOGRAM	"ddt_histogram"
-#define	ZPOOL_CONFIG_DDT_OBJ_STATS	"ddt_object_stats"
-#define	ZPOOL_CONFIG_DDT_STATS		"ddt_stats"
-#define	ZPOOL_CONFIG_SPLIT		"splitcfg"
-#define	ZPOOL_CONFIG_ORIG_GUID		"orig_guid"
-#define	ZPOOL_CONFIG_SPLIT_GUID		"split_guid"
-#define	ZPOOL_CONFIG_SPLIT_LIST		"guid_list"
-#define	ZPOOL_CONFIG_REMOVING		"removing"
-#define	ZPOOL_CONFIG_RESILVER_TXG	"resilver_txg"
-#define	ZPOOL_CONFIG_REBUILD_TXG	"rebuild_txg"
-#define	ZPOOL_CONFIG_COMMENT		"comment"
-#define	ZPOOL_CONFIG_SUSPENDED		"suspended"	/* not stored on disk */
-#define	ZPOOL_CONFIG_SUSPENDED_REASON	"suspended_reason"	/* not stored */
-#define	ZPOOL_CONFIG_TIMESTAMP		"timestamp"	/* not stored on disk */
-#define	ZPOOL_CONFIG_BOOTFS		"bootfs"	/* not stored on disk */
-#define	ZPOOL_CONFIG_MISSING_DEVICES	"missing_vdevs"	/* not stored on disk */
-#define	ZPOOL_CONFIG_LOAD_INFO		"load_info"	/* not stored on disk */
-#define	ZPOOL_CONFIG_REWIND_INFO	"rewind_info"	/* not stored on disk */
-#define	ZPOOL_CONFIG_UNSUP_FEAT		"unsup_feat"	/* not stored on disk */
-#define	ZPOOL_CONFIG_ENABLED_FEAT	"enabled_feat"	/* not stored on disk */
-#define	ZPOOL_CONFIG_CAN_RDONLY		"can_rdonly"	/* not stored on disk */
-#define	ZPOOL_CONFIG_FEATURES_FOR_READ	"features_for_read"
-#define	ZPOOL_CONFIG_FEATURE_STATS	"feature_stats"	/* not stored on disk */
-#define	ZPOOL_CONFIG_ERRATA		"errata"	/* not stored on disk */
-#define	ZPOOL_CONFIG_VDEV_TOP_ZAP	"com.delphix:vdev_zap_top"
-#define	ZPOOL_CONFIG_VDEV_LEAF_ZAP	"com.delphix:vdev_zap_leaf"
-#define	ZPOOL_CONFIG_HAS_PER_VDEV_ZAPS	"com.delphix:has_per_vdev_zaps"
-#define	ZPOOL_CONFIG_RESILVER_DEFER	"com.datto:resilver_defer"
-#define	ZPOOL_CONFIG_CACHEFILE		"cachefile"	/* not stored on disk */
-#define	ZPOOL_CONFIG_MMP_STATE		"mmp_state"	/* not stored on disk */
-#define	ZPOOL_CONFIG_MMP_TXG		"mmp_txg"	/* not stored on disk */
-#define	ZPOOL_CONFIG_MMP_SEQ		"mmp_seq"	/* not stored on disk */
-#define	ZPOOL_CONFIG_MMP_HOSTNAME	"mmp_hostname"	/* not stored on disk */
-#define	ZPOOL_CONFIG_MMP_HOSTID		"mmp_hostid"	/* not stored on disk */
-#define	ZPOOL_CONFIG_ALLOCATION_BIAS	"alloc_bias"	/* not stored on disk */
-#define	ZPOOL_CONFIG_EXPANSION_TIME	"expansion_time"	/* not stored */
-#define	ZPOOL_CONFIG_REBUILD_STATS	"org.openzfs:rebuild_stats"
-#define	ZPOOL_CONFIG_COMPATIBILITY	"compatibility"
+#define ZPOOL_CONFIG_VDEV_ENC_SYSFS_PATH "vdev_enc_sysfs_path"
+
+#define ZPOOL_CONFIG_WHOLE_DISK "whole_disk"
+#define ZPOOL_CONFIG_ERRCOUNT "error_count"
+#define ZPOOL_CONFIG_NOT_PRESENT "not_present"
+#define ZPOOL_CONFIG_SPARES "spares"
+#define ZPOOL_CONFIG_IS_SPARE "is_spare"
+#define ZPOOL_CONFIG_NPARITY "nparity"
+#define ZPOOL_CONFIG_HOSTID "hostid"
+#define ZPOOL_CONFIG_HOSTNAME "hostname"
+#define ZPOOL_CONFIG_LOADED_TIME "initial_load_time"
+#define ZPOOL_CONFIG_UNSPARE "unspare"
+#define ZPOOL_CONFIG_PHYS_PATH "phys_path"
+#define ZPOOL_CONFIG_IS_LOG "is_log"
+#define ZPOOL_CONFIG_L2CACHE "l2cache"
+#define ZPOOL_CONFIG_HOLE_ARRAY "hole_array"
+#define ZPOOL_CONFIG_VDEV_CHILDREN "vdev_children"
+#define ZPOOL_CONFIG_IS_HOLE "is_hole"
+#define ZPOOL_CONFIG_DDT_HISTOGRAM "ddt_histogram"
+#define ZPOOL_CONFIG_DDT_OBJ_STATS "ddt_object_stats"
+#define ZPOOL_CONFIG_DDT_STATS "ddt_stats"
+#define ZPOOL_CONFIG_SPLIT "splitcfg"
+#define ZPOOL_CONFIG_ORIG_GUID "orig_guid"
+#define ZPOOL_CONFIG_SPLIT_GUID "split_guid"
+#define ZPOOL_CONFIG_SPLIT_LIST "guid_list"
+#define ZPOOL_CONFIG_REMOVING "removing"
+#define ZPOOL_CONFIG_RESILVER_TXG "resilver_txg"
+#define ZPOOL_CONFIG_REBUILD_TXG "rebuild_txg"
+#define ZPOOL_CONFIG_COMMENT "comment"
+#define ZPOOL_CONFIG_SUSPENDED "suspended"				 /* not stored on disk */
+#define ZPOOL_CONFIG_SUSPENDED_REASON "suspended_reason" /* not stored */
+#define ZPOOL_CONFIG_TIMESTAMP "timestamp"				 /* not stored on disk */
+#define ZPOOL_CONFIG_BOOTFS "bootfs"					 /* not stored on disk */
+#define ZPOOL_CONFIG_MISSING_DEVICES "missing_vdevs"	 /* not stored on disk */
+#define ZPOOL_CONFIG_LOAD_INFO "load_info"				 /* not stored on disk */
+#define ZPOOL_CONFIG_REWIND_INFO "rewind_info"			 /* not stored on disk */
+#define ZPOOL_CONFIG_UNSUP_FEAT "unsup_feat"			 /* not stored on disk */
+#define ZPOOL_CONFIG_ENABLED_FEAT "enabled_feat"		 /* not stored on disk */
+#define ZPOOL_CONFIG_CAN_RDONLY "can_rdonly"			 /* not stored on disk */
+#define ZPOOL_CONFIG_FEATURES_FOR_READ "features_for_read"
+#define ZPOOL_CONFIG_FEATURE_STATS "feature_stats" /* not stored on disk */
+#define ZPOOL_CONFIG_ERRATA "errata"			   /* not stored on disk */
+#define ZPOOL_CONFIG_VDEV_TOP_ZAP "com.delphix:vdev_zap_top"
+#define ZPOOL_CONFIG_VDEV_LEAF_ZAP "com.delphix:vdev_zap_leaf"
+#define ZPOOL_CONFIG_HAS_PER_VDEV_ZAPS "com.delphix:has_per_vdev_zaps"
+#define ZPOOL_CONFIG_RESILVER_DEFER "com.datto:resilver_defer"
+#define ZPOOL_CONFIG_CACHEFILE "cachefile"			 /* not stored on disk */
+#define ZPOOL_CONFIG_MMP_STATE "mmp_state"			 /* not stored on disk */
+#define ZPOOL_CONFIG_MMP_TXG "mmp_txg"				 /* not stored on disk */
+#define ZPOOL_CONFIG_MMP_SEQ "mmp_seq"				 /* not stored on disk */
+#define ZPOOL_CONFIG_MMP_HOSTNAME "mmp_hostname"	 /* not stored on disk */
+#define ZPOOL_CONFIG_MMP_HOSTID "mmp_hostid"		 /* not stored on disk */
+#define ZPOOL_CONFIG_ALLOCATION_BIAS "alloc_bias"	 /* not stored on disk */
+#define ZPOOL_CONFIG_EXPANSION_TIME "expansion_time" /* not stored */
+#define ZPOOL_CONFIG_REBUILD_STATS "org.openzfs:rebuild_stats"
+#define ZPOOL_CONFIG_COMPATIBILITY "compatibility"
 
 /*
  * The persistent vdev state is stored as separate values rather than a single
  * 'vdev_state' entry.  This is because a device can be in multiple states, such
  * as offline and degraded.
  */
-#define	ZPOOL_CONFIG_OFFLINE		"offline"
-#define	ZPOOL_CONFIG_FAULTED		"faulted"
-#define	ZPOOL_CONFIG_DEGRADED		"degraded"
-#define	ZPOOL_CONFIG_REMOVED		"removed"
-#define	ZPOOL_CONFIG_FRU		"fru"
-#define	ZPOOL_CONFIG_AUX_STATE		"aux_state"
+#define ZPOOL_CONFIG_OFFLINE "offline"
+#define ZPOOL_CONFIG_FAULTED "faulted"
+#define ZPOOL_CONFIG_DEGRADED "degraded"
+#define ZPOOL_CONFIG_REMOVED "removed"
+#define ZPOOL_CONFIG_FRU "fru"
+#define ZPOOL_CONFIG_AUX_STATE "aux_state"
 
 /* Pool load policy parameters */
-#define	ZPOOL_LOAD_POLICY		"load-policy"
-#define	ZPOOL_LOAD_REWIND_POLICY	"load-rewind-policy"
-#define	ZPOOL_LOAD_REQUEST_TXG		"load-request-txg"
-#define	ZPOOL_LOAD_META_THRESH		"load-meta-thresh"
-#define	ZPOOL_LOAD_DATA_THRESH		"load-data-thresh"
+#define ZPOOL_LOAD_POLICY "load-policy"
+#define ZPOOL_LOAD_REWIND_POLICY "load-rewind-policy"
+#define ZPOOL_LOAD_REQUEST_TXG "load-request-txg"
+#define ZPOOL_LOAD_META_THRESH "load-meta-thresh"
+#define ZPOOL_LOAD_DATA_THRESH "load-data-thresh"
 
 /* Rewind data discovered */
-#define	ZPOOL_CONFIG_LOAD_TIME		"rewind_txg_ts"
-#define	ZPOOL_CONFIG_LOAD_META_ERRORS	"verify_meta_errors"
-#define	ZPOOL_CONFIG_LOAD_DATA_ERRORS	"verify_data_errors"
-#define	ZPOOL_CONFIG_REWIND_TIME	"seconds_of_rewind"
+#define ZPOOL_CONFIG_LOAD_TIME "rewind_txg_ts"
+#define ZPOOL_CONFIG_LOAD_META_ERRORS "verify_meta_errors"
+#define ZPOOL_CONFIG_LOAD_DATA_ERRORS "verify_data_errors"
+#define ZPOOL_CONFIG_REWIND_TIME "seconds_of_rewind"
 
 /* dRAID configuration */
-#define	ZPOOL_CONFIG_DRAID_NDATA	"draid_ndata"
-#define	ZPOOL_CONFIG_DRAID_NSPARES	"draid_nspares"
-#define	ZPOOL_CONFIG_DRAID_NGROUPS	"draid_ngroups"
-
-#define	VDEV_TYPE_ROOT			"root"
-#define	VDEV_TYPE_MIRROR		"mirror"
-#define	VDEV_TYPE_REPLACING		"replacing"
-#define	VDEV_TYPE_RAIDZ			"raidz"
-#define	VDEV_TYPE_DRAID			"draid"
-#define	VDEV_TYPE_DRAID_SPARE		"dspare"
-#define	VDEV_TYPE_DISK			"disk"
-#define	VDEV_TYPE_FILE			"file"
-#define	VDEV_TYPE_MISSING		"missing"
-#define	VDEV_TYPE_HOLE			"hole"
-#define	VDEV_TYPE_SPARE			"spare"
-#define	VDEV_TYPE_LOG			"log"
-#define	VDEV_TYPE_L2CACHE		"l2cache"
-#define	VDEV_TYPE_INDIRECT		"indirect"
-
-#define	VDEV_RAIDZ_MAXPARITY		3
-
-#define	VDEV_DRAID_MAXPARITY		3
-#define	VDEV_DRAID_MIN_CHILDREN		2
-#define	VDEV_DRAID_MAX_CHILDREN		UINT8_MAX
+#define ZPOOL_CONFIG_DRAID_NDATA "draid_ndata"
+#define ZPOOL_CONFIG_DRAID_NSPARES "draid_nspares"
+#define ZPOOL_CONFIG_DRAID_NGROUPS "draid_ngroups"
+
+#define VDEV_TYPE_ROOT "root"
+#define VDEV_TYPE_MIRROR "mirror"
+#define VDEV_TYPE_MY_MIRROR "my_mirror"
+#define VDEV_TYPE_REPLACING "replacing"
+#define VDEV_TYPE_RAIDZ "raidz"
+#define VDEV_TYPE_MY_RAIDZ "my_raidz"
+#define VDEV_TYPE_DRAID "draid"
+#define VDEV_TYPE_DRAID_SPARE "dspare"
+#define VDEV_TYPE_DISK "disk"
+#define VDEV_TYPE_FILE "file"
+#define VDEV_TYPE_MISSING "missing"
+#define VDEV_TYPE_HOLE "hole"
+#define VDEV_TYPE_SPARE "spare"
+#define VDEV_TYPE_LOG "log"
+#define VDEV_TYPE_L2CACHE "l2cache"
+#define VDEV_TYPE_INDIRECT "indirect"
+
+#define VDEV_RAIDZ_MAXPARITY 3
+
+#define VDEV_DRAID_MAXPARITY 3
+#define VDEV_DRAID_MIN_CHILDREN 2
+#define VDEV_DRAID_MAX_CHILDREN UINT8_MAX
 
 /* VDEV_TOP_ZAP_* are used in top-level vdev ZAP objects. */
-#define	VDEV_TOP_ZAP_INDIRECT_OBSOLETE_SM \
+#define VDEV_TOP_ZAP_INDIRECT_OBSOLETE_SM \
 	"com.delphix:indirect_obsolete_sm"
-#define	VDEV_TOP_ZAP_OBSOLETE_COUNTS_ARE_PRECISE \
+#define VDEV_TOP_ZAP_OBSOLETE_COUNTS_ARE_PRECISE \
 	"com.delphix:obsolete_counts_are_precise"
-#define	VDEV_TOP_ZAP_POOL_CHECKPOINT_SM \
+#define VDEV_TOP_ZAP_POOL_CHECKPOINT_SM \
 	"com.delphix:pool_checkpoint_sm"
-#define	VDEV_TOP_ZAP_MS_UNFLUSHED_PHYS_TXGS \
+#define VDEV_TOP_ZAP_MS_UNFLUSHED_PHYS_TXGS \
 	"com.delphix:ms_unflushed_phys_txgs"
 
-#define	VDEV_TOP_ZAP_VDEV_REBUILD_PHYS \
+#define VDEV_TOP_ZAP_VDEV_REBUILD_PHYS \
 	"org.openzfs:vdev_rebuild"
 
-#define	VDEV_TOP_ZAP_ALLOCATION_BIAS \
+#define VDEV_TOP_ZAP_ALLOCATION_BIAS \
 	"org.zfsonlinux:allocation_bias"
 
 /* vdev metaslab allocation bias */
-#define	VDEV_ALLOC_BIAS_LOG		"log"
-#define	VDEV_ALLOC_BIAS_SPECIAL		"special"
-#define	VDEV_ALLOC_BIAS_DEDUP		"dedup"
+#define VDEV_ALLOC_BIAS_LOG "log"
+#define VDEV_ALLOC_BIAS_SPECIAL "special"
+#define VDEV_ALLOC_BIAS_DEDUP "dedup"
 
 /* vdev initialize state */
-#define	VDEV_LEAF_ZAP_INITIALIZE_LAST_OFFSET	\
+#define VDEV_LEAF_ZAP_INITIALIZE_LAST_OFFSET \
 	"com.delphix:next_offset_to_initialize"
-#define	VDEV_LEAF_ZAP_INITIALIZE_STATE	\
+#define VDEV_LEAF_ZAP_INITIALIZE_STATE \
 	"com.delphix:vdev_initialize_state"
-#define	VDEV_LEAF_ZAP_INITIALIZE_ACTION_TIME	\
+#define VDEV_LEAF_ZAP_INITIALIZE_ACTION_TIME \
 	"com.delphix:vdev_initialize_action_time"
 
 /* vdev TRIM state */
-#define	VDEV_LEAF_ZAP_TRIM_LAST_OFFSET	\
+#define VDEV_LEAF_ZAP_TRIM_LAST_OFFSET \
 	"org.zfsonlinux:next_offset_to_trim"
-#define	VDEV_LEAF_ZAP_TRIM_STATE	\
+#define VDEV_LEAF_ZAP_TRIM_STATE \
 	"org.zfsonlinux:vdev_trim_state"
-#define	VDEV_LEAF_ZAP_TRIM_ACTION_TIME	\
+#define VDEV_LEAF_ZAP_TRIM_ACTION_TIME \
 	"org.zfsonlinux:vdev_trim_action_time"
-#define	VDEV_LEAF_ZAP_TRIM_RATE		\
+#define VDEV_LEAF_ZAP_TRIM_RATE \
 	"org.zfsonlinux:vdev_trim_rate"
-#define	VDEV_LEAF_ZAP_TRIM_PARTIAL	\
+#define VDEV_LEAF_ZAP_TRIM_PARTIAL \
 	"org.zfsonlinux:vdev_trim_partial"
-#define	VDEV_LEAF_ZAP_TRIM_SECURE	\
+#define VDEV_LEAF_ZAP_TRIM_SECURE \
 	"org.zfsonlinux:vdev_trim_secure"
 
 /*
  * This is needed in userland to report the minimum necessary device size.
  */
-#define	SPA_MINDEVSIZE		(64ULL << 20)
+#define SPA_MINDEVSIZE (64ULL << 20)
 
 /*
  * Set if the fragmentation has not yet been calculated. This can happen
  * because the space maps have not been upgraded or the histogram feature
  * is not enabled.
  */
-#define	ZFS_FRAG_INVALID	UINT64_MAX
+#define ZFS_FRAG_INVALID UINT64_MAX
 
 /*
  * The location of the pool configuration repository, shared between kernel and
  * userland.
  */
-#define	ZPOOL_CACHE_BOOT	"/boot/zfs/zpool.cache"
-#define	ZPOOL_CACHE		"/etc/zfs/zpool.cache"
+#define ZPOOL_CACHE_BOOT "/boot/zfs/zpool.cache"
+#define ZPOOL_CACHE "/etc/zfs/zpool.cache"
 /*
  * Settings for zpool compatibility features files
  */
-#define	ZPOOL_SYSCONF_COMPAT_D	SYSCONFDIR "/zfs/compatibility.d"
-#define	ZPOOL_DATA_COMPAT_D	PKGDATADIR "/compatibility.d"
-#define	ZPOOL_COMPAT_MAXSIZE	16384
+#define ZPOOL_SYSCONF_COMPAT_D SYSCONFDIR "/zfs/compatibility.d"
+#define ZPOOL_DATA_COMPAT_D PKGDATADIR "/compatibility.d"
+#define ZPOOL_COMPAT_MAXSIZE 16384
 
 /*
  * Hard-wired compatibility settings
  */
-#define	ZPOOL_COMPAT_LEGACY	"legacy"
-#define	ZPOOL_COMPAT_OFF	"off"
+#define ZPOOL_COMPAT_LEGACY "legacy"
+#define ZPOOL_COMPAT_OFF "off"
 
-/*
- * vdev states are ordered from least to most healthy.
- * A vdev that's CANT_OPEN or below is considered unusable.
- */
-typedef enum vdev_state {
-	VDEV_STATE_UNKNOWN = 0,	/* Uninitialized vdev			*/
-	VDEV_STATE_CLOSED,	/* Not currently open			*/
-	VDEV_STATE_OFFLINE,	/* Not allowed to open			*/
-	VDEV_STATE_REMOVED,	/* Explicitly removed from system	*/
-	VDEV_STATE_CANT_OPEN,	/* Tried to open, but failed		*/
-	VDEV_STATE_FAULTED,	/* External request to fault device	*/
-	VDEV_STATE_DEGRADED,	/* Replicated vdev with unhealthy kids	*/
-	VDEV_STATE_HEALTHY	/* Presumed good			*/
-} vdev_state_t;
-
-#define	VDEV_STATE_ONLINE	VDEV_STATE_HEALTHY
+	/*
+	 * vdev states are ordered from least to most healthy.
+	 * A vdev that's CANT_OPEN or below is considered unusable.
+	 */
+	typedef enum vdev_state
+	{
+		VDEV_STATE_UNKNOWN = 0, /* Uninitialized vdev			*/
+		VDEV_STATE_CLOSED,		/* Not currently open			*/
+		VDEV_STATE_OFFLINE,		/* Not allowed to open			*/
+		VDEV_STATE_REMOVED,		/* Explicitly removed from system	*/
+		VDEV_STATE_CANT_OPEN,	/* Tried to open, but failed		*/
+		VDEV_STATE_FAULTED,		/* External request to fault device	*/
+		VDEV_STATE_DEGRADED,	/* Replicated vdev with unhealthy kids	*/
+		VDEV_STATE_HEALTHY		/* Presumed good			*/
+	} vdev_state_t;
+
+#define VDEV_STATE_ONLINE VDEV_STATE_HEALTHY
 
-/*
- * vdev aux states.  When a vdev is in the CANT_OPEN state, the aux field
- * of the vdev stats structure uses these constants to distinguish why.
- */
-typedef enum vdev_aux {
-	VDEV_AUX_NONE,		/* no error				*/
-	VDEV_AUX_OPEN_FAILED,	/* ldi_open_*() or vn_open() failed	*/
-	VDEV_AUX_CORRUPT_DATA,	/* bad label or disk contents		*/
-	VDEV_AUX_NO_REPLICAS,	/* insufficient number of replicas	*/
-	VDEV_AUX_BAD_GUID_SUM,	/* vdev guid sum doesn't match		*/
-	VDEV_AUX_TOO_SMALL,	/* vdev size is too small		*/
-	VDEV_AUX_BAD_LABEL,	/* the label is OK but invalid		*/
-	VDEV_AUX_VERSION_NEWER,	/* on-disk version is too new		*/
-	VDEV_AUX_VERSION_OLDER,	/* on-disk version is too old		*/
-	VDEV_AUX_UNSUP_FEAT,	/* unsupported features			*/
-	VDEV_AUX_SPARED,	/* hot spare used in another pool	*/
-	VDEV_AUX_ERR_EXCEEDED,	/* too many errors			*/
-	VDEV_AUX_IO_FAILURE,	/* experienced I/O failure		*/
-	VDEV_AUX_BAD_LOG,	/* cannot read log chain(s)		*/
-	VDEV_AUX_EXTERNAL,	/* external diagnosis or forced fault	*/
-	VDEV_AUX_SPLIT_POOL,	/* vdev was split off into another pool	*/
-	VDEV_AUX_BAD_ASHIFT,	/* vdev ashift is invalid		*/
-	VDEV_AUX_EXTERNAL_PERSIST,	/* persistent forced fault	*/
-	VDEV_AUX_ACTIVE,	/* vdev active on a different host	*/
-	VDEV_AUX_CHILDREN_OFFLINE, /* all children are offline		*/
-	VDEV_AUX_ASHIFT_TOO_BIG, /* vdev's min block size is too large   */
-} vdev_aux_t;
+	/*
+	 * vdev aux states.  When a vdev is in the CANT_OPEN state, the aux field
+	 * of the vdev stats structure uses these constants to distinguish why.
+	 */
+	typedef enum vdev_aux
+	{
+		VDEV_AUX_NONE,			   /* no error				*/
+		VDEV_AUX_OPEN_FAILED,	   /* ldi_open_*() or vn_open() failed	*/
+		VDEV_AUX_CORRUPT_DATA,	   /* bad label or disk contents		*/
+		VDEV_AUX_NO_REPLICAS,	   /* insufficient number of replicas	*/
+		VDEV_AUX_BAD_GUID_SUM,	   /* vdev guid sum doesn't match		*/
+		VDEV_AUX_TOO_SMALL,		   /* vdev size is too small		*/
+		VDEV_AUX_BAD_LABEL,		   /* the label is OK but invalid		*/
+		VDEV_AUX_VERSION_NEWER,	   /* on-disk version is too new		*/
+		VDEV_AUX_VERSION_OLDER,	   /* on-disk version is too old		*/
+		VDEV_AUX_UNSUP_FEAT,	   /* unsupported features			*/
+		VDEV_AUX_SPARED,		   /* hot spare used in another pool	*/
+		VDEV_AUX_ERR_EXCEEDED,	   /* too many errors			*/
+		VDEV_AUX_IO_FAILURE,	   /* experienced I/O failure		*/
+		VDEV_AUX_BAD_LOG,		   /* cannot read log chain(s)		*/
+		VDEV_AUX_EXTERNAL,		   /* external diagnosis or forced fault	*/
+		VDEV_AUX_SPLIT_POOL,	   /* vdev was split off into another pool	*/
+		VDEV_AUX_BAD_ASHIFT,	   /* vdev ashift is invalid		*/
+		VDEV_AUX_EXTERNAL_PERSIST, /* persistent forced fault	*/
+		VDEV_AUX_ACTIVE,		   /* vdev active on a different host	*/
+		VDEV_AUX_CHILDREN_OFFLINE, /* all children are offline		*/
+		VDEV_AUX_ASHIFT_TOO_BIG,   /* vdev's min block size is too large   */
+	} vdev_aux_t;
 
-/*
- * pool state.  The following states are written to disk as part of the normal
- * SPA lifecycle: ACTIVE, EXPORTED, DESTROYED, SPARE, L2CACHE.  The remaining
- * states are software abstractions used at various levels to communicate
- * pool state.
- */
-typedef enum pool_state {
-	POOL_STATE_ACTIVE = 0,		/* In active use		*/
-	POOL_STATE_EXPORTED,		/* Explicitly exported		*/
-	POOL_STATE_DESTROYED,		/* Explicitly destroyed		*/
-	POOL_STATE_SPARE,		/* Reserved for hot spare use	*/
-	POOL_STATE_L2CACHE,		/* Level 2 ARC device		*/
-	POOL_STATE_UNINITIALIZED,	/* Internal spa_t state		*/
-	POOL_STATE_UNAVAIL,		/* Internal libzfs state	*/
-	POOL_STATE_POTENTIALLY_ACTIVE	/* Internal libzfs state	*/
-} pool_state_t;
+	/*
+	 * pool state.  The following states are written to disk as part of the normal
+	 * SPA lifecycle: ACTIVE, EXPORTED, DESTROYED, SPARE, L2CACHE.  The remaining
+	 * states are software abstractions used at various levels to communicate
+	 * pool state.
+	 */
+	typedef enum pool_state
+	{
+		POOL_STATE_ACTIVE = 0,		  /* In active use		*/
+		POOL_STATE_EXPORTED,		  /* Explicitly exported		*/
+		POOL_STATE_DESTROYED,		  /* Explicitly destroyed		*/
+		POOL_STATE_SPARE,			  /* Reserved for hot spare use	*/
+		POOL_STATE_L2CACHE,			  /* Level 2 ARC device		*/
+		POOL_STATE_UNINITIALIZED,	  /* Internal spa_t state		*/
+		POOL_STATE_UNAVAIL,			  /* Internal libzfs state	*/
+		POOL_STATE_POTENTIALLY_ACTIVE /* Internal libzfs state	*/
+	} pool_state_t;
 
-/*
- * mmp state. The following states provide additional detail describing
- * why a pool couldn't be safely imported.
- */
-typedef enum mmp_state {
-	MMP_STATE_ACTIVE = 0,		/* In active use		*/
-	MMP_STATE_INACTIVE,		/* Inactive and safe to import	*/
-	MMP_STATE_NO_HOSTID		/* System hostid is not set	*/
-} mmp_state_t;
+	/*
+	 * mmp state. The following states provide additional detail describing
+	 * why a pool couldn't be safely imported.
+	 */
+	typedef enum mmp_state
+	{
+		MMP_STATE_ACTIVE = 0, /* In active use		*/
+		MMP_STATE_INACTIVE,	  /* Inactive and safe to import	*/
+		MMP_STATE_NO_HOSTID	  /* System hostid is not set	*/
+	} mmp_state_t;
 
-/*
- * Scan Functions.
- */
-typedef enum pool_scan_func {
-	POOL_SCAN_NONE,
-	POOL_SCAN_SCRUB,
-	POOL_SCAN_RESILVER,
-	POOL_SCAN_FUNCS
-} pool_scan_func_t;
+	/*
+	 * Scan Functions.
+	 */
+	typedef enum pool_scan_func
+	{
+		POOL_SCAN_NONE,
+		POOL_SCAN_SCRUB,
+		POOL_SCAN_RESILVER,
+		POOL_SCAN_FUNCS
+	} pool_scan_func_t;
 
-/*
- * Used to control scrub pause and resume.
- */
-typedef enum pool_scrub_cmd {
-	POOL_SCRUB_NORMAL = 0,
-	POOL_SCRUB_PAUSE,
-	POOL_SCRUB_FLAGS_END
-} pool_scrub_cmd_t;
-
-typedef enum {
-	CS_NONE,
-	CS_CHECKPOINT_EXISTS,
-	CS_CHECKPOINT_DISCARDING,
-	CS_NUM_STATES
-} checkpoint_state_t;
-
-typedef struct pool_checkpoint_stat {
-	uint64_t pcs_state;		/* checkpoint_state_t */
-	uint64_t pcs_start_time;	/* time checkpoint/discard started */
-	uint64_t pcs_space;		/* checkpointed space */
-} pool_checkpoint_stat_t;
+	/*
+	 * Used to control scrub pause and resume.
+	 */
+	typedef enum pool_scrub_cmd
+	{
+		POOL_SCRUB_NORMAL = 0,
+		POOL_SCRUB_PAUSE,
+		POOL_SCRUB_FLAGS_END
+	} pool_scrub_cmd_t;
+
+	typedef enum
+	{
+		CS_NONE,
+		CS_CHECKPOINT_EXISTS,
+		CS_CHECKPOINT_DISCARDING,
+		CS_NUM_STATES
+	} checkpoint_state_t;
+
+	typedef struct pool_checkpoint_stat
+	{
+		uint64_t pcs_state;		 /* checkpoint_state_t */
+		uint64_t pcs_start_time; /* time checkpoint/discard started */
+		uint64_t pcs_space;		 /* checkpointed space */
+	} pool_checkpoint_stat_t;
 
-/*
- * ZIO types.  Needed to interpret vdev statistics below.
- */
-typedef enum zio_type {
-	ZIO_TYPE_NULL = 0,
-	ZIO_TYPE_READ,
-	ZIO_TYPE_WRITE,
-	ZIO_TYPE_FREE,
-	ZIO_TYPE_CLAIM,
-	ZIO_TYPE_IOCTL,
-	ZIO_TYPE_TRIM,
-	ZIO_TYPES
-} zio_type_t;
+	/*
+	 * ZIO types.  Needed to interpret vdev statistics below.
+	 */
+	typedef enum zio_type
+	{
+		ZIO_TYPE_NULL = 0,
+		ZIO_TYPE_READ,
+		ZIO_TYPE_WRITE,
+		ZIO_TYPE_FREE,
+		ZIO_TYPE_CLAIM,
+		ZIO_TYPE_IOCTL,
+		ZIO_TYPE_TRIM,
+		ZIO_TYPE_MLEC_WRITE_DATA,
+		ZIO_TYPES
+	} zio_type_t;
 
-/*
- * Pool statistics.  Note: all fields should be 64-bit because this
- * is passed between kernel and userland as an nvlist uint64 array.
- */
-typedef struct pool_scan_stat {
-	/* values stored on disk */
-	uint64_t	pss_func;	/* pool_scan_func_t */
-	uint64_t	pss_state;	/* dsl_scan_state_t */
-	uint64_t	pss_start_time;	/* scan start time */
-	uint64_t	pss_end_time;	/* scan end time */
-	uint64_t	pss_to_examine;	/* total bytes to scan */
-	uint64_t	pss_examined;	/* total bytes located by scanner */
-	uint64_t	pss_to_process; /* total bytes to process */
-	uint64_t	pss_processed;	/* total processed bytes */
-	uint64_t	pss_errors;	/* scan errors	*/
-
-	/* values not stored on disk */
-	uint64_t	pss_pass_exam; /* examined bytes per scan pass */
-	uint64_t	pss_pass_start;	/* start time of a scan pass */
-	uint64_t	pss_pass_scrub_pause; /* pause time of a scrub pass */
-	/* cumulative time scrub spent paused, needed for rate calculation */
-	uint64_t	pss_pass_scrub_spent_paused;
-	uint64_t	pss_pass_issued; /* issued bytes per scan pass */
-	uint64_t	pss_issued;	/* total bytes checked by scanner */
-} pool_scan_stat_t;
-
-typedef struct pool_removal_stat {
-	uint64_t prs_state; /* dsl_scan_state_t */
-	uint64_t prs_removing_vdev;
-	uint64_t prs_start_time;
-	uint64_t prs_end_time;
-	uint64_t prs_to_copy; /* bytes that need to be copied */
-	uint64_t prs_copied; /* bytes copied so far */
 	/*
-	 * bytes of memory used for indirect mappings.
-	 * This includes all removed vdevs.
+	 * Pool statistics.  Note: all fields should be 64-bit because this
+	 * is passed between kernel and userland as an nvlist uint64 array.
 	 */
-	uint64_t prs_mapping_memory;
-} pool_removal_stat_t;
-
-typedef enum dsl_scan_state {
-	DSS_NONE,
-	DSS_SCANNING,
-	DSS_FINISHED,
-	DSS_CANCELED,
-	DSS_NUM_STATES
-} dsl_scan_state_t;
-
-typedef struct vdev_rebuild_stat {
-	uint64_t vrs_state;		/* vdev_rebuild_state_t */
-	uint64_t vrs_start_time;	/* time_t */
-	uint64_t vrs_end_time;		/* time_t */
-	uint64_t vrs_scan_time_ms;	/* total run time (millisecs) */
-	uint64_t vrs_bytes_scanned;	/* allocated bytes scanned */
-	uint64_t vrs_bytes_issued;	/* read bytes issued */
-	uint64_t vrs_bytes_rebuilt;	/* rebuilt bytes */
-	uint64_t vrs_bytes_est;		/* total bytes to scan */
-	uint64_t vrs_errors;		/* scanning errors */
-	uint64_t vrs_pass_time_ms;	/* pass run time (millisecs) */
-	uint64_t vrs_pass_bytes_scanned; /* bytes scanned since start/resume */
-	uint64_t vrs_pass_bytes_issued;	/* bytes rebuilt since start/resume */
-} vdev_rebuild_stat_t;
+	typedef struct pool_scan_stat
+	{
+		/* values stored on disk */
+		uint64_t pss_func;		 /* pool_scan_func_t */
+		uint64_t pss_state;		 /* dsl_scan_state_t */
+		uint64_t pss_start_time; /* scan start time */
+		uint64_t pss_end_time;	 /* scan end time */
+		uint64_t pss_to_examine; /* total bytes to scan */
+		uint64_t pss_examined;	 /* total bytes located by scanner */
+		uint64_t pss_to_process; /* total bytes to process */
+		uint64_t pss_processed;	 /* total processed bytes */
+		uint64_t pss_errors;	 /* scan errors	*/
+
+		/* values not stored on disk */
+		uint64_t pss_pass_exam;		   /* examined bytes per scan pass */
+		uint64_t pss_pass_start;	   /* start time of a scan pass */
+		uint64_t pss_pass_scrub_pause; /* pause time of a scrub pass */
+		/* cumulative time scrub spent paused, needed for rate calculation */
+		uint64_t pss_pass_scrub_spent_paused;
+		uint64_t pss_pass_issued; /* issued bytes per scan pass */
+		uint64_t pss_issued;	  /* total bytes checked by scanner */
+	} pool_scan_stat_t;
+
+	typedef struct pool_removal_stat
+	{
+		uint64_t prs_state; /* dsl_scan_state_t */
+		uint64_t prs_removing_vdev;
+		uint64_t prs_start_time;
+		uint64_t prs_end_time;
+		uint64_t prs_to_copy; /* bytes that need to be copied */
+		uint64_t prs_copied;  /* bytes copied so far */
+		/*
+		 * bytes of memory used for indirect mappings.
+		 * This includes all removed vdevs.
+		 */
+		uint64_t prs_mapping_memory;
+	} pool_removal_stat_t;
+
+	typedef enum dsl_scan_state
+	{
+		DSS_NONE,
+		DSS_SCANNING,
+		DSS_FINISHED,
+		DSS_CANCELED,
+		DSS_NUM_STATES
+	} dsl_scan_state_t;
+
+	typedef struct vdev_rebuild_stat
+	{
+		uint64_t vrs_state;				 /* vdev_rebuild_state_t */
+		uint64_t vrs_start_time;		 /* time_t */
+		uint64_t vrs_end_time;			 /* time_t */
+		uint64_t vrs_scan_time_ms;		 /* total run time (millisecs) */
+		uint64_t vrs_bytes_scanned;		 /* allocated bytes scanned */
+		uint64_t vrs_bytes_issued;		 /* read bytes issued */
+		uint64_t vrs_bytes_rebuilt;		 /* rebuilt bytes */
+		uint64_t vrs_bytes_est;			 /* total bytes to scan */
+		uint64_t vrs_errors;			 /* scanning errors */
+		uint64_t vrs_pass_time_ms;		 /* pass run time (millisecs) */
+		uint64_t vrs_pass_bytes_scanned; /* bytes scanned since start/resume */
+		uint64_t vrs_pass_bytes_issued;	 /* bytes rebuilt since start/resume */
+	} vdev_rebuild_stat_t;
 
-/*
- * Errata described by https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER.
- * The ordering of this enum must be maintained to ensure the errata identifiers
- * map to the correct documentation.  New errata may only be appended to the
- * list and must contain corresponding documentation at the above link.
- */
-typedef enum zpool_errata {
-	ZPOOL_ERRATA_NONE,
-	ZPOOL_ERRATA_ZOL_2094_SCRUB,
-	ZPOOL_ERRATA_ZOL_2094_ASYNC_DESTROY,
-	ZPOOL_ERRATA_ZOL_6845_ENCRYPTION,
-	ZPOOL_ERRATA_ZOL_8308_ENCRYPTION,
-} zpool_errata_t;
+	/*
+	 * Errata described by https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER.
+	 * The ordering of this enum must be maintained to ensure the errata identifiers
+	 * map to the correct documentation.  New errata may only be appended to the
+	 * list and must contain corresponding documentation at the above link.
+	 */
+	typedef enum zpool_errata
+	{
+		ZPOOL_ERRATA_NONE,
+		ZPOOL_ERRATA_ZOL_2094_SCRUB,
+		ZPOOL_ERRATA_ZOL_2094_ASYNC_DESTROY,
+		ZPOOL_ERRATA_ZOL_6845_ENCRYPTION,
+		ZPOOL_ERRATA_ZOL_8308_ENCRYPTION,
+	} zpool_errata_t;
 
 /*
  * Vdev statistics.  Note: all fields should be 64-bit because this
@@ -1067,492 +1108,512 @@ typedef enum zpool_errata {
  * order to keep subsequent members at their known fixed offsets.  When
  * adding a new field it must be added to the end the structure.
  */
-#define	VS_ZIO_TYPES	6
-
-typedef struct vdev_stat {
-	hrtime_t	vs_timestamp;		/* time since vdev load	*/
-	uint64_t	vs_state;		/* vdev state		*/
-	uint64_t	vs_aux;			/* see vdev_aux_t	*/
-	uint64_t	vs_alloc;		/* space allocated	*/
-	uint64_t	vs_space;		/* total capacity	*/
-	uint64_t	vs_dspace;		/* deflated capacity	*/
-	uint64_t	vs_rsize;		/* replaceable dev size */
-	uint64_t	vs_esize;		/* expandable dev size */
-	uint64_t	vs_ops[VS_ZIO_TYPES];	/* operation count	*/
-	uint64_t	vs_bytes[VS_ZIO_TYPES];	/* bytes read/written	*/
-	uint64_t	vs_read_errors;		/* read errors		*/
-	uint64_t	vs_write_errors;	/* write errors		*/
-	uint64_t	vs_checksum_errors;	/* checksum errors	*/
-	uint64_t	vs_initialize_errors;	/* initializing errors	*/
-	uint64_t	vs_self_healed;		/* self-healed bytes	*/
-	uint64_t	vs_scan_removing;	/* removing?	*/
-	uint64_t	vs_scan_processed;	/* scan processed bytes	*/
-	uint64_t	vs_fragmentation;	/* device fragmentation */
-	uint64_t	vs_initialize_bytes_done; /* bytes initialized */
-	uint64_t	vs_initialize_bytes_est; /* total bytes to initialize */
-	uint64_t	vs_initialize_state;	/* vdev_initializing_state_t */
-	uint64_t	vs_initialize_action_time; /* time_t */
-	uint64_t	vs_checkpoint_space;    /* checkpoint-consumed space */
-	uint64_t	vs_resilver_deferred;	/* resilver deferred	*/
-	uint64_t	vs_slow_ios;		/* slow IOs */
-	uint64_t	vs_trim_errors;		/* trimming errors	*/
-	uint64_t	vs_trim_notsup;		/* supported by device */
-	uint64_t	vs_trim_bytes_done;	/* bytes trimmed */
-	uint64_t	vs_trim_bytes_est;	/* total bytes to trim */
-	uint64_t	vs_trim_state;		/* vdev_trim_state_t */
-	uint64_t	vs_trim_action_time;	/* time_t */
-	uint64_t	vs_rebuild_processed;	/* bytes rebuilt */
-	uint64_t	vs_configured_ashift;   /* TLV vdev_ashift */
-	uint64_t	vs_logical_ashift;	/* vdev_logical_ashift  */
-	uint64_t	vs_physical_ashift;	/* vdev_physical_ashift */
-	uint64_t	vs_pspace;		/* physical capacity */
-} vdev_stat_t;
+#define VS_ZIO_TYPES 7
+
+	typedef struct vdev_stat
+	{
+		hrtime_t vs_timestamp;				/* time since vdev load	*/
+		uint64_t vs_state;					/* vdev state		*/
+		uint64_t vs_aux;					/* see vdev_aux_t	*/
+		uint64_t vs_alloc;					/* space allocated	*/
+		uint64_t vs_space;					/* total capacity	*/
+		uint64_t vs_dspace;					/* deflated capacity	*/
+		uint64_t vs_rsize;					/* replaceable dev size */
+		uint64_t vs_esize;					/* expandable dev size */
+		uint64_t vs_ops[VS_ZIO_TYPES];		/* operation count	*/
+		uint64_t vs_bytes[VS_ZIO_TYPES];	/* bytes read/written	*/
+		uint64_t vs_read_errors;			/* read errors		*/
+		uint64_t vs_write_errors;			/* write errors		*/
+		uint64_t vs_checksum_errors;		/* checksum errors	*/
+		uint64_t vs_initialize_errors;		/* initializing errors	*/
+		uint64_t vs_self_healed;			/* self-healed bytes	*/
+		uint64_t vs_scan_removing;			/* removing?	*/
+		uint64_t vs_scan_processed;			/* scan processed bytes	*/
+		uint64_t vs_fragmentation;			/* device fragmentation */
+		uint64_t vs_initialize_bytes_done;	/* bytes initialized */
+		uint64_t vs_initialize_bytes_est;	/* total bytes to initialize */
+		uint64_t vs_initialize_state;		/* vdev_initializing_state_t */
+		uint64_t vs_initialize_action_time; /* time_t */
+		uint64_t vs_checkpoint_space;		/* checkpoint-consumed space */
+		uint64_t vs_resilver_deferred;		/* resilver deferred	*/
+		uint64_t vs_slow_ios;				/* slow IOs */
+		uint64_t vs_trim_errors;			/* trimming errors	*/
+		uint64_t vs_trim_notsup;			/* supported by device */
+		uint64_t vs_trim_bytes_done;		/* bytes trimmed */
+		uint64_t vs_trim_bytes_est;			/* total bytes to trim */
+		uint64_t vs_trim_state;				/* vdev_trim_state_t */
+		uint64_t vs_trim_action_time;		/* time_t */
+		uint64_t vs_rebuild_processed;		/* bytes rebuilt */
+		uint64_t vs_configured_ashift;		/* TLV vdev_ashift */
+		uint64_t vs_logical_ashift;			/* vdev_logical_ashift  */
+		uint64_t vs_physical_ashift;		/* vdev_physical_ashift */
+		uint64_t vs_pspace;					/* physical capacity */
+	} vdev_stat_t;
 
 /* BEGIN CSTYLED */
-#define	VDEV_STAT_VALID(field, uint64_t_field_count) \
-    ((uint64_t_field_count * sizeof (uint64_t)) >=	 \
-     (offsetof(vdev_stat_t, field) + sizeof (((vdev_stat_t *)NULL)->field)))
-/* END CSTYLED */
-
-/*
- * Extended stats
- *
- * These are stats which aren't included in the original iostat output.  For
- * convenience, they are grouped together in vdev_stat_ex, although each stat
- * is individually exported as an nvlist.
- */
-typedef struct vdev_stat_ex {
-	/* Number of ZIOs issued to disk and waiting to finish */
-	uint64_t vsx_active_queue[ZIO_PRIORITY_NUM_QUEUEABLE];
-
-	/* Number of ZIOs pending to be issued to disk */
-	uint64_t vsx_pend_queue[ZIO_PRIORITY_NUM_QUEUEABLE];
+#define VDEV_STAT_VALID(field, uint64_t_field_count) \
+	((uint64_t_field_count * sizeof(uint64_t)) >=    \
+	 (offsetof(vdev_stat_t, field) + sizeof(((vdev_stat_t *)NULL)->field)))
+	/* END CSTYLED */
 
 	/*
-	 * Below are the histograms for various latencies. Buckets are in
-	 * units of nanoseconds.
+	 * Extended stats
+	 *
+	 * These are stats which aren't included in the original iostat output.  For
+	 * convenience, they are grouped together in vdev_stat_ex, although each stat
+	 * is individually exported as an nvlist.
 	 */
+	typedef struct vdev_stat_ex
+	{
+		/* Number of ZIOs issued to disk and waiting to finish */
+		uint64_t vsx_active_queue[ZIO_PRIORITY_NUM_QUEUEABLE];
 
-	/*
-	 * 2^37 nanoseconds = 134s. Timeouts will probably start kicking in
-	 * before this.
-	 */
-#define	VDEV_L_HISTO_BUCKETS 37		/* Latency histo buckets */
-#define	VDEV_RQ_HISTO_BUCKETS 25	/* Request size histo buckets */
+		/* Number of ZIOs pending to be issued to disk */
+		uint64_t vsx_pend_queue[ZIO_PRIORITY_NUM_QUEUEABLE];
 
-	/* Amount of time in ZIO queue (ns) */
-	uint64_t vsx_queue_histo[ZIO_PRIORITY_NUM_QUEUEABLE]
-	    [VDEV_L_HISTO_BUCKETS];
+		/*
+		 * Below are the histograms for various latencies. Buckets are in
+		 * units of nanoseconds.
+		 */
 
-	/* Total ZIO latency (ns).  Includes queuing and disk access time */
-	uint64_t vsx_total_histo[ZIO_TYPES][VDEV_L_HISTO_BUCKETS];
+		/*
+		 * 2^37 nanoseconds = 134s. Timeouts will probably start kicking in
+		 * before this.
+		 */
+#define VDEV_L_HISTO_BUCKETS 37	 /* Latency histo buckets */
+#define VDEV_RQ_HISTO_BUCKETS 25 /* Request size histo buckets */
 
-	/* Amount of time to read/write the disk (ns) */
-	uint64_t vsx_disk_histo[ZIO_TYPES][VDEV_L_HISTO_BUCKETS];
+		/* Amount of time in ZIO queue (ns) */
+		uint64_t vsx_queue_histo[ZIO_PRIORITY_NUM_QUEUEABLE]
+								[VDEV_L_HISTO_BUCKETS];
 
-	/* "lookup the bucket for a value" histogram macros */
-#define	HISTO(val, buckets) (val != 0 ? MIN(highbit64(val) - 1, \
-	    buckets - 1) : 0)
-#define	L_HISTO(a) HISTO(a, VDEV_L_HISTO_BUCKETS)
-#define	RQ_HISTO(a) HISTO(a, VDEV_RQ_HISTO_BUCKETS)
+		/* Total ZIO latency (ns).  Includes queuing and disk access time */
+		uint64_t vsx_total_histo[ZIO_TYPES][VDEV_L_HISTO_BUCKETS];
 
-	/* Physical IO histogram */
-	uint64_t vsx_ind_histo[ZIO_PRIORITY_NUM_QUEUEABLE]
-	    [VDEV_RQ_HISTO_BUCKETS];
+		/* Amount of time to read/write the disk (ns) */
+		uint64_t vsx_disk_histo[ZIO_TYPES][VDEV_L_HISTO_BUCKETS];
 
-	/* Delegated (aggregated) physical IO histogram */
-	uint64_t vsx_agg_histo[ZIO_PRIORITY_NUM_QUEUEABLE]
-	    [VDEV_RQ_HISTO_BUCKETS];
+		/* "lookup the bucket for a value" histogram macros */
+#define HISTO(val, buckets) (val != 0 ? MIN(highbit64(val) - 1, \
+											buckets - 1)        \
+									  : 0)
+#define L_HISTO(a) HISTO(a, VDEV_L_HISTO_BUCKETS)
+#define RQ_HISTO(a) HISTO(a, VDEV_RQ_HISTO_BUCKETS)
 
-} vdev_stat_ex_t;
+		/* Physical IO histogram */
+		uint64_t vsx_ind_histo[ZIO_PRIORITY_NUM_QUEUEABLE]
+							  [VDEV_RQ_HISTO_BUCKETS];
 
-/*
- * Initialize functions.
- */
-typedef enum pool_initialize_func {
-	POOL_INITIALIZE_START,
-	POOL_INITIALIZE_CANCEL,
-	POOL_INITIALIZE_SUSPEND,
-	POOL_INITIALIZE_UNINIT,
-	POOL_INITIALIZE_FUNCS
-} pool_initialize_func_t;
+		/* Delegated (aggregated) physical IO histogram */
+		uint64_t vsx_agg_histo[ZIO_PRIORITY_NUM_QUEUEABLE]
+							  [VDEV_RQ_HISTO_BUCKETS];
 
-/*
- * TRIM functions.
- */
-typedef enum pool_trim_func {
-	POOL_TRIM_START,
-	POOL_TRIM_CANCEL,
-	POOL_TRIM_SUSPEND,
-	POOL_TRIM_FUNCS
-} pool_trim_func_t;
+	} vdev_stat_ex_t;
 
-/*
- * DDT statistics.  Note: all fields should be 64-bit because this
- * is passed between kernel and userland as an nvlist uint64 array.
- */
-typedef struct ddt_object {
-	uint64_t	ddo_count;	/* number of elements in ddt	*/
-	uint64_t	ddo_dspace;	/* size of ddt on disk		*/
-	uint64_t	ddo_mspace;	/* size of ddt in-core		*/
-} ddt_object_t;
-
-typedef struct ddt_stat {
-	uint64_t	dds_blocks;	/* blocks			*/
-	uint64_t	dds_lsize;	/* logical size			*/
-	uint64_t	dds_psize;	/* physical size		*/
-	uint64_t	dds_dsize;	/* deflated allocated size	*/
-	uint64_t	dds_ref_blocks;	/* referenced blocks		*/
-	uint64_t	dds_ref_lsize;	/* referenced lsize * refcnt	*/
-	uint64_t	dds_ref_psize;	/* referenced psize * refcnt	*/
-	uint64_t	dds_ref_dsize;	/* referenced dsize * refcnt	*/
-} ddt_stat_t;
-
-typedef struct ddt_histogram {
-	ddt_stat_t	ddh_stat[64];	/* power-of-two histogram buckets */
-} ddt_histogram_t;
-
-#define	ZVOL_DRIVER	"zvol"
-#define	ZFS_DRIVER	"zfs"
-#define	ZFS_DEV		"/dev/zfs"
-
-#define	ZFS_SUPER_MAGIC	0x2fc12fc1
+	/*
+	 * Initialize functions.
+	 */
+	typedef enum pool_initialize_func
+	{
+		POOL_INITIALIZE_START,
+		POOL_INITIALIZE_CANCEL,
+		POOL_INITIALIZE_SUSPEND,
+		POOL_INITIALIZE_UNINIT,
+		POOL_INITIALIZE_FUNCS
+	} pool_initialize_func_t;
+
+	/*
+	 * TRIM functions.
+	 */
+	typedef enum pool_trim_func
+	{
+		POOL_TRIM_START,
+		POOL_TRIM_CANCEL,
+		POOL_TRIM_SUSPEND,
+		POOL_TRIM_FUNCS
+	} pool_trim_func_t;
+
+	/*
+	 * DDT statistics.  Note: all fields should be 64-bit because this
+	 * is passed between kernel and userland as an nvlist uint64 array.
+	 */
+	typedef struct ddt_object
+	{
+		uint64_t ddo_count;	 /* number of elements in ddt	*/
+		uint64_t ddo_dspace; /* size of ddt on disk		*/
+		uint64_t ddo_mspace; /* size of ddt in-core		*/
+	} ddt_object_t;
+
+	typedef struct ddt_stat
+	{
+		uint64_t dds_blocks;	 /* blocks			*/
+		uint64_t dds_lsize;		 /* logical size			*/
+		uint64_t dds_psize;		 /* physical size		*/
+		uint64_t dds_dsize;		 /* deflated allocated size	*/
+		uint64_t dds_ref_blocks; /* referenced blocks		*/
+		uint64_t dds_ref_lsize;	 /* referenced lsize * refcnt	*/
+		uint64_t dds_ref_psize;	 /* referenced psize * refcnt	*/
+		uint64_t dds_ref_dsize;	 /* referenced dsize * refcnt	*/
+	} ddt_stat_t;
+
+	typedef struct ddt_histogram
+	{
+		ddt_stat_t ddh_stat[64]; /* power-of-two histogram buckets */
+	} ddt_histogram_t;
+
+#define ZVOL_DRIVER "zvol"
+#define ZFS_DRIVER "zfs"
+#define ZFS_DEV "/dev/zfs"
+
+#define ZFS_SUPER_MAGIC 0x2fc12fc1
 
 /* general zvol path */
-#define	ZVOL_DIR		"/dev/zvol/"
-
-#define	ZVOL_MAJOR		230
-#define	ZVOL_MINOR_BITS		4
-#define	ZVOL_MINOR_MASK		((1U << ZVOL_MINOR_BITS) - 1)
-#define	ZVOL_MINORS		(1 << 4)
-#define	ZVOL_DEV_NAME		"zd"
-
-#define	ZVOL_PROP_NAME		"name"
-#define	ZVOL_DEFAULT_BLOCKSIZE	8192
-
-typedef enum {
-	VDEV_INITIALIZE_NONE,
-	VDEV_INITIALIZE_ACTIVE,
-	VDEV_INITIALIZE_CANCELED,
-	VDEV_INITIALIZE_SUSPENDED,
-	VDEV_INITIALIZE_COMPLETE
-} vdev_initializing_state_t;
-
-typedef enum {
-	VDEV_TRIM_NONE,
-	VDEV_TRIM_ACTIVE,
-	VDEV_TRIM_CANCELED,
-	VDEV_TRIM_SUSPENDED,
-	VDEV_TRIM_COMPLETE,
-} vdev_trim_state_t;
-
-typedef enum {
-	VDEV_REBUILD_NONE,
-	VDEV_REBUILD_ACTIVE,
-	VDEV_REBUILD_CANCELED,
-	VDEV_REBUILD_COMPLETE,
-} vdev_rebuild_state_t;
+#define ZVOL_DIR "/dev/zvol/"
+
+#define ZVOL_MAJOR 230
+#define ZVOL_MINOR_BITS 4
+#define ZVOL_MINOR_MASK ((1U << ZVOL_MINOR_BITS) - 1)
+#define ZVOL_MINORS (1 << 4)
+#define ZVOL_DEV_NAME "zd"
+
+#define ZVOL_PROP_NAME "name"
+#define ZVOL_DEFAULT_BLOCKSIZE 8192
+
+	typedef enum
+	{
+		VDEV_INITIALIZE_NONE,
+		VDEV_INITIALIZE_ACTIVE,
+		VDEV_INITIALIZE_CANCELED,
+		VDEV_INITIALIZE_SUSPENDED,
+		VDEV_INITIALIZE_COMPLETE
+	} vdev_initializing_state_t;
+
+	typedef enum
+	{
+		VDEV_TRIM_NONE,
+		VDEV_TRIM_ACTIVE,
+		VDEV_TRIM_CANCELED,
+		VDEV_TRIM_SUSPENDED,
+		VDEV_TRIM_COMPLETE,
+	} vdev_trim_state_t;
+
+	typedef enum
+	{
+		VDEV_REBUILD_NONE,
+		VDEV_REBUILD_ACTIVE,
+		VDEV_REBUILD_CANCELED,
+		VDEV_REBUILD_COMPLETE,
+	} vdev_rebuild_state_t;
 
 /*
  * nvlist name constants. Facilitate restricting snapshot iteration range for
  * the "list next snapshot" ioctl
  */
-#define	SNAP_ITER_MIN_TXG	"snap_iter_min_txg"
-#define	SNAP_ITER_MAX_TXG	"snap_iter_max_txg"
+#define SNAP_ITER_MIN_TXG "snap_iter_min_txg"
+#define SNAP_ITER_MAX_TXG "snap_iter_max_txg"
 
-/*
- * /dev/zfs ioctl numbers.
- *
- * These numbers cannot change over time. New ioctl numbers must be appended.
- */
-typedef enum zfs_ioc {
+	/*
+	 * /dev/zfs ioctl numbers.
+	 *
+	 * These numbers cannot change over time. New ioctl numbers must be appended.
+	 */
+	typedef enum zfs_ioc
+	{
 	/*
 	 * Core features - 81/128 numbers reserved.
 	 */
 #ifdef __FreeBSD__
-	ZFS_IOC_FIRST =	0,
+		ZFS_IOC_FIRST = 0,
 #else
-	ZFS_IOC_FIRST =	('Z' << 8),
+	ZFS_IOC_FIRST = ('Z' << 8),
 #endif
-	ZFS_IOC = ZFS_IOC_FIRST,
-	ZFS_IOC_POOL_CREATE = ZFS_IOC_FIRST,	/* 0x5a00 */
-	ZFS_IOC_POOL_DESTROY,			/* 0x5a01 */
-	ZFS_IOC_POOL_IMPORT,			/* 0x5a02 */
-	ZFS_IOC_POOL_EXPORT,			/* 0x5a03 */
-	ZFS_IOC_POOL_CONFIGS,			/* 0x5a04 */
-	ZFS_IOC_POOL_STATS,			/* 0x5a05 */
-	ZFS_IOC_POOL_TRYIMPORT,			/* 0x5a06 */
-	ZFS_IOC_POOL_SCAN,			/* 0x5a07 */
-	ZFS_IOC_POOL_FREEZE,			/* 0x5a08 */
-	ZFS_IOC_POOL_UPGRADE,			/* 0x5a09 */
-	ZFS_IOC_POOL_GET_HISTORY,		/* 0x5a0a */
-	ZFS_IOC_VDEV_ADD,			/* 0x5a0b */
-	ZFS_IOC_VDEV_REMOVE,			/* 0x5a0c */
-	ZFS_IOC_VDEV_SET_STATE,			/* 0x5a0d */
-	ZFS_IOC_VDEV_ATTACH,			/* 0x5a0e */
-	ZFS_IOC_VDEV_DETACH,			/* 0x5a0f */
-	ZFS_IOC_VDEV_SETPATH,			/* 0x5a10 */
-	ZFS_IOC_VDEV_SETFRU,			/* 0x5a11 */
-	ZFS_IOC_OBJSET_STATS,			/* 0x5a12 */
-	ZFS_IOC_OBJSET_ZPLPROPS,		/* 0x5a13 */
-	ZFS_IOC_DATASET_LIST_NEXT,		/* 0x5a14 */
-	ZFS_IOC_SNAPSHOT_LIST_NEXT,		/* 0x5a15 */
-	ZFS_IOC_SET_PROP,			/* 0x5a16 */
-	ZFS_IOC_CREATE,				/* 0x5a17 */
-	ZFS_IOC_DESTROY,			/* 0x5a18 */
-	ZFS_IOC_ROLLBACK,			/* 0x5a19 */
-	ZFS_IOC_RENAME,				/* 0x5a1a */
-	ZFS_IOC_RECV,				/* 0x5a1b */
-	ZFS_IOC_SEND,				/* 0x5a1c */
-	ZFS_IOC_INJECT_FAULT,			/* 0x5a1d */
-	ZFS_IOC_CLEAR_FAULT,			/* 0x5a1e */
-	ZFS_IOC_INJECT_LIST_NEXT,		/* 0x5a1f */
-	ZFS_IOC_ERROR_LOG,			/* 0x5a20 */
-	ZFS_IOC_CLEAR,				/* 0x5a21 */
-	ZFS_IOC_PROMOTE,			/* 0x5a22 */
-	ZFS_IOC_SNAPSHOT,			/* 0x5a23 */
-	ZFS_IOC_DSOBJ_TO_DSNAME,		/* 0x5a24 */
-	ZFS_IOC_OBJ_TO_PATH,			/* 0x5a25 */
-	ZFS_IOC_POOL_SET_PROPS,			/* 0x5a26 */
-	ZFS_IOC_POOL_GET_PROPS,			/* 0x5a27 */
-	ZFS_IOC_SET_FSACL,			/* 0x5a28 */
-	ZFS_IOC_GET_FSACL,			/* 0x5a29 */
-	ZFS_IOC_SHARE,				/* 0x5a2a */
-	ZFS_IOC_INHERIT_PROP,			/* 0x5a2b */
-	ZFS_IOC_SMB_ACL,			/* 0x5a2c */
-	ZFS_IOC_USERSPACE_ONE,			/* 0x5a2d */
-	ZFS_IOC_USERSPACE_MANY,			/* 0x5a2e */
-	ZFS_IOC_USERSPACE_UPGRADE,		/* 0x5a2f */
-	ZFS_IOC_HOLD,				/* 0x5a30 */
-	ZFS_IOC_RELEASE,			/* 0x5a31 */
-	ZFS_IOC_GET_HOLDS,			/* 0x5a32 */
-	ZFS_IOC_OBJSET_RECVD_PROPS,		/* 0x5a33 */
-	ZFS_IOC_VDEV_SPLIT,			/* 0x5a34 */
-	ZFS_IOC_NEXT_OBJ,			/* 0x5a35 */
-	ZFS_IOC_DIFF,				/* 0x5a36 */
-	ZFS_IOC_TMP_SNAPSHOT,			/* 0x5a37 */
-	ZFS_IOC_OBJ_TO_STATS,			/* 0x5a38 */
-	ZFS_IOC_SPACE_WRITTEN,			/* 0x5a39 */
-	ZFS_IOC_SPACE_SNAPS,			/* 0x5a3a */
-	ZFS_IOC_DESTROY_SNAPS,			/* 0x5a3b */
-	ZFS_IOC_POOL_REGUID,			/* 0x5a3c */
-	ZFS_IOC_POOL_REOPEN,			/* 0x5a3d */
-	ZFS_IOC_SEND_PROGRESS,			/* 0x5a3e */
-	ZFS_IOC_LOG_HISTORY,			/* 0x5a3f */
-	ZFS_IOC_SEND_NEW,			/* 0x5a40 */
-	ZFS_IOC_SEND_SPACE,			/* 0x5a41 */
-	ZFS_IOC_CLONE,				/* 0x5a42 */
-	ZFS_IOC_BOOKMARK,			/* 0x5a43 */
-	ZFS_IOC_GET_BOOKMARKS,			/* 0x5a44 */
-	ZFS_IOC_DESTROY_BOOKMARKS,		/* 0x5a45 */
-	ZFS_IOC_RECV_NEW,			/* 0x5a46 */
-	ZFS_IOC_POOL_SYNC,			/* 0x5a47 */
-	ZFS_IOC_CHANNEL_PROGRAM,		/* 0x5a48 */
-	ZFS_IOC_LOAD_KEY,			/* 0x5a49 */
-	ZFS_IOC_UNLOAD_KEY,			/* 0x5a4a */
-	ZFS_IOC_CHANGE_KEY,			/* 0x5a4b */
-	ZFS_IOC_REMAP,				/* 0x5a4c */
-	ZFS_IOC_POOL_CHECKPOINT,		/* 0x5a4d */
-	ZFS_IOC_POOL_DISCARD_CHECKPOINT,	/* 0x5a4e */
-	ZFS_IOC_POOL_INITIALIZE,		/* 0x5a4f */
-	ZFS_IOC_POOL_TRIM,			/* 0x5a50 */
-	ZFS_IOC_REDACT,				/* 0x5a51 */
-	ZFS_IOC_GET_BOOKMARK_PROPS,		/* 0x5a52 */
-	ZFS_IOC_WAIT,				/* 0x5a53 */
-	ZFS_IOC_WAIT_FS,			/* 0x5a54 */
-
-	/*
-	 * Per-platform (Optional) - 8/128 numbers reserved.
-	 */
-	ZFS_IOC_PLATFORM = ZFS_IOC_FIRST + 0x80,
-	ZFS_IOC_EVENTS_NEXT,			/* 0x81 (Linux) */
-	ZFS_IOC_EVENTS_CLEAR,			/* 0x82 (Linux) */
-	ZFS_IOC_EVENTS_SEEK,			/* 0x83 (Linux) */
-	ZFS_IOC_NEXTBOOT,			/* 0x84 (FreeBSD) */
-	ZFS_IOC_JAIL,				/* 0x85 (FreeBSD) */
-	ZFS_IOC_UNJAIL,				/* 0x86 (FreeBSD) */
-	ZFS_IOC_SET_BOOTENV,			/* 0x87 */
-	ZFS_IOC_GET_BOOTENV,			/* 0x88 */
-	ZFS_IOC_LAST
-} zfs_ioc_t;
+		ZFS_IOC = ZFS_IOC_FIRST,
+		ZFS_IOC_POOL_CREATE = ZFS_IOC_FIRST, /* 0x5a00 */
+		ZFS_IOC_POOL_DESTROY,				 /* 0x5a01 */
+		ZFS_IOC_POOL_IMPORT,				 /* 0x5a02 */
+		ZFS_IOC_POOL_EXPORT,				 /* 0x5a03 */
+		ZFS_IOC_POOL_CONFIGS,				 /* 0x5a04 */
+		ZFS_IOC_POOL_STATS,					 /* 0x5a05 */
+		ZFS_IOC_POOL_TRYIMPORT,				 /* 0x5a06 */
+		ZFS_IOC_POOL_SCAN,					 /* 0x5a07 */
+		ZFS_IOC_POOL_FREEZE,				 /* 0x5a08 */
+		ZFS_IOC_POOL_UPGRADE,				 /* 0x5a09 */
+		ZFS_IOC_POOL_GET_HISTORY,			 /* 0x5a0a */
+		ZFS_IOC_VDEV_ADD,					 /* 0x5a0b */
+		ZFS_IOC_VDEV_REMOVE,				 /* 0x5a0c */
+		ZFS_IOC_VDEV_SET_STATE,				 /* 0x5a0d */
+		ZFS_IOC_VDEV_ATTACH,				 /* 0x5a0e */
+		ZFS_IOC_VDEV_DETACH,				 /* 0x5a0f */
+		ZFS_IOC_VDEV_SETPATH,				 /* 0x5a10 */
+		ZFS_IOC_VDEV_SETFRU,				 /* 0x5a11 */
+		ZFS_IOC_OBJSET_STATS,				 /* 0x5a12 */
+		ZFS_IOC_OBJSET_ZPLPROPS,			 /* 0x5a13 */
+		ZFS_IOC_DATASET_LIST_NEXT,			 /* 0x5a14 */
+		ZFS_IOC_SNAPSHOT_LIST_NEXT,			 /* 0x5a15 */
+		ZFS_IOC_SET_PROP,					 /* 0x5a16 */
+		ZFS_IOC_CREATE,						 /* 0x5a17 */
+		ZFS_IOC_DESTROY,					 /* 0x5a18 */
+		ZFS_IOC_ROLLBACK,					 /* 0x5a19 */
+		ZFS_IOC_RENAME,						 /* 0x5a1a */
+		ZFS_IOC_RECV,						 /* 0x5a1b */
+		ZFS_IOC_SEND,						 /* 0x5a1c */
+		ZFS_IOC_INJECT_FAULT,				 /* 0x5a1d */
+		ZFS_IOC_CLEAR_FAULT,				 /* 0x5a1e */
+		ZFS_IOC_INJECT_LIST_NEXT,			 /* 0x5a1f */
+		ZFS_IOC_ERROR_LOG,					 /* 0x5a20 */
+		ZFS_IOC_CLEAR,						 /* 0x5a21 */
+		ZFS_IOC_PROMOTE,					 /* 0x5a22 */
+		ZFS_IOC_SNAPSHOT,					 /* 0x5a23 */
+		ZFS_IOC_DSOBJ_TO_DSNAME,			 /* 0x5a24 */
+		ZFS_IOC_OBJ_TO_PATH,				 /* 0x5a25 */
+		ZFS_IOC_POOL_SET_PROPS,				 /* 0x5a26 */
+		ZFS_IOC_POOL_GET_PROPS,				 /* 0x5a27 */
+		ZFS_IOC_SET_FSACL,					 /* 0x5a28 */
+		ZFS_IOC_GET_FSACL,					 /* 0x5a29 */
+		ZFS_IOC_SHARE,						 /* 0x5a2a */
+		ZFS_IOC_INHERIT_PROP,				 /* 0x5a2b */
+		ZFS_IOC_SMB_ACL,					 /* 0x5a2c */
+		ZFS_IOC_USERSPACE_ONE,				 /* 0x5a2d */
+		ZFS_IOC_USERSPACE_MANY,				 /* 0x5a2e */
+		ZFS_IOC_USERSPACE_UPGRADE,			 /* 0x5a2f */
+		ZFS_IOC_HOLD,						 /* 0x5a30 */
+		ZFS_IOC_RELEASE,					 /* 0x5a31 */
+		ZFS_IOC_GET_HOLDS,					 /* 0x5a32 */
+		ZFS_IOC_OBJSET_RECVD_PROPS,			 /* 0x5a33 */
+		ZFS_IOC_VDEV_SPLIT,					 /* 0x5a34 */
+		ZFS_IOC_NEXT_OBJ,					 /* 0x5a35 */
+		ZFS_IOC_DIFF,						 /* 0x5a36 */
+		ZFS_IOC_TMP_SNAPSHOT,				 /* 0x5a37 */
+		ZFS_IOC_OBJ_TO_STATS,				 /* 0x5a38 */
+		ZFS_IOC_SPACE_WRITTEN,				 /* 0x5a39 */
+		ZFS_IOC_SPACE_SNAPS,				 /* 0x5a3a */
+		ZFS_IOC_DESTROY_SNAPS,				 /* 0x5a3b */
+		ZFS_IOC_POOL_REGUID,				 /* 0x5a3c */
+		ZFS_IOC_POOL_REOPEN,				 /* 0x5a3d */
+		ZFS_IOC_SEND_PROGRESS,				 /* 0x5a3e */
+		ZFS_IOC_LOG_HISTORY,				 /* 0x5a3f */
+		ZFS_IOC_SEND_NEW,					 /* 0x5a40 */
+		ZFS_IOC_SEND_SPACE,					 /* 0x5a41 */
+		ZFS_IOC_CLONE,						 /* 0x5a42 */
+		ZFS_IOC_BOOKMARK,					 /* 0x5a43 */
+		ZFS_IOC_GET_BOOKMARKS,				 /* 0x5a44 */
+		ZFS_IOC_DESTROY_BOOKMARKS,			 /* 0x5a45 */
+		ZFS_IOC_RECV_NEW,					 /* 0x5a46 */
+		ZFS_IOC_POOL_SYNC,					 /* 0x5a47 */
+		ZFS_IOC_CHANNEL_PROGRAM,			 /* 0x5a48 */
+		ZFS_IOC_LOAD_KEY,					 /* 0x5a49 */
+		ZFS_IOC_UNLOAD_KEY,					 /* 0x5a4a */
+		ZFS_IOC_CHANGE_KEY,					 /* 0x5a4b */
+		ZFS_IOC_REMAP,						 /* 0x5a4c */
+		ZFS_IOC_POOL_CHECKPOINT,			 /* 0x5a4d */
+		ZFS_IOC_POOL_DISCARD_CHECKPOINT,	 /* 0x5a4e */
+		ZFS_IOC_POOL_INITIALIZE,			 /* 0x5a4f */
+		ZFS_IOC_POOL_TRIM,					 /* 0x5a50 */
+		ZFS_IOC_REDACT,						 /* 0x5a51 */
+		ZFS_IOC_GET_BOOKMARK_PROPS,			 /* 0x5a52 */
+		ZFS_IOC_WAIT,						 /* 0x5a53 */
+		ZFS_IOC_WAIT_FS,					 /* 0x5a54 */
+		ZFS_IOC_POOL_EASY_SCAN,				 /* 0x5a55 */
+		ZFS_IOC_POOL_ALL_DNODE,				 /* 0x5a56 */
+		ZFS_IOC_POOL_FAILED_CHUNKS,          /* 0x5a57 */
+
+		/*
+		 * Per-platform (Optional) - 8/128 numbers reserved.
+		 */
+		ZFS_IOC_PLATFORM = ZFS_IOC_FIRST + 0x80,
+		ZFS_IOC_EVENTS_NEXT,  /* 0x81 (Linux) */
+		ZFS_IOC_EVENTS_CLEAR, /* 0x82 (Linux) */
+		ZFS_IOC_EVENTS_SEEK,  /* 0x83 (Linux) */
+		ZFS_IOC_NEXTBOOT,	  /* 0x84 (FreeBSD) */
+		ZFS_IOC_JAIL,		  /* 0x85 (FreeBSD) */
+		ZFS_IOC_UNJAIL,		  /* 0x86 (FreeBSD) */
+		ZFS_IOC_SET_BOOTENV,  /* 0x87 */
+		ZFS_IOC_GET_BOOTENV,  /* 0x88 */
+		ZFS_MLEC_RECEIVE_DATA,		  /* 0x89 (Linux MLEC Test)*/
+		ZFS_IOC_LAST
+	} zfs_ioc_t;
 
 /*
  * zvol ioctl to get dataset name
  */
-#define	BLKZNAME		_IOR(0x12, 125, char[ZFS_MAX_DATASET_NAME_LEN])
+#define BLKZNAME _IOR(0x12, 125, char[ZFS_MAX_DATASET_NAME_LEN])
 
-/*
- * ZFS-specific error codes used for returning descriptive errors
- * to the userland through zfs ioctls.
- *
- * The enum implicitly includes all the error codes from errno.h.
- * New code should use and extend this enum for errors that are
- * not described precisely by generic errno codes.
- *
- * These numbers should not change over time. New entries should be appended.
- *
- * (Keep in sync with contrib/pyzfs/libzfs_core/_constants.py)
- */
-typedef enum {
-	ZFS_ERR_CHECKPOINT_EXISTS = 1024,
-	ZFS_ERR_DISCARDING_CHECKPOINT,
-	ZFS_ERR_NO_CHECKPOINT,
-	ZFS_ERR_DEVRM_IN_PROGRESS,
-	ZFS_ERR_VDEV_TOO_BIG,
-	ZFS_ERR_IOC_CMD_UNAVAIL,
-	ZFS_ERR_IOC_ARG_UNAVAIL,
-	ZFS_ERR_IOC_ARG_REQUIRED,
-	ZFS_ERR_IOC_ARG_BADTYPE,
-	ZFS_ERR_WRONG_PARENT,
-	ZFS_ERR_FROM_IVSET_GUID_MISSING,
-	ZFS_ERR_FROM_IVSET_GUID_MISMATCH,
-	ZFS_ERR_SPILL_BLOCK_FLAG_MISSING,
-	ZFS_ERR_UNKNOWN_SEND_STREAM_FEATURE,
-	ZFS_ERR_EXPORT_IN_PROGRESS,
-	ZFS_ERR_BOOKMARK_SOURCE_NOT_ANCESTOR,
-	ZFS_ERR_STREAM_TRUNCATED,
-	ZFS_ERR_STREAM_LARGE_BLOCK_MISMATCH,
-	ZFS_ERR_RESILVER_IN_PROGRESS,
-	ZFS_ERR_REBUILD_IN_PROGRESS,
-	ZFS_ERR_BADPROP,
-} zfs_errno_t;
+	/*
+	 * ZFS-specific error codes used for returning descriptive errors
+	 * to the userland through zfs ioctls.
+	 *
+	 * The enum implicitly includes all the error codes from errno.h.
+	 * New code should use and extend this enum for errors that are
+	 * not described precisely by generic errno codes.
+	 *
+	 * These numbers should not change over time. New entries should be appended.
+	 *
+	 * (Keep in sync with contrib/pyzfs/libzfs_core/_constants.py)
+	 */
+	typedef enum
+	{
+		ZFS_ERR_CHECKPOINT_EXISTS = 1024,
+		ZFS_ERR_DISCARDING_CHECKPOINT,
+		ZFS_ERR_NO_CHECKPOINT,
+		ZFS_ERR_DEVRM_IN_PROGRESS,
+		ZFS_ERR_VDEV_TOO_BIG,
+		ZFS_ERR_IOC_CMD_UNAVAIL,
+		ZFS_ERR_IOC_ARG_UNAVAIL,
+		ZFS_ERR_IOC_ARG_REQUIRED,
+		ZFS_ERR_IOC_ARG_BADTYPE,
+		ZFS_ERR_WRONG_PARENT,
+		ZFS_ERR_FROM_IVSET_GUID_MISSING,
+		ZFS_ERR_FROM_IVSET_GUID_MISMATCH,
+		ZFS_ERR_SPILL_BLOCK_FLAG_MISSING,
+		ZFS_ERR_UNKNOWN_SEND_STREAM_FEATURE,
+		ZFS_ERR_EXPORT_IN_PROGRESS,
+		ZFS_ERR_BOOKMARK_SOURCE_NOT_ANCESTOR,
+		ZFS_ERR_STREAM_TRUNCATED,
+		ZFS_ERR_STREAM_LARGE_BLOCK_MISMATCH,
+		ZFS_ERR_RESILVER_IN_PROGRESS,
+		ZFS_ERR_REBUILD_IN_PROGRESS,
+		ZFS_ERR_BADPROP,
+	} zfs_errno_t;
 
-/*
- * Internal SPA load state.  Used by FMA diagnosis engine.
- */
-typedef enum {
-	SPA_LOAD_NONE,		/* no load in progress	*/
-	SPA_LOAD_OPEN,		/* normal open		*/
-	SPA_LOAD_IMPORT,	/* import in progress	*/
-	SPA_LOAD_TRYIMPORT,	/* tryimport in progress */
-	SPA_LOAD_RECOVER,	/* recovery requested	*/
-	SPA_LOAD_ERROR,		/* load failed		*/
-	SPA_LOAD_CREATE		/* creation in progress */
-} spa_load_state_t;
-
-typedef enum {
-	ZPOOL_WAIT_CKPT_DISCARD,
-	ZPOOL_WAIT_FREE,
-	ZPOOL_WAIT_INITIALIZE,
-	ZPOOL_WAIT_REPLACE,
-	ZPOOL_WAIT_REMOVE,
-	ZPOOL_WAIT_RESILVER,
-	ZPOOL_WAIT_SCRUB,
-	ZPOOL_WAIT_TRIM,
-	ZPOOL_WAIT_NUM_ACTIVITIES
-} zpool_wait_activity_t;
-
-typedef enum {
-	ZFS_WAIT_DELETEQ,
-	ZFS_WAIT_NUM_ACTIVITIES
-} zfs_wait_activity_t;
+	/*
+	 * Internal SPA load state.  Used by FMA diagnosis engine.
+	 */
+	typedef enum
+	{
+		SPA_LOAD_NONE,		/* no load in progress	*/
+		SPA_LOAD_OPEN,		/* normal open		*/
+		SPA_LOAD_IMPORT,	/* import in progress	*/
+		SPA_LOAD_TRYIMPORT, /* tryimport in progress */
+		SPA_LOAD_RECOVER,	/* recovery requested	*/
+		SPA_LOAD_ERROR,		/* load failed		*/
+		SPA_LOAD_CREATE		/* creation in progress */
+	} spa_load_state_t;
+
+	typedef enum
+	{
+		ZPOOL_WAIT_CKPT_DISCARD,
+		ZPOOL_WAIT_FREE,
+		ZPOOL_WAIT_INITIALIZE,
+		ZPOOL_WAIT_REPLACE,
+		ZPOOL_WAIT_REMOVE,
+		ZPOOL_WAIT_RESILVER,
+		ZPOOL_WAIT_SCRUB,
+		ZPOOL_WAIT_TRIM,
+		ZPOOL_WAIT_NUM_ACTIVITIES
+	} zpool_wait_activity_t;
+
+	typedef enum
+	{
+		ZFS_WAIT_DELETEQ,
+		ZFS_WAIT_NUM_ACTIVITIES
+	} zfs_wait_activity_t;
 
 /*
  * Bookmark name values.
  */
-#define	ZPOOL_ERR_LIST		"error list"
-#define	ZPOOL_ERR_DATASET	"dataset"
-#define	ZPOOL_ERR_OBJECT	"object"
+#define ZPOOL_ERR_LIST "error list"
+#define ZPOOL_ERR_DATASET "dataset"
+#define ZPOOL_ERR_OBJECT "object"
 
-#define	HIS_MAX_RECORD_LEN	(MAXPATHLEN + MAXPATHLEN + 1)
+#define HIS_MAX_RECORD_LEN (MAXPATHLEN + MAXPATHLEN + 1)
 
 /*
  * The following are names used in the nvlist describing
  * the pool's history log.
  */
-#define	ZPOOL_HIST_RECORD	"history record"
-#define	ZPOOL_HIST_TIME		"history time"
-#define	ZPOOL_HIST_CMD		"history command"
-#define	ZPOOL_HIST_WHO		"history who"
-#define	ZPOOL_HIST_ZONE		"history zone"
-#define	ZPOOL_HIST_HOST		"history hostname"
-#define	ZPOOL_HIST_TXG		"history txg"
-#define	ZPOOL_HIST_INT_EVENT	"history internal event"
-#define	ZPOOL_HIST_INT_STR	"history internal str"
-#define	ZPOOL_HIST_INT_NAME	"internal_name"
-#define	ZPOOL_HIST_IOCTL	"ioctl"
-#define	ZPOOL_HIST_INPUT_NVL	"in_nvl"
-#define	ZPOOL_HIST_OUTPUT_NVL	"out_nvl"
-#define	ZPOOL_HIST_OUTPUT_SIZE	"out_size"
-#define	ZPOOL_HIST_DSNAME	"dsname"
-#define	ZPOOL_HIST_DSID		"dsid"
-#define	ZPOOL_HIST_ERRNO	"errno"
-#define	ZPOOL_HIST_ELAPSED_NS	"elapsed_ns"
+#define ZPOOL_HIST_RECORD "history record"
+#define ZPOOL_HIST_TIME "history time"
+#define ZPOOL_HIST_CMD "history command"
+#define ZPOOL_HIST_WHO "history who"
+#define ZPOOL_HIST_ZONE "history zone"
+#define ZPOOL_HIST_HOST "history hostname"
+#define ZPOOL_HIST_TXG "history txg"
+#define ZPOOL_HIST_INT_EVENT "history internal event"
+#define ZPOOL_HIST_INT_STR "history internal str"
+#define ZPOOL_HIST_INT_NAME "internal_name"
+#define ZPOOL_HIST_IOCTL "ioctl"
+#define ZPOOL_HIST_INPUT_NVL "in_nvl"
+#define ZPOOL_HIST_OUTPUT_NVL "out_nvl"
+#define ZPOOL_HIST_OUTPUT_SIZE "out_size"
+#define ZPOOL_HIST_DSNAME "dsname"
+#define ZPOOL_HIST_DSID "dsid"
+#define ZPOOL_HIST_ERRNO "errno"
+#define ZPOOL_HIST_ELAPSED_NS "elapsed_ns"
 
 /*
  * Special nvlist name that will not have its args recorded in the pool's
  * history log.
  */
-#define	ZPOOL_HIDDEN_ARGS	"hidden_args"
+#define ZPOOL_HIDDEN_ARGS "hidden_args"
 
 /*
  * The following are names used when invoking ZFS_IOC_POOL_INITIALIZE.
  */
-#define	ZPOOL_INITIALIZE_COMMAND	"initialize_command"
-#define	ZPOOL_INITIALIZE_VDEVS		"initialize_vdevs"
+#define ZPOOL_INITIALIZE_COMMAND "initialize_command"
+#define ZPOOL_INITIALIZE_VDEVS "initialize_vdevs"
 
 /*
  * The following are names used when invoking ZFS_IOC_POOL_TRIM.
  */
-#define	ZPOOL_TRIM_COMMAND		"trim_command"
-#define	ZPOOL_TRIM_VDEVS		"trim_vdevs"
-#define	ZPOOL_TRIM_RATE			"trim_rate"
-#define	ZPOOL_TRIM_SECURE		"trim_secure"
+#define ZPOOL_TRIM_COMMAND "trim_command"
+#define ZPOOL_TRIM_VDEVS "trim_vdevs"
+#define ZPOOL_TRIM_RATE "trim_rate"
+#define ZPOOL_TRIM_SECURE "trim_secure"
 
 /*
  * The following are names used when invoking ZFS_IOC_POOL_WAIT.
  */
-#define	ZPOOL_WAIT_ACTIVITY		"wait_activity"
-#define	ZPOOL_WAIT_TAG			"wait_tag"
-#define	ZPOOL_WAIT_WAITED		"wait_waited"
+#define ZPOOL_WAIT_ACTIVITY "wait_activity"
+#define ZPOOL_WAIT_TAG "wait_tag"
+#define ZPOOL_WAIT_WAITED "wait_waited"
 
 /*
  * The following are names used when invoking ZFS_IOC_WAIT_FS.
  */
-#define	ZFS_WAIT_ACTIVITY		"wait_activity"
-#define	ZFS_WAIT_WAITED			"wait_waited"
+#define ZFS_WAIT_ACTIVITY "wait_activity"
+#define ZFS_WAIT_WAITED "wait_waited"
 
 /*
  * Flags for ZFS_IOC_VDEV_SET_STATE
  */
-#define	ZFS_ONLINE_CHECKREMOVE	0x1
-#define	ZFS_ONLINE_UNSPARE	0x2
-#define	ZFS_ONLINE_FORCEFAULT	0x4
-#define	ZFS_ONLINE_EXPAND	0x8
-#define	ZFS_ONLINE_SPARE	0x10
-#define	ZFS_OFFLINE_TEMPORARY	0x1
+#define ZFS_ONLINE_CHECKREMOVE 0x1
+#define ZFS_ONLINE_UNSPARE 0x2
+#define ZFS_ONLINE_FORCEFAULT 0x4
+#define ZFS_ONLINE_EXPAND 0x8
+#define ZFS_ONLINE_SPARE 0x10
+#define ZFS_OFFLINE_TEMPORARY 0x1
 
 /*
  * Flags for ZFS_IOC_POOL_IMPORT
  */
-#define	ZFS_IMPORT_NORMAL	0x0
-#define	ZFS_IMPORT_VERBATIM	0x1
-#define	ZFS_IMPORT_ANY_HOST	0x2
-#define	ZFS_IMPORT_MISSING_LOG	0x4
-#define	ZFS_IMPORT_ONLY		0x8
-#define	ZFS_IMPORT_TEMP_NAME	0x10
-#define	ZFS_IMPORT_SKIP_MMP	0x20
-#define	ZFS_IMPORT_LOAD_KEYS	0x40
-#define	ZFS_IMPORT_CHECKPOINT	0x80
+#define ZFS_IMPORT_NORMAL 0x0
+#define ZFS_IMPORT_VERBATIM 0x1
+#define ZFS_IMPORT_ANY_HOST 0x2
+#define ZFS_IMPORT_MISSING_LOG 0x4
+#define ZFS_IMPORT_ONLY 0x8
+#define ZFS_IMPORT_TEMP_NAME 0x10
+#define ZFS_IMPORT_SKIP_MMP 0x20
+#define ZFS_IMPORT_LOAD_KEYS 0x40
+#define ZFS_IMPORT_CHECKPOINT 0x80
 
 /*
  * Channel program argument/return nvlist keys and defaults.
  */
-#define	ZCP_ARG_PROGRAM		"program"
-#define	ZCP_ARG_ARGLIST		"arg"
-#define	ZCP_ARG_SYNC		"sync"
-#define	ZCP_ARG_INSTRLIMIT	"instrlimit"
-#define	ZCP_ARG_MEMLIMIT	"memlimit"
+#define ZCP_ARG_PROGRAM "program"
+#define ZCP_ARG_ARGLIST "arg"
+#define ZCP_ARG_SYNC "sync"
+#define ZCP_ARG_INSTRLIMIT "instrlimit"
+#define ZCP_ARG_MEMLIMIT "memlimit"
 
-#define	ZCP_ARG_CLIARGV		"argv"
+#define ZCP_ARG_CLIARGV "argv"
 
-#define	ZCP_RET_ERROR		"error"
-#define	ZCP_RET_RETURN		"return"
+#define ZCP_RET_ERROR "error"
+#define ZCP_RET_RETURN "return"
 
-#define	ZCP_DEFAULT_INSTRLIMIT	(10 * 1000 * 1000)
-#define	ZCP_MAX_INSTRLIMIT	(10 * ZCP_DEFAULT_INSTRLIMIT)
-#define	ZCP_DEFAULT_MEMLIMIT	(10 * 1024 * 1024)
-#define	ZCP_MAX_MEMLIMIT	(10 * ZCP_DEFAULT_MEMLIMIT)
+#define ZCP_DEFAULT_INSTRLIMIT (10 * 1000 * 1000)
+#define ZCP_MAX_INSTRLIMIT (10 * ZCP_DEFAULT_INSTRLIMIT)
+#define ZCP_DEFAULT_MEMLIMIT (10 * 1024 * 1024)
+#define ZCP_MAX_MEMLIMIT (10 * ZCP_DEFAULT_MEMLIMIT)
 
 /*
  * Sysevent payload members.  ZFS will generate the following sysevents with the
@@ -1602,24 +1663,23 @@ typedef enum {
  * characters that could be potentially unexpected to consumers of the
  * sysevents.
  */
-#define	ZFS_EV_POOL_NAME	"pool_name"
-#define	ZFS_EV_POOL_GUID	"pool_guid"
-#define	ZFS_EV_VDEV_PATH	"vdev_path"
-#define	ZFS_EV_VDEV_GUID	"vdev_guid"
-#define	ZFS_EV_HIST_TIME	"history_time"
-#define	ZFS_EV_HIST_CMD		"history_command"
-#define	ZFS_EV_HIST_WHO		"history_who"
-#define	ZFS_EV_HIST_ZONE	"history_zone"
-#define	ZFS_EV_HIST_HOST	"history_hostname"
-#define	ZFS_EV_HIST_TXG		"history_txg"
-#define	ZFS_EV_HIST_INT_EVENT	"history_internal_event"
-#define	ZFS_EV_HIST_INT_STR	"history_internal_str"
-#define	ZFS_EV_HIST_INT_NAME	"history_internal_name"
-#define	ZFS_EV_HIST_IOCTL	"history_ioctl"
-#define	ZFS_EV_HIST_DSNAME	"history_dsname"
-#define	ZFS_EV_HIST_DSID	"history_dsid"
-#define	ZFS_EV_RESILVER_TYPE	"resilver_type"
-
+#define ZFS_EV_POOL_NAME "pool_name"
+#define ZFS_EV_POOL_GUID "pool_guid"
+#define ZFS_EV_VDEV_PATH "vdev_path"
+#define ZFS_EV_VDEV_GUID "vdev_guid"
+#define ZFS_EV_HIST_TIME "history_time"
+#define ZFS_EV_HIST_CMD "history_command"
+#define ZFS_EV_HIST_WHO "history_who"
+#define ZFS_EV_HIST_ZONE "history_zone"
+#define ZFS_EV_HIST_HOST "history_hostname"
+#define ZFS_EV_HIST_TXG "history_txg"
+#define ZFS_EV_HIST_INT_EVENT "history_internal_event"
+#define ZFS_EV_HIST_INT_STR "history_internal_str"
+#define ZFS_EV_HIST_INT_NAME "history_internal_name"
+#define ZFS_EV_HIST_IOCTL "history_ioctl"
+#define ZFS_EV_HIST_DSNAME "history_dsname"
+#define ZFS_EV_HIST_DSID "history_dsid"
+#define ZFS_EV_RESILVER_TYPE "resilver_type"
 
 /*
  * We currently support block sizes from 512 bytes to 16MB.
@@ -1635,34 +1695,33 @@ typedef enum {
  * to 32MB, the dnode's dn_datablkszsec can only store sizes up to
  * 32MB - 512 bytes.  Therefore, we limit SPA_MAXBLOCKSIZE to 16MB.
  */
-#define	SPA_MINBLOCKSHIFT	9
-#define	SPA_OLD_MAXBLOCKSHIFT	17
-#define	SPA_MAXBLOCKSHIFT	24
-#define	SPA_MINBLOCKSIZE	(1ULL << SPA_MINBLOCKSHIFT)
-#define	SPA_OLD_MAXBLOCKSIZE	(1ULL << SPA_OLD_MAXBLOCKSHIFT)
-#define	SPA_MAXBLOCKSIZE	(1ULL << SPA_MAXBLOCKSHIFT)
-
-
-/* supported encryption algorithms */
-enum zio_encrypt {
-	ZIO_CRYPT_INHERIT = 0,
-	ZIO_CRYPT_ON,
-	ZIO_CRYPT_OFF,
-	ZIO_CRYPT_AES_128_CCM,
-	ZIO_CRYPT_AES_192_CCM,
-	ZIO_CRYPT_AES_256_CCM,
-	ZIO_CRYPT_AES_128_GCM,
-	ZIO_CRYPT_AES_192_GCM,
-	ZIO_CRYPT_AES_256_GCM,
-	ZIO_CRYPT_FUNCTIONS
-};
-
-#define	ZIO_CRYPT_ON_VALUE	ZIO_CRYPT_AES_256_GCM
-#define	ZIO_CRYPT_DEFAULT	ZIO_CRYPT_OFF
-
-
-#ifdef	__cplusplus
+#define SPA_MINBLOCKSHIFT 9
+#define SPA_OLD_MAXBLOCKSHIFT 17
+#define SPA_MAXBLOCKSHIFT 24
+#define SPA_MINBLOCKSIZE (1ULL << SPA_MINBLOCKSHIFT)
+#define SPA_OLD_MAXBLOCKSIZE (1ULL << SPA_OLD_MAXBLOCKSHIFT)
+#define SPA_MAXBLOCKSIZE (1ULL << SPA_MAXBLOCKSHIFT)
+
+	/* supported encryption algorithms */
+	enum zio_encrypt
+	{
+		ZIO_CRYPT_INHERIT = 0,
+		ZIO_CRYPT_ON,
+		ZIO_CRYPT_OFF,
+		ZIO_CRYPT_AES_128_CCM,
+		ZIO_CRYPT_AES_192_CCM,
+		ZIO_CRYPT_AES_256_CCM,
+		ZIO_CRYPT_AES_128_GCM,
+		ZIO_CRYPT_AES_192_GCM,
+		ZIO_CRYPT_AES_256_GCM,
+		ZIO_CRYPT_FUNCTIONS
+	};
+
+#define ZIO_CRYPT_ON_VALUE ZIO_CRYPT_AES_256_GCM
+#define ZIO_CRYPT_DEFAULT ZIO_CRYPT_OFF
+
+#ifdef __cplusplus
 }
 #endif
 
-#endif	/* _SYS_FS_ZFS_H */
+#endif /* _SYS_FS_ZFS_H */
diff --git a/include/sys/vdev.h b/include/sys/vdev.h
index de08bbf16..ac506ff51 100644
--- a/include/sys/vdev.h
+++ b/include/sys/vdev.h
@@ -62,7 +62,7 @@ extern int vdev_copy_path_strict(vdev_t *, vdev_t *);
 extern void vdev_copy_path_relaxed(vdev_t *, vdev_t *);
 extern void vdev_close(vdev_t *);
 extern int vdev_create(vdev_t *, uint64_t txg, boolean_t isreplace);
-extern void vdev_reopen(vdev_t *);
+extern int vdev_reopen(vdev_t *);
 extern int vdev_validate_aux(vdev_t *vd);
 extern zio_t *vdev_probe(vdev_t *vd, zio_t *pio);
 extern boolean_t vdev_is_concrete(vdev_t *vd);
diff --git a/include/sys/vdev_impl.h b/include/sys/vdev_impl.h
index 9d4a8062b..32d0b69aa 100644
--- a/include/sys/vdev_impl.h
+++ b/include/sys/vdev_impl.h
@@ -102,6 +102,7 @@ typedef void vdev_metaslab_init_func_t(vdev_t *vd, uint64_t *startp,
 typedef void vdev_config_generate_func_t(vdev_t *vd, nvlist_t *nv);
 typedef uint64_t vdev_nparity_func_t(vdev_t *vd);
 typedef uint64_t vdev_ndisks_func_t(vdev_t *vd);
+typedef uint64_t vdev_direct_data_inject_func(vdev_t *vd);
 
 typedef const struct vdev_ops {
 	vdev_init_func_t		*vdev_op_init;
@@ -125,6 +126,7 @@ typedef const struct vdev_ops {
 	vdev_nparity_func_t		*vdev_op_nparity;
 	vdev_ndisks_func_t		*vdev_op_ndisks;
 	vdev_kobj_post_evt_func_t	*vdev_op_kobj_evt_post;
+	// vdev_direct_data_inject_func  *vdev_direct_data_inject;
 	char				vdev_op_type[16];
 	boolean_t			vdev_op_leaf;
 } vdev_ops_t;
@@ -610,6 +612,8 @@ extern vdev_ops_t vdev_missing_ops;
 extern vdev_ops_t vdev_hole_ops;
 extern vdev_ops_t vdev_spare_ops;
 extern vdev_ops_t vdev_indirect_ops;
+extern vdev_ops_t vdev_my_mirror_ops;
+extern vdev_ops_t vdev_my_raidz_ops;
 
 /*
  * Common size functions
diff --git a/include/sys/vdev_raidz.h b/include/sys/vdev_raidz.h
index c7cf0af6d..fa09e833b 100644
--- a/include/sys/vdev_raidz.h
+++ b/include/sys/vdev_raidz.h
@@ -54,6 +54,20 @@ void vdev_raidz_checksum_error(zio_t *, struct raidz_col *, abd_t *);
 
 extern const zio_vsd_ops_t vdev_raidz_vsd_ops;
 
+/*
+ * vdev_my_raidz interface
+ */
+struct raidz_map *vdev_my_raidz_map_alloc(struct zio *, uint64_t, uint64_t,
+    uint64_t);
+void vdev_my_raidz_map_free(struct raidz_map *);
+void vdev_my_raidz_generate_parity_row(struct raidz_map *, struct raidz_row *);
+void vdev_my_raidz_generate_parity(struct raidz_map *);
+void vdev_my_raidz_reconstruct(struct raidz_map *, const int *, int);
+void vdev_my_raidz_child_done(zio_t *);
+void vdev_my_raidz_io_done(zio_t *);
+void vdev_my_raidz_checksum_error(zio_t *, struct raidz_col *, abd_t *);
+
+extern const zio_vsd_ops_t vdev_my_raidz_vsd_ops;
 /*
  * vdev_raidz_math interface
  */
diff --git a/include/sys/zio.h b/include/sys/zio.h
index 39de5175b..4f1318bab 100644
--- a/include/sys/zio.h
+++ b/include/sys/zio.h
@@ -505,6 +505,11 @@ struct zio {
 
 	/* Taskq dispatching state */
 	taskq_ent_t	io_tqent;
+
+	/* MLEC stuff */
+	char test_tag[20];
+	blkptr_t *mlec_write_target;
+	uint64_t mlec_write_col_idx;
 };
 
 enum blk_verify_flag {
diff --git a/install-vm.md b/install-vm.md
new file mode 100644
index 000000000..ec30728ef
--- /dev/null
+++ b/install-vm.md
@@ -0,0 +1,48 @@
+```
+sudo apt update
+sudo apt install -y qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virt-manager
+```
+
+```
+sudo systemctl enable --now libvirtd
+```
+
+```
+virsh list --all
+```
+
+```
+cd ~
+wget https://releases.ubuntu.com/20.04/ubuntu-20.04.6-live-server-amd64.iso
+sudo mv ~/ubuntu-20.04.6-live-server-amd64.iso /var/lib/libvirt/images/
+```
+
+
+```
+sudo apt install bridge-utils
+sudo brctl addbr br0
+sudo ip link set br0 up
+
+```
+
+
+
+```
+sudo virt-install \
+--name ubuntu20-vm \
+--ram 128000 \
+--vcpus 128 \
+--disk path=/var/lib/libvirt/images/ubuntu20-vm.qcow2,size=200 \
+--os-type linux \
+--os-variant ubuntu20.04 \
+--graphics none \
+--console pty,target_type=serial \
+--cdrom /var/lib/libvirt/images/ubuntu-20.04.6-live-server-amd64.iso
+
+
+```
+
+```
+chmod +r ~/ubuntu-20.04.6-live-server-amd64.iso
+chmod +x ~
+```
\ No newline at end of file
diff --git a/lib/libzfs/libzfs_pool.c b/lib/libzfs/libzfs_pool.c
index fc6c6e8e2..245951d2f 100644
--- a/lib/libzfs/libzfs_pool.c
+++ b/lib/libzfs/libzfs_pool.c
@@ -2555,6 +2555,178 @@ zpool_scan(zpool_handle_t *zhp, pool_scan_func_t func, pool_scrub_cmd_t cmd)
 	}
 }
 
+int
+zpool_get_failed_chunks(zpool_handle_t *zhp, int64_t objset_id, int64_t object_id) {
+	zfs_cmd_t zc;
+	libzfs_handle_t *hdl = zhp->zpool_hdl;
+
+	// printf("zpool_get_failed_chunks\n");
+
+	// Copy the zc name
+	(void) strlcpy(zc.zc_name, zhp->zpool_name, sizeof (zc.zc_name));
+	
+	// // Configure input
+	// nvlist_t *input;
+	// nvlist_alloc(&input, NV_UNIQUE_NAME, 0);
+	// nvlist_add_uint64(input, "objset_id", objset_id);
+
+	// printf("=====\n");
+	// size_t nvsz;
+	// int err = nvlist_size(input, &nvsz, NV_ENCODE_NATIVE);
+	// assert(err == 0);
+
+	// char *nvbuf = malloc(nvsz);
+
+	// err = nvlist_pack(input, &nvbuf, &nvsz, NV_ENCODE_NATIVE, 0);
+	// assert(err == 0);
+
+	// zc.zc_nvlist_src_size = nvsz;
+	// zc.zc_nvlist_src = (uintptr_t)nvbuf;
+
+	// Set output size
+	// TODO: fix this excessive memory allocation
+	zc.zc_nvlist_dst_size = 60000;
+	zc.zc_nvlist_dst = (uint64_t)(uintptr_t)zfs_alloc(hdl, zc.zc_nvlist_dst_size);
+
+	errno = 0;
+	errno = zfs_ioctl(hdl, ZFS_IOC_POOL_FAILED_CHUNKS, &zc);
+
+	// printf("Error no %d\n", errno);
+
+	nvlist_t *out;
+	nvlist_alloc(&out, NV_UNIQUE_NAME, 0);
+
+	int error = 0;
+	error = nvlist_unpack((char *)zc.zc_nvlist_dst, zc.zc_nvlist_dst_size, &out, 0);
+	if (error) {
+		printf("Error unpacking %d\n", error);
+	}
+
+	// nvpair_t *pair;
+	// while ((pair = nvlist_next_nvpair(out, pair)) != NULL) {
+	// 	// Get the attributes
+	// 	char *dnode_id_str = nvpair_name(pair);
+	// 	uint64_t dnode_id = atoi(dnode_id_str);
+	// 	printf("dnode %ld\n", dnode_id);
+	// }
+
+	// nvlist_free(input);
+	nvlist_free(out);
+
+	return 0;
+}
+
+int
+zpool_get_all_dnode(zpool_handle_t *zhp) {
+	zfs_cmd_t zc;
+	libzfs_handle_t *hdl = zhp->zpool_hdl;
+
+	// Copy the zc name
+	(void) strlcpy(zc.zc_name, zhp->zpool_name, sizeof (zc.zc_name));
+	
+	// Set output size
+	// TODO: fix this excessive memory allocation
+	zc.zc_nvlist_dst_size = 6000;
+	zc.zc_nvlist_dst = (uint64_t)(uintptr_t)zfs_alloc(hdl, zc.zc_nvlist_dst_size);
+
+	errno = 0;
+	errno = zfs_ioctl(hdl, ZFS_IOC_POOL_ALL_DNODE, &zc);
+
+	// printf("Error no %d\n", errno);
+	// Read the nvlist
+
+	nvlist_t *out;
+	nvlist_alloc(&out, NV_UNIQUE_NAME, 0);
+
+	int error = 0;
+	error = nvlist_unpack((char *)zc.zc_nvlist_dst, zc.zc_nvlist_dst_size, &out, 0);
+	if (error) {
+		printf("Get all dnode error unpacking %d\n", error);
+	}
+
+	nvpair_t *nvp = NULL;
+	while ((nvp = nvlist_next_nvpair(out, nvp)) != NULL) {
+		printf("Datanode name %s\n", nvpair_name(nvp));
+		// Get the nested nvlist attributes
+		nvlist_t *temp_attributes;
+		nvlist_alloc(&temp_attributes, NV_UNIQUE_NAME, 0);
+		nvpair_value_nvlist(nvp, &temp_attributes);
+		
+		int64_t objset, object, type;
+		char *path;
+		nvlist_lookup_int64(temp_attributes, "objset", &objset);
+		nvlist_lookup_int64(temp_attributes, "object", &object);
+		nvlist_lookup_int64(temp_attributes, "type", &type);
+		nvlist_lookup_string(temp_attributes, "path", &path);
+		printf("dnode %ld:%ld, type %ld, path %s\n", objset, object, type, path);
+
+		nvlist_free(temp_attributes);
+	}
+
+	nvlist_free(out);
+
+	return 0;
+}
+
+/* Easy Zpool scan for easy scrub. */
+int
+zpool_easy_scan(zpool_handle_t *zhp)
+{
+	zfs_cmd_t zc;
+	libzfs_handle_t *hdl = zhp->zpool_hdl;
+
+	// Copy the zc name
+	(void) strlcpy(zc.zc_name, zhp->zpool_name, sizeof (zc.zc_name));
+
+	// Set output size
+	// TODO: fix this excessive memory allocation
+	zc.zc_nvlist_dst_size = 200;
+	zc.zc_nvlist_dst = (uint64_t)(uintptr_t)zfs_alloc(hdl, zc.zc_nvlist_dst_size);
+
+	errno = 0;
+	errno = zfs_ioctl(hdl, ZFS_IOC_POOL_EASY_SCAN, &zc);
+
+	// printf("Error no %d\n", errno);
+	// Read the nvlist
+
+	nvlist_t *out;
+	nvlist_alloc(&out, NV_UNIQUE_NAME, 0);
+
+	int error = 0;
+	error = nvlist_unpack((char *)zc.zc_nvlist_dst, zc.zc_nvlist_dst_size, &out, 0);
+	if (error) {
+		printf("Error unpacking %d\n", error);
+	}
+
+	int64_t *child_status;
+	uint_t actual_len;
+
+	int64_t num_children;
+	nvlist_lookup_int64(out, "children", &num_children);
+	printf("Num children %ld\n", num_children );
+
+	error = nvlist_lookup_int64_array(out, "children_status", &child_status, &actual_len);
+	// error = nvlist_lookup_int16_array(out, "children_status", child_status, 3);
+	if (error) {
+		printf("Error while looking up out nvlist, error %d\n", error);
+	} else {
+		printf("Got %d elems\n", actual_len);
+	}
+
+	for (int i = 0; i < 3; i++) {
+		printf("%ld ", child_status[i]);
+	}
+	printf("\n");
+
+	if (errno) {
+		printf("easy scrub detect disk failures for %s\n", zc.zc_name);
+	}
+
+	nvlist_free(out);
+
+	return 0;
+}
+
 /*
  * Find a vdev that matches the search criteria specified. We use the
  * the nvpair name to determine how we should look for the device.
diff --git a/lib/libzfs/os/freebsd/libzfs_compat.c b/lib/libzfs/os/freebsd/libzfs_compat.c
index 0e8a3b121..a1ed98539 100644
--- a/lib/libzfs/os/freebsd/libzfs_compat.c
+++ b/lib/libzfs/os/freebsd/libzfs_compat.c
@@ -201,6 +201,7 @@ libzfs_error_init(int error)
 int
 zfs_ioctl(libzfs_handle_t *hdl, int request, zfs_cmd_t *zc)
 {
+	printf("Calling zfs_ioctl_fd");
 	return (zfs_ioctl_fd(hdl->libzfs_fd, request, zc));
 }
 
diff --git a/lib/libzfs/os/linux/libzfs_util_os.c b/lib/libzfs/os/linux/libzfs_util_os.c
index e2482c571..bd5a4f1b1 100644
--- a/lib/libzfs/os/linux/libzfs_util_os.c
+++ b/lib/libzfs/os/linux/libzfs_util_os.c
@@ -48,6 +48,7 @@
 int
 zfs_ioctl(libzfs_handle_t *hdl, int request, zfs_cmd_t *zc)
 {
+	// printf("Calling linux ioctl with fd %d, request %d\n", hdl->libzfs_fd, request);
 	return (ioctl(hdl->libzfs_fd, request, zc));
 }
 
diff --git a/lib/libzfs_core/libzfs_core.c b/lib/libzfs_core/libzfs_core.c
index 9b2050c67..7d27f9eac 100644
--- a/lib/libzfs_core/libzfs_core.c
+++ b/lib/libzfs_core/libzfs_core.c
@@ -172,6 +172,7 @@ static int
 lzc_ioctl(zfs_ioc_t ioc, const char *name,
     nvlist_t *source, nvlist_t **resultp)
 {
+	// printf("lzc_ioctl called with %s and gfd %d\n", name, g_fd);
 	zfs_cmd_t zc = {"\0"};
 	int error = 0;
 	char *packed = NULL;
@@ -210,6 +211,7 @@ lzc_ioctl(zfs_ioc_t ioc, const char *name,
 		}
 	}
 
+	// printf("Calling zfs_ioctl_fd()\n");
 	while (zfs_ioctl_fd(g_fd, ioc, &zc) != 0) {
 		/*
 		 * If ioctl exited with ENOMEM, we retry the ioctl after
@@ -1641,3 +1643,23 @@ lzc_get_bootenv(const char *pool, nvlist_t **outnvl)
 {
 	return (lzc_ioctl(ZFS_IOC_GET_BOOTENV, pool, NULL, outnvl));
 }
+
+/*
+* A test method for MLEC impl
+*/
+int
+lzc_mlec_receive_data(const char *pool, nvlist_t *input) 
+{
+	return (lzc_ioctl(ZFS_MLEC_RECEIVE_DATA, pool, input, NULL));
+}
+
+int lzc_mlec_get_failed_chunks(const char *pool, nvlist_t *input, nvlist_t **output) {
+	return (lzc_ioctl(ZFS_IOC_POOL_FAILED_CHUNKS, pool, input, output));
+}
+
+/**
+ * MLEC get all dnode
+ */
+int lzc_mlec_get_all_dnode(const char *pool, nvlist_t *input, nvlist_t **output) {
+	return (lzc_ioctl(ZFS_IOC_POOL_ALL_DNODE, pool, input, output));
+}
\ No newline at end of file
diff --git a/lib/libzpool/Makefile.am b/lib/libzpool/Makefile.am
index 4ce3b4cd2..24aee3997 100644
--- a/lib/libzpool/Makefile.am
+++ b/lib/libzpool/Makefile.am
@@ -132,9 +132,11 @@ KERNEL_C = \
 	vdev_initialize.c \
 	vdev_label.c \
 	vdev_mirror.c \
+	vdev_my_mirror.c \
 	vdev_missing.c \
 	vdev_queue.c \
 	vdev_raidz.c \
+	vdev_my_raidz.c \
 	vdev_raidz_math_aarch64_neon.c \
 	vdev_raidz_math_aarch64_neonx2.c \
 	vdev_raidz_math_avx2.c \
diff --git a/lib/libzutil/os/freebsd/zutil_compat.c b/lib/libzutil/os/freebsd/zutil_compat.c
index baaf4b598..0a38f4e21 100644
--- a/lib/libzutil/os/freebsd/zutil_compat.c
+++ b/lib/libzutil/os/freebsd/zutil_compat.c
@@ -47,6 +47,7 @@ get_zfs_ioctl_version(void)
 static int
 zcmd_ioctl_compat(int fd, int request, zfs_cmd_t *zc, const int cflag)
 {
+	printf("zcmd_ioctl_compat() called\n");
 	int newrequest, ret;
 	void *zc_c = NULL;
 	unsigned long ncmd;
@@ -94,12 +95,15 @@ zcmd_ioctl_compat(int fd, int request, zfs_cmd_t *zc, const int cflag)
 int
 zfs_ioctl_fd(int fd, unsigned long request, zfs_cmd_t *zc)
 {
+	printf("zfs_ioctl_fd() called\n");
 	size_t oldsize;
 	int ret, cflag = ZFS_CMD_COMPAT_NONE;
-
+	
 	if (zfs_ioctl_version == ZFS_IOCVER_UNDEF)
 		zfs_ioctl_version = get_zfs_ioctl_version();
 
+	printf("zfs_ioctl_fd() wtih version %d\n", zfs_ioctl_version);
+
 	switch (zfs_ioctl_version) {
 		case ZFS_IOCVER_LEGACY:
 			cflag = ZFS_CMD_COMPAT_LEGACY;
@@ -113,9 +117,11 @@ zfs_ioctl_fd(int fd, unsigned long request, zfs_cmd_t *zc)
 	}
 
 	oldsize = zc->zc_nvlist_dst_size;
+	printf("Calling zcmd_ioctl_compat()\n");
 	ret = zcmd_ioctl_compat(fd, request, zc, cflag);
 
 	if (ret == 0 && oldsize < zc->zc_nvlist_dst_size) {
+		printf("Wrong size!");
 		ret = -1;
 		errno = ENOMEM;
 	}
diff --git a/lib/libzutil/os/linux/zutil_compat.c b/lib/libzutil/os/linux/zutil_compat.c
index 173ae9cb6..6cd6b149e 100644
--- a/lib/libzutil/os/linux/zutil_compat.c
+++ b/lib/libzutil/os/linux/zutil_compat.c
@@ -26,5 +26,6 @@
 int
 zfs_ioctl_fd(int fd, unsigned long request, zfs_cmd_t *zc)
 {
+	// printf("zfs_ioctl_fd() for OS Linux called with fd %d and %s\n", fd, zc->zc_name);
 	return (ioctl(fd, request, zc));
 }
diff --git a/log.txt b/log.txt
new file mode 100644
index 000000000..6abe20fd7
--- /dev/null
+++ b/log.txt
@@ -0,0 +1,101 @@
+timestamp    message 
+1697602047   vdev.c:718:vdev_alloc(): get ops sucessful
+1697602047   vdev.c:744:vdev_alloc(): <2>
+1697602047   vdev.c:745:vdev_alloc(): root
+1697602047   vdev.c:752:vdev_alloc(): non-root
+1697602047   vdev.c:764:vdev_alloc(): next step
+1697602047   vdev.c:792:vdev_alloc(): <2.5>
+1697602047   vdev.c:805:vdev_alloc(): <3>
+1697602047   vdev.c:841:vdev_alloc(): <4>
+1697602047   vdev.c:915:vdev_alloc(): <5>
+1697602047   vdev.c:718:vdev_alloc(): get ops sucessful
+1697602047   vdev.c:744:vdev_alloc(): <2>
+1697602047   vdev.c:745:vdev_alloc(): raidz
+1697602047   vdev.c:752:vdev_alloc(): non-root
+1697602047   vdev.c:764:vdev_alloc(): next step
+1697602047   vdev.c:792:vdev_alloc(): <2.5>
+1697602047   vdev.c:805:vdev_alloc(): <3>
+1697602047   vdev.c:841:vdev_alloc(): <4>
+1697602047   vdev.c:915:vdev_alloc(): <5>
+1697602047   vdev.c:718:vdev_alloc(): get ops sucessful
+1697602047   vdev.c:744:vdev_alloc(): <2>
+1697602047   vdev.c:745:vdev_alloc(): file
+1697602047   vdev.c:752:vdev_alloc(): non-root
+1697602047   vdev.c:764:vdev_alloc(): next step
+1697602047   vdev.c:792:vdev_alloc(): <2.5>
+1697602047   vdev.c:805:vdev_alloc(): <3>
+1697602047   vdev.c:841:vdev_alloc(): <4>
+1697602047   vdev.c:915:vdev_alloc(): <5>
+1697602047   vdev.c:718:vdev_alloc(): get ops sucessful
+1697602047   vdev.c:744:vdev_alloc(): <2>
+1697602047   vdev.c:745:vdev_alloc(): file
+1697602047   vdev.c:752:vdev_alloc(): non-root
+1697602047   vdev.c:764:vdev_alloc(): next step
+1697602047   vdev.c:792:vdev_alloc(): <2.5>
+1697602047   vdev.c:805:vdev_alloc(): <3>
+1697602047   vdev.c:841:vdev_alloc(): <4>
+1697602047   vdev.c:915:vdev_alloc(): <5>
+1697602047   vdev.c:718:vdev_alloc(): get ops sucessful
+1697602047   vdev.c:744:vdev_alloc(): <2>
+1697602047   vdev.c:745:vdev_alloc(): file
+1697602047   vdev.c:752:vdev_alloc(): non-root
+1697602047   vdev.c:764:vdev_alloc(): next step
+1697602047   vdev.c:792:vdev_alloc(): <2.5>
+1697602047   vdev.c:805:vdev_alloc(): <3>
+1697602047   vdev.c:841:vdev_alloc(): <4>
+1697602047   vdev.c:915:vdev_alloc(): <5>
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 create pool version 5000; software version zfs-2.1.12-10-g4216685f9; uts mlec-vm 5.15.0-86-generic #96-Ubuntu SMP Wed Sep 20 08:23:49 UTC 2023 x86_64
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@async_destroy=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@empty_bpobj=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@lz4_compress=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@multi_vdev_crash_dump=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@spacemap_histogram=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@enabled_txg=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@hole_birth=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@extensible_dataset=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@embedded_data=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@bookmarks=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@filesystem_limits=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@large_blocks=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@large_dnode=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@sha512=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@skein=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@edonr=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@userobj_accounting=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@encryption=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@project_quota=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@device_removal=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@obsolete_counts=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@zpool_checkpoint=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@spacemap_v2=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@allocation_classes=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@resilver_defer=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@bookmark_v2=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@redaction_bookmarks=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@redacted_datasets=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@bookmark_written=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@log_spacemap=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@livelist=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@device_rebuild=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@zstd_compress=enabled
+1697602047   spa_history.c:307:spa_history_log_sync(): txg 4 set feature@draid=enabled
+1697602047   mmp.c:240:mmp_thread_start(): MMP thread started pool 'test' gethrtime 1528497219260
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 0, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528497 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 1000000040000000, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 1, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528497 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 100000003e9bd37b, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 2, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528497 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 100000003d37a6f5, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 3, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528497 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 100000003bd37a70, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 4, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528497 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 100000003a6f4dea, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 5, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528497 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 10000000390b2165, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 6, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528497 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 1000000037a6f4df, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 7, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528497 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 100000003642c85a, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 8, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528498 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 1000000034de9bd4, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 9, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 10, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 11, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 12, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 13, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 14, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 15, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 16, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   metaslab.c:2437:metaslab_load_impl(): metaslab_load: txg 4, spa test, vdev_id 0, ms_id 17, smp_length 0, unflushed_allocs 0, unflushed_frees 0, freed 0, defer 0 + 0, unloaded time 1528532 ms, loading_time 0 ms, ms_max_size 536870912, max size error 536870912, old_weight 740000000000001, new_weight 740000000000001
+1697602047   spa_history.c:294:spa_history_log_sync(): command: zpool create test raidz /scratch/1.img /scratch/2.img /scratch/3.img
diff --git a/module/Makefile.bsd b/module/Makefile.bsd
index 6bd78690f..6cfb21bd2 100644
--- a/module/Makefile.bsd
+++ b/module/Makefile.bsd
@@ -258,9 +258,11 @@ SRCS+=	abd.c \
 	vdev_initialize.c \
 	vdev_label.c \
 	vdev_mirror.c \
+	vdev_my_mirror.c \
 	vdev_missing.c \
 	vdev_queue.c \
 	vdev_raidz.c \
+	vdev_my_raidz.c \
 	vdev_raidz_math.c \
 	vdev_raidz_math_scalar.c \
 	vdev_rebuild.c \
diff --git a/module/nvpair/nvpair.c b/module/nvpair/nvpair.c
index 9834dedd8..3d5da0527 100644
--- a/module/nvpair/nvpair.c
+++ b/module/nvpair/nvpair.c
@@ -2731,6 +2731,7 @@ nvlist_xunpack(char *buf, size_t buflen, nvlist_t **nvlp, nv_alloc_t *nva)
 	if ((err = nvlist_xalloc(&nvl, 0, nva)) != 0)
 		return (err);
 
+
 	if ((err = nvlist_common(nvl, buf, &buflen, NV_ENCODE_NATIVE,
 	    NVS_OP_DECODE)) != 0)
 		nvlist_free(nvl);
diff --git a/module/os/linux/zfs/vdev_file.c b/module/os/linux/zfs/vdev_file.c
index 98338e604..986f08ff5 100644
--- a/module/os/linux/zfs/vdev_file.c
+++ b/module/os/linux/zfs/vdev_file.c
@@ -209,6 +209,8 @@ vdev_file_io_strategy(void *arg)
 	size = zio->io_size;
 	resid = 0;
 
+	zfs_dbgmsg("file dev %s write at offset %llu, size %ld", vd->vdev_path, (u_longlong_t)off, size);
+
 	if (zio->io_type == ZIO_TYPE_READ) {
 		buf = abd_borrow_buf(zio->io_abd, zio->io_size);
 		err = zfs_file_pread(vf->vf_file, buf, size, off, &resid);
@@ -239,6 +241,7 @@ vdev_file_io_fsync(void *arg)
 static void
 vdev_file_io_start(zio_t *zio)
 {
+	zfs_dbgmsg("vdev_file_io_start called");
 	vdev_t *vd = zio->io_vd;
 	vdev_file_t *vf = vd->vdev_tsd;
 
diff --git a/module/os/linux/zfs/zfs_ioctl_os.c b/module/os/linux/zfs/zfs_ioctl_os.c
index 767d3a377..7116fcd28 100644
--- a/module/os/linux/zfs/zfs_ioctl_os.c
+++ b/module/os/linux/zfs/zfs_ioctl_os.c
@@ -180,7 +180,8 @@ zfsdev_release(struct inode *ino, struct file *filp)
 
 static long
 zfsdev_ioctl(struct file *filp, unsigned cmd, unsigned long arg)
-{
+{\
+	zfs_dbgmsg("zfsdev_ioctl() called with cmd %d\n", cmd);
 	uint_t vecnum;
 	zfs_cmd_t *zc;
 	int error, rc;
diff --git a/module/zfs/Makefile.in b/module/zfs/Makefile.in
index 0e04d7ef0..0ec158e00 100644
--- a/module/zfs/Makefile.in
+++ b/module/zfs/Makefile.in
@@ -92,9 +92,11 @@ $(MODULE)-objs += vdev_indirect_mapping.o
 $(MODULE)-objs += vdev_initialize.o
 $(MODULE)-objs += vdev_label.o
 $(MODULE)-objs += vdev_mirror.o
+$(MODULE)-objs += vdev_my_mirror.o
 $(MODULE)-objs += vdev_missing.o
 $(MODULE)-objs += vdev_queue.o
 $(MODULE)-objs += vdev_raidz.o
+$(MODULE)-objs += vdev_my_raidz.o
 $(MODULE)-objs += vdev_raidz_math.o
 $(MODULE)-objs += vdev_raidz_math_scalar.o
 $(MODULE)-objs += vdev_rebuild.o
diff --git a/module/zfs/dnode.c b/module/zfs/dnode.c
index ed75c3bdf..fc8420832 100644
--- a/module/zfs/dnode.c
+++ b/module/zfs/dnode.c
@@ -1419,6 +1419,7 @@ dnode_hold_impl(objset_t *os, uint64_t object, int flag, int slots,
 	 * which may require us to read from the root filesystem while
 	 * holding some (not all) of the locks as writer.
 	 */
+	zfs_dbgmsg("spa_config_held %d", spa_config_held(os->os_spa, SCL_ALL, RW_WRITER));
 	ASSERT(spa_config_held(os->os_spa, SCL_ALL, RW_WRITER) == 0 ||
 	    (spa_is_root(os->os_spa) &&
 	    spa_config_held(os->os_spa, SCL_STATE, RW_WRITER)));
@@ -1710,6 +1711,8 @@ dnode_add_ref(dnode_t *dn, void *tag)
 		mutex_exit(&dn->dn_mtx);
 		return (FALSE);
 	}
+
+	// zfs_dbgmsg("dnode ref count %lld", dn->dn_holds.rc_count);
 	VERIFY(1 < zfs_refcount_add(&dn->dn_holds, tag));
 	mutex_exit(&dn->dn_mtx);
 	return (TRUE);
diff --git a/module/zfs/dsl_dataset.c b/module/zfs/dsl_dataset.c
index 4e5a0606f..97d67568a 100644
--- a/module/zfs/dsl_dataset.c
+++ b/module/zfs/dsl_dataset.c
@@ -569,13 +569,16 @@ dsl_dataset_hold_obj(dsl_pool_t *dp, uint64_t dsobj, void *tag,
 	ASSERT(dsl_pool_config_held(dp));
 
 	err = dmu_bonus_hold(mos, dsobj, tag, &dbuf);
-	if (err != 0)
+	if (err != 0) {
+		zfs_dbgmsg("dmu bonus hold failed");
 		return (err);
+	}
 
 	/* Make sure dsobj has the correct object type. */
 	dmu_object_info_from_db(dbuf, &doi);
 	if (doi.doi_bonus_type != DMU_OT_DSL_DATASET) {
 		dmu_buf_rele(dbuf, tag);
+		zfs_dbgmsg("doi bonus allocation failed");
 		return (SET_ERROR(EINVAL));
 	}
 
@@ -594,6 +597,7 @@ dsl_dataset_hold_obj(dsl_pool_t *dp, uint64_t dsobj, void *tag,
 		if (err != 0) {
 			kmem_free(ds, sizeof (dsl_dataset_t));
 			dmu_buf_rele(dbuf, tag);
+			zfs_dbgmsg("dsl_dir_hold_obj error out");
 			return (err);
 		}
 
diff --git a/module/zfs/dsl_scan.c b/module/zfs/dsl_scan.c
index f0a851ff5..15b4f2ba8 100644
--- a/module/zfs/dsl_scan.c
+++ b/module/zfs/dsl_scan.c
@@ -873,6 +873,7 @@ dsl_scan(dsl_pool_t *dp, pool_scan_func_t func)
 		return (SET_ERROR(err));
 	}
 
+
 	return (dsl_sync_task(spa_name(spa), dsl_scan_setup_check,
 	    dsl_scan_setup_sync, &func, 0, ZFS_SPACE_CHECK_EXTRA_RESERVED));
 }
diff --git a/module/zfs/mmp.c b/module/zfs/mmp.c
index 139bb0acd..d12374fca 100644
--- a/module/zfs/mmp.c
+++ b/module/zfs/mmp.c
@@ -674,7 +674,7 @@ mmp_thread(void *arg)
 			    spa_name(spa),
 			    NSEC2MSEC(gethrtime() - mmp->mmp_last_write),
 			    gethrtime());
-			zio_suspend(spa, NULL, ZIO_SUSPEND_MMP);
+			// zio_suspend(spa, NULL, ZIO_SUSPEND_MMP);
 		}
 
 		if (multihost && !suspended)
diff --git a/module/zfs/spa.c b/module/zfs/spa.c
index 5f238e691..17e0bdf3f 100644
--- a/module/zfs/spa.c
+++ b/module/zfs/spa.c
@@ -160,6 +160,7 @@ const zio_taskq_info_t zio_taskqs[ZIO_TYPES][ZIO_TASKQ_TYPES] = {
 	{ ZTI_ONE,	ZTI_NULL,	ZTI_ONE,	ZTI_NULL }, /* CLAIM */
 	{ ZTI_ONE,	ZTI_NULL,	ZTI_ONE,	ZTI_NULL }, /* IOCTL */
 	{ ZTI_N(4),	ZTI_NULL,	ZTI_ONE,	ZTI_NULL }, /* TRIM */
+	{ ZTI_ONE,	ZTI_NULL,	ZTI_ONE,	ZTI_NULL }  /* MLEC stuff */
 };
 
 static void spa_sync_version(void *arg, dmu_tx_t *tx);
@@ -979,6 +980,8 @@ spa_taskqs_init(spa_t *spa, zio_type_t t, zio_taskq_type_t q)
 	uint_t cpus, flags = TASKQ_DYNAMIC;
 	boolean_t batch = B_FALSE;
 
+	tqs->stqs_count = 0;
+
 	switch (mode) {
 	case ZTI_MODE_FIXED:
 		ASSERT3U(value, >, 0);
@@ -1033,13 +1036,17 @@ spa_taskqs_init(spa_t *spa, zio_type_t t, zio_taskq_type_t q)
 		return;
 
 	default:
-		panic("unrecognized mode for %s_%s taskq (%u:%u) in "
+		zfs_dbgmsg("unrecognized mode for %s_%s taskq (%u:%u) in "
 		    "spa_activate()",
 		    zio_type_name[t], zio_taskq_types[q], mode, value);
+		// panic("unrecognized mode for %s_%s taskq (%u:%u) in "
+		//     "spa_activate()",
+		//     zio_type_name[t], zio_taskq_types[q], mode, value);
 		break;
 	}
 
 	ASSERT3U(count, >, 0);
+	zfs_dbgmsg("Assigning tqs count %llu",  (longlong_t) count);
 	tqs->stqs_count = count;
 	tqs->stqs_taskq = kmem_alloc(count * sizeof (taskq_t *), KM_SLEEP);
 
@@ -1121,18 +1128,24 @@ void
 spa_taskq_dispatch_ent(spa_t *spa, zio_type_t t, zio_taskq_type_t q,
     task_func_t *func, void *arg, uint_t flags, taskq_ent_t *ent)
 {
+	zfs_dbgmsg("spa_taskq_dispatch called for type %d-%d", t, q);
+	ASSERT3P(spa, !=, NULL);
 	spa_taskqs_t *tqs = &spa->spa_zio_taskq[t][q];
 	taskq_t *tq;
 
+	ASSERT3P(tqs, !=, NULL);
 	ASSERT3P(tqs->stqs_taskq, !=, NULL);
 	ASSERT3U(tqs->stqs_count, !=, 0);
 
+	zfs_dbgmsg("spa_taskq_dispatch passed assertion with stqs_count %x", (uint_t) tqs->stqs_count);
 	if (tqs->stqs_count == 1) {
 		tq = tqs->stqs_taskq[0];
 	} else {
+		zfs_dbgmsg("stqs count not 1 %d %llu %ld", tqs->stqs_count, (longlong_t)((uint64_t)gethrtime()), ((uint64_t)gethrtime()) % tqs->stqs_count);
 		tq = tqs->stqs_taskq[((uint64_t)gethrtime()) % tqs->stqs_count];
 	}
 
+	zfs_dbgmsg("Dispatching taskq ent");
 	taskq_dispatch_ent(tq, func, arg, flags, ent);
 }
 
@@ -1166,6 +1179,7 @@ spa_create_zio_taskqs(spa_t *spa)
 {
 	for (int t = 0; t < ZIO_TYPES; t++) {
 		for (int q = 0; q < ZIO_TASKQ_TYPES; q++) {
+			zfs_dbgmsg("Initializing task queue %d-%d", t, q);
 			spa_taskqs_init(spa, t, q);
 		}
 	}
@@ -5147,6 +5161,7 @@ static int
 spa_open_common(const char *pool, spa_t **spapp, void *tag, nvlist_t *nvpolicy,
     nvlist_t **config)
 {
+	zfs_dbgmsg("spa_open_common called");
 	spa_t *spa;
 	spa_load_state_t state = SPA_LOAD_OPEN;
 	int error;
@@ -5165,6 +5180,7 @@ spa_open_common(const char *pool, spa_t **spapp, void *tag, nvlist_t *nvpolicy,
 		mutex_enter(&spa_namespace_lock);
 		locked = B_TRUE;
 	}
+	zfs_dbgmsg("spa namespace lock acquired");
 
 	if ((spa = spa_lookup(pool)) == NULL) {
 		if (locked)
@@ -5172,6 +5188,8 @@ spa_open_common(const char *pool, spa_t **spapp, void *tag, nvlist_t *nvpolicy,
 		return (SET_ERROR(ENOENT));
 	}
 
+	zfs_dbgmsg("spa_lookup good");
+
 	if (spa->spa_state == POOL_STATE_UNINITIALIZED) {
 		zpool_load_policy_t policy;
 
@@ -5246,6 +5264,7 @@ spa_open_common(const char *pool, spa_t **spapp, void *tag, nvlist_t *nvpolicy,
 	}
 
 	if (locked) {
+		zfs_dbgmsg("spa_lookup lock is held");
 		spa->spa_last_open_failed = 0;
 		spa->spa_last_ubsync_txg = 0;
 		spa->spa_load_txg = 0;
@@ -5255,6 +5274,7 @@ spa_open_common(const char *pool, spa_t **spapp, void *tag, nvlist_t *nvpolicy,
 	if (firstopen)
 		zvol_create_minors_recursive(spa_name(spa));
 
+	zfs_dbgmsg("spa_open reference acquired");
 	*spapp = spa;
 
 	return (0);
@@ -9219,7 +9239,7 @@ spa_sync_rewrite_vdev_config(spa_t *spa, dmu_tx_t *tx)
 
 		if (error == 0)
 			break;
-		zio_suspend(spa, NULL, ZIO_SUSPEND_IOERR);
+		// zio_suspend(spa, NULL, ZIO_SUSPEND_IOERR);
 		zio_resume_wait(spa);
 	}
 }
diff --git a/module/zfs/vdev.c b/module/zfs/vdev.c
index 57259b8ce..5326a9062 100644
--- a/module/zfs/vdev.c
+++ b/module/zfs/vdev.c
@@ -236,6 +236,8 @@ static vdev_ops_t *vdev_ops_table[] = {
 	&vdev_missing_ops,
 	&vdev_hole_ops,
 	&vdev_indirect_ops,
+	&vdev_my_mirror_ops,
+	&vdev_my_raidz_ops,
 	NULL
 };
 
@@ -247,6 +249,8 @@ vdev_getops(const char *type)
 {
 	vdev_ops_t *ops, **opspp;
 
+	/* zfs_dbgmsg("type: %s", vdev_my_mirror_ops.vdev_op_type); */
+
 	for (opspp = vdev_ops_table; (ops = *opspp) != NULL; opspp++)
 		if (strcmp(ops->vdev_op_type, type) == 0)
 			break;
@@ -707,9 +711,12 @@ vdev_alloc(spa_t *spa, vdev_t **vdp, nvlist_t *nv, vdev_t *parent, uint_t id,
 	if (nvlist_lookup_string(nv, ZPOOL_CONFIG_TYPE, &type) != 0)
 		return (SET_ERROR(EINVAL));
 
+
 	if ((ops = vdev_getops(type)) == NULL)
 		return (SET_ERROR(EINVAL));
 
+	zfs_dbgmsg("get ops sucessful");
+
 	/*
 	 * If this is a load, get the vdev guid from the nvlist.
 	 * Otherwise, vdev_alloc_common() will generate one for us.
@@ -734,12 +741,15 @@ vdev_alloc(spa_t *spa, vdev_t **vdp, nvlist_t *nv, vdev_t *parent, uint_t id,
 			return (SET_ERROR(EINVAL));
 	}
 
+	zfs_dbgmsg("<2>");
+	zfs_dbgmsg("%s", ops->vdev_op_type);
 	/*
 	 * The first allocated vdev must be of type 'root'.
 	 */
 	if (ops != &vdev_root_ops && spa->spa_root_vdev == NULL)
 		return (SET_ERROR(EINVAL));
 
+	zfs_dbgmsg("non-root");
 	/*
 	 * Determine whether we're a log vdev.
 	 */
@@ -751,6 +761,7 @@ vdev_alloc(spa_t *spa, vdev_t **vdp, nvlist_t *nv, vdev_t *parent, uint_t id,
 	if (ops == &vdev_hole_ops && spa_version(spa) < SPA_VERSION_HOLES)
 		return (SET_ERROR(ENOTSUP));
 
+	zfs_dbgmsg("next step");
 	if (top_level && alloctype == VDEV_ALLOC_ADD) {
 		char *bias;
 
@@ -778,6 +789,7 @@ vdev_alloc(spa_t *spa, vdev_t **vdp, nvlist_t *nv, vdev_t *parent, uint_t id,
 		}
 	}
 
+	zfs_dbgmsg("<2.5>");
 	/*
 	 * Initialize the vdev specific data.  This is done before calling
 	 * vdev_alloc_common() since it may fail and this simplifies the
@@ -790,7 +802,7 @@ vdev_alloc(spa_t *spa, vdev_t **vdp, nvlist_t *nv, vdev_t *parent, uint_t id,
 			return (rc);
 		}
 	}
-
+	zfs_dbgmsg("<3>");
 	vd = vdev_alloc_common(spa, id, guid, ops);
 	vd->vdev_tsd = tsd;
 	vd->vdev_islog = islog;
@@ -826,6 +838,7 @@ vdev_alloc(spa_t *spa, vdev_t **vdp, nvlist_t *nv, vdev_t *parent, uint_t id,
 	if (nvlist_lookup_string(nv, ZPOOL_CONFIG_FRU, &vd->vdev_fru) == 0)
 		vd->vdev_fru = spa_strdup(vd->vdev_fru);
 
+	zfs_dbgmsg("<4>");
 	/*
 	 * Set the whole_disk property.  If it's not specified, leave the value
 	 * as -1.
@@ -899,6 +912,7 @@ vdev_alloc(spa_t *spa, vdev_t **vdp, nvlist_t *nv, vdev_t *parent, uint_t id,
 		ASSERT0(vd->vdev_leaf_zap);
 	}
 
+	zfs_dbgmsg("<5>");
 	/*
 	 * If we're a leaf vdev, try to load the DTL object and other state.
 	 */
@@ -2254,11 +2268,11 @@ vdev_validate(vdev_t *vd)
 		txg = spa_last_synced_txg(spa);
 
 	if ((label = vdev_label_read_config(vd, txg)) == NULL) {
-		vdev_set_state(vd, B_FALSE, VDEV_STATE_CANT_OPEN,
-		    VDEV_AUX_BAD_LABEL);
+		//vdev_set_state(vd, B_FALSE, VDEV_STATE_CANT_OPEN,
+		    //VDEV_AUX_BAD_LABEL);
 		vdev_dbgmsg(vd, "vdev_validate: failed reading config for "
 		    "txg %llu", (u_longlong_t)txg);
-		return (0);
+		return (77);
 	}
 
 	/*
@@ -2607,10 +2621,11 @@ vdev_rele(vdev_t *vd)
  * on the spa_config_lock.  Instead we only obtain the leaf's physical size.
  * If the leaf has never been opened then open it, as usual.
  */
-void
+int
 vdev_reopen(vdev_t *vd)
 {
 	spa_t *spa = vd->vdev_spa;
+	int err = 0;
 
 	ASSERT(spa_config_held(spa, SCL_STATE_ALL, RW_WRITER) == SCL_STATE_ALL);
 
@@ -2642,7 +2657,7 @@ vdev_reopen(vdev_t *vd)
 			spa_async_request(spa, SPA_ASYNC_L2CACHE_TRIM);
 		}
 	} else {
-		(void) vdev_validate(vd);
+		err = vdev_validate(vd);
 	}
 
 	/*
@@ -2660,6 +2675,9 @@ vdev_reopen(vdev_t *vd)
 	 * Reassess parent vdev's health.
 	 */
 	vdev_propagate_state(vd);
+		
+	zfs_dbgmsg("%d", err);
+	return err;
 }
 
 int
@@ -3877,6 +3895,7 @@ vdev_psize_to_asize(vdev_t *vd, uint64_t psize)
 int
 vdev_fault(spa_t *spa, uint64_t guid, vdev_aux_t aux)
 {
+	zfs_dbgmsg("vdev_fault called");
 	vdev_t *vd, *tvd;
 
 	spa_vdev_state_enter(spa, SCL_NONE);
@@ -3926,17 +3945,17 @@ vdev_fault(spa_t *spa, uint64_t guid, vdev_aux_t aux)
 	 * Faulted state takes precedence over degraded.
 	 */
 	vd->vdev_delayed_close = B_FALSE;
-	vd->vdev_faulted = 1ULL;
-	vd->vdev_degraded = 0ULL;
-	vdev_set_state(vd, B_FALSE, VDEV_STATE_FAULTED, aux);
+	// vd->vdev_faulted = 1ULL;
+	// vd->vdev_degraded = 0ULL;
+	// vdev_set_state(vd, B_FALSE, VDEV_STATE_FAULTED, aux);
 
 	/*
 	 * If this device has the only valid copy of the data, then
 	 * back off and simply mark the vdev as degraded instead.
 	 */
 	if (!tvd->vdev_islog && vd->vdev_aux == NULL && vdev_dtl_required(vd)) {
-		vd->vdev_degraded = 1ULL;
-		vd->vdev_faulted = 0ULL;
+		// vd->vdev_degraded = 1ULL;
+		// vd->vdev_faulted = 0ULL;
 
 		/*
 		 * If we reopen the device and it's not dead, only then do we
@@ -5093,6 +5112,7 @@ vdev_propagate_state(vdev_t *vd)
 void
 vdev_set_state(vdev_t *vd, boolean_t isopen, vdev_state_t state, vdev_aux_t aux)
 {
+	zfs_dbgmsg("vdev_set_state called with state %d", state);
 	uint64_t save_state;
 	spa_t *spa = vd->vdev_spa;
 
diff --git a/module/zfs/vdev_my_mirror.c b/module/zfs/vdev_my_mirror.c
new file mode 100644
index 000000000..b15400d14
--- /dev/null
+++ b/module/zfs/vdev_my_mirror.c
@@ -0,0 +1,975 @@
+/*
+ * CDDL HEADER START
+ *
+ * The contents of this file are subject to the terms of the
+ * Common Development and Distribution License (the "License").
+ * You may not use this file except in compliance with the License.
+ *
+ * You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+ * or http://www.opensolaris.org/os/licensing.
+ * See the License for the specific language governing permissions
+ * and limitations under the License.
+ *
+ * When distributing Covered Code, include this CDDL HEADER in each
+ * file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+ * If applicable, add the following below this CDDL HEADER, with the
+ * fields enclosed by brackets "[]" replaced with your own identifying
+ * information: Portions Copyright [yyyy] [name of copyright owner]
+ *
+ * CDDL HEADER END
+ */
+/*
+ * Copyright 2010 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ */
+
+/*
+ * Copyright (c) 2012, 2015 by Delphix. All rights reserved.
+ */
+
+#include <sys/zfs_context.h>
+#include <sys/spa.h>
+#include <sys/spa_impl.h>
+#include <sys/dsl_pool.h>
+#include <sys/dsl_scan.h>
+#include <sys/vdev_impl.h>
+#include <sys/vdev_draid.h>
+#include <sys/zio.h>
+#include <sys/zio_checksum.h>
+#include <sys/abd.h>
+#include <sys/fs/zfs.h>
+
+/*
+ * Vdev my_mirror kstats
+ */
+
+typedef struct my_mirror_stats {
+	kstat_named_t vdev_my_mirror_stat_rotating_linear;
+	kstat_named_t vdev_my_mirror_stat_rotating_offset;
+	kstat_named_t vdev_my_mirror_stat_rotating_seek;
+	kstat_named_t vdev_my_mirror_stat_non_rotating_linear;
+	kstat_named_t vdev_my_mirror_stat_non_rotating_seek;
+
+	kstat_named_t vdev_my_mirror_stat_preferred_found;
+	kstat_named_t vdev_my_mirror_stat_preferred_not_found;
+} my_mirror_stats_t;
+
+static my_mirror_stats_t my_mirror_stats = {
+	/* New I/O follows directly the last I/O */
+	{ "rotating_linear",			KSTAT_DATA_UINT64 },
+	/* New I/O is within zfs_vdev_my_mirror_rotating_seek_offset of the last */
+	{ "rotating_offset",			KSTAT_DATA_UINT64 },
+	/* New I/O requires random seek */
+	{ "rotating_seek",			KSTAT_DATA_UINT64 },
+	/* New I/O follows directly the last I/O  (nonrot) */
+	{ "non_rotating_linear",		KSTAT_DATA_UINT64 },
+	/* New I/O requires random seek (nonrot) */
+	{ "non_rotating_seek",			KSTAT_DATA_UINT64 },
+	/* Preferred child vdev found */
+	{ "preferred_found",			KSTAT_DATA_UINT64 },
+	/* Preferred child vdev not found or equal load  */
+	{ "preferred_not_found",		KSTAT_DATA_UINT64 },
+
+};
+
+#define	my_mirror_STAT(stat)		(my_mirror_stats.stat.value.ui64)
+#define	my_mirror_INCR(stat, val) 		atomic_add_64(&my_mirror_STAT(stat), val)
+#define	my_mirror_BUMP(stat)		my_mirror_INCR(stat, 1)
+
+/*
+
+decide later whether they are needed 
+
+void
+vdev_my_mirror_stat_init(void)
+{
+	my_mirror_ksp = kstat_create("zfs", 0, "vdev_my_mirror_stats",
+	    "misc", KSTAT_TYPE_NAMED,
+	    sizeof (my_mirror_stats) / sizeof (kstat_named_t), KSTAT_FLAG_VIRTUAL);
+	if (my_mirror_ksp != NULL) {
+		my_mirror_ksp->ks_data = &my_mirror_stats;
+		kstat_install(my_mirror_ksp);
+	}
+}
+
+void
+vdev_my_mirror_stat_fini(void)
+{
+	if (my_mirror_ksp != NULL) {
+		kstat_delete(my_mirror_ksp);
+		my_mirror_ksp = NULL;
+	}
+}
+*/
+
+/*
+ * Virtual device vector for my_mirroring.
+ */
+typedef struct my_mirror_child {
+	vdev_t		*mc_vd;
+	abd_t		*mc_abd;
+	uint64_t	mc_offset;
+	int		mc_error;
+	int		mc_load;
+	uint8_t		mc_tried;
+	uint8_t		mc_skipped;
+	uint8_t		mc_speculative;
+	uint8_t		mc_rebuilding;
+} my_mirror_child_t;
+
+typedef struct my_mirror_map {
+	int		*mm_preferred;
+	int		mm_preferred_cnt;
+	int		mm_children;
+	boolean_t	mm_resilvering;
+	boolean_t	mm_rebuilding;
+	boolean_t	mm_root;
+	my_mirror_child_t	mm_child[];
+} my_mirror_map_t;
+
+static int vdev_my_mirror_shift = 21;
+
+/*
+ * The load configuration settings below are tuned by default for
+ * the case where all devices are of the same rotational type.
+ *
+ * If there is a mixture of rotating and non-rotating media, setting
+ * zfs_vdev_my_mirror_non_rotating_seek_inc to 0 may well provide better results
+ * as it will direct more reads to the non-rotating vdevs which are more likely
+ * to have a higher performance.
+ */
+
+/* Rotating media load calculation configuration. */
+static int zfs_vdev_my_mirror_rotating_inc = 0;
+static int zfs_vdev_my_mirror_rotating_seek_inc = 5;
+static int zfs_vdev_my_mirror_rotating_seek_offset = 1 * 1024 * 1024;
+
+/* Non-rotating media load calculation configuration. */
+static int zfs_vdev_my_mirror_non_rotating_inc = 0;
+static int zfs_vdev_my_mirror_non_rotating_seek_inc = 1;
+
+static inline size_t
+vdev_my_mirror_map_size(int children)
+{
+	return (offsetof(my_mirror_map_t, mm_child[children]) +
+	    sizeof (int) * children);
+}
+
+static inline my_mirror_map_t *
+vdev_my_mirror_map_alloc(int children, boolean_t resilvering, boolean_t root)
+{
+	my_mirror_map_t *mm;
+
+	mm = kmem_zalloc(vdev_my_mirror_map_size(children), KM_SLEEP);
+	mm->mm_children = children;
+	mm->mm_resilvering = resilvering;
+	mm->mm_root = root;
+	mm->mm_preferred = (int *)((uintptr_t)mm +
+	    offsetof(my_mirror_map_t, mm_child[children]));
+
+	return (mm);
+}
+
+static void
+vdev_my_mirror_map_free(zio_t *zio)
+{
+	my_mirror_map_t *mm = zio->io_vsd;
+
+	kmem_free(mm, vdev_my_mirror_map_size(mm->mm_children));
+}
+
+static const zio_vsd_ops_t vdev_my_mirror_vsd_ops = {
+	.vsd_free = vdev_my_mirror_map_free,
+};
+
+static int
+vdev_my_mirror_load(my_mirror_map_t *mm, vdev_t *vd, uint64_t zio_offset)
+{
+	uint64_t last_offset;
+	int64_t offset_diff;
+	int load;
+
+	/* All DVAs have equal weight at the root. */
+	if (mm->mm_root)
+		return (INT_MAX);
+
+	/*
+	 * We don't return INT_MAX if the device is resilvering i.e.
+	 * vdev_resilver_txg != 0 as when tested performance was slightly
+	 * worse overall when resilvering with compared to without.
+	 */
+
+	/* Fix zio_offset for leaf vdevs */
+	if (vd->vdev_ops->vdev_op_leaf)
+		zio_offset += VDEV_LABEL_START_SIZE;
+
+	/* Standard load based on pending queue length. */
+	load = vdev_queue_length(vd);
+	last_offset = vdev_queue_last_offset(vd);
+
+	if (vd->vdev_nonrot) {
+		/* Non-rotating media. */
+		if (last_offset == zio_offset) {
+			my_mirror_BUMP(vdev_my_mirror_stat_non_rotating_linear);
+			return (load + zfs_vdev_my_mirror_non_rotating_inc);
+		}
+
+		/*
+		 * Apply a seek penalty even for non-rotating devices as
+		 * sequential I/O's can be aggregated into fewer operations on
+		 * the device, thus avoiding unnecessary per-command overhead
+		 * and boosting performance.
+		 */
+		my_mirror_BUMP(vdev_my_mirror_stat_non_rotating_seek);
+		return (load + zfs_vdev_my_mirror_non_rotating_seek_inc);
+	}
+
+	/* Rotating media I/O's which directly follow the last I/O. */
+	if (last_offset == zio_offset) {
+		my_mirror_BUMP(vdev_my_mirror_stat_rotating_linear);
+		return (load + zfs_vdev_my_mirror_rotating_inc);
+	}
+
+	/*
+	 * Apply half the seek increment to I/O's within seek offset
+	 * of the last I/O issued to this vdev as they should incur less
+	 * of a seek increment.
+	 */
+	offset_diff = (int64_t)(last_offset - zio_offset);
+	if (ABS(offset_diff) < zfs_vdev_my_mirror_rotating_seek_offset) {
+		my_mirror_BUMP(vdev_my_mirror_stat_rotating_offset);
+		return (load + (zfs_vdev_my_mirror_rotating_seek_inc / 2));
+	}
+
+	/* Apply the full seek increment to all other I/O's. */
+	my_mirror_BUMP(vdev_my_mirror_stat_rotating_seek);
+	return (load + zfs_vdev_my_mirror_rotating_seek_inc);
+}
+
+static boolean_t
+vdev_my_mirror_rebuilding(vdev_t *vd)
+{
+	if (vd->vdev_ops->vdev_op_leaf && vd->vdev_rebuild_txg)
+		return (B_TRUE);
+
+	for (int i = 0; i < vd->vdev_children; i++) {
+		if (vdev_my_mirror_rebuilding(vd->vdev_child[i])) {
+			return (B_TRUE);
+		}
+	}
+
+	return (B_FALSE);
+}
+
+/*
+ * Avoid inlining the function to keep vdev_my_mirror_io_start(), which
+ * is this functions only caller, as small as possible on the stack.
+ */
+noinline static my_mirror_map_t *
+vdev_my_mirror_map_init(zio_t *zio)
+{
+	my_mirror_map_t *mm = NULL;
+	my_mirror_child_t *mc;
+	vdev_t *vd = zio->io_vd;
+	int c;
+
+	if (vd == NULL) {
+		dva_t *dva = zio->io_bp->blk_dva;
+		spa_t *spa = zio->io_spa;
+		dsl_scan_t *scn = spa->spa_dsl_pool->dp_scan;
+		dva_t dva_copy[SPA_DVAS_PER_BP];
+
+		/*
+		 * The sequential scrub code sorts and issues all DVAs
+		 * of a bp separately. Each of these IOs includes all
+		 * original DVA copies so that repairs can be performed
+		 * in the event of an error, but we only actually want
+		 * to check the first DVA since the others will be
+		 * checked by their respective sorted IOs. Only if we
+		 * hit an error will we try all DVAs upon retrying.
+		 *
+		 * Note: This check is safe even if the user switches
+		 * from a legacy scrub to a sequential one in the middle
+		 * of processing, since scn_is_sorted isn't updated until
+		 * all outstanding IOs from the previous scrub pass
+		 * complete.
+		 */
+		if ((zio->io_flags & ZIO_FLAG_SCRUB) &&
+		    !(zio->io_flags & ZIO_FLAG_IO_RETRY) &&
+		    dsl_scan_scrubbing(spa->spa_dsl_pool) &&
+		    scn->scn_is_sorted) {
+			c = 1;
+		} else {
+			c = BP_GET_NDVAS(zio->io_bp);
+		}
+
+		/*
+		 * If the pool cannot be written to, then infer that some
+		 * DVAs might be invalid or point to vdevs that do not exist.
+		 * We skip them.
+		 */
+		if (!spa_writeable(spa)) {
+			ASSERT3U(zio->io_type, ==, ZIO_TYPE_READ);
+			int j = 0;
+			for (int i = 0; i < c; i++) {
+				if (zfs_dva_valid(spa, &dva[i], zio->io_bp))
+					dva_copy[j++] = dva[i];
+			}
+			if (j == 0) {
+				zio->io_vsd = NULL;
+				zio->io_error = ENXIO;
+				return (NULL);
+			}
+			if (j < c) {
+				dva = dva_copy;
+				c = j;
+			}
+		}
+
+		mm = vdev_my_mirror_map_alloc(c, B_FALSE, B_TRUE);
+		for (c = 0; c < mm->mm_children; c++) {
+			mc = &mm->mm_child[c];
+
+			mc->mc_vd = vdev_lookup_top(spa, DVA_GET_VDEV(&dva[c]));
+			mc->mc_offset = DVA_GET_OFFSET(&dva[c]);
+			if (mc->mc_vd == NULL) {
+				kmem_free(mm, vdev_my_mirror_map_size(
+				    mm->mm_children));
+				zio->io_vsd = NULL;
+				zio->io_error = ENXIO;
+				return (NULL);
+			}
+		}
+	} else {
+		/*
+		 * If we are resilvering, then we should handle scrub reads
+		 * differently; we shouldn't issue them to the resilvering
+		 * device because it might not have those blocks.
+		 *
+		 * We are resilvering iff:
+		 * 1) We are a replacing vdev (ie our name is "replacing-1" or
+		 *    "spare-1" or something like that), and
+		 * 2) The pool is currently being resilvered.
+		 *
+		 * We cannot simply check vd->vdev_resilver_txg, because it's
+		 * not set in this path.
+		 *
+		 * Nor can we just check our vdev_ops; there are cases (such as
+		 * when a user types "zpool replace pool odev spare_dev" and
+		 * spare_dev is in the spare list, or when a spare device is
+		 * automatically used to replace a DEGRADED device) when
+		 * resilvering is complete but both the original vdev and the
+		 * spare vdev remain in the pool.  That behavior is intentional.
+		 * It helps implement the policy that a spare should be
+		 * automatically removed from the pool after the user replaces
+		 * the device that originally failed.
+		 *
+		 * If a spa load is in progress, then spa_dsl_pool may be
+		 * uninitialized.  But we shouldn't be resilvering during a spa
+		 * load anyway.
+		 */
+		boolean_t replacing = (vd->vdev_ops == &vdev_replacing_ops ||
+		    vd->vdev_ops == &vdev_spare_ops) &&
+		    spa_load_state(vd->vdev_spa) == SPA_LOAD_NONE &&
+		    dsl_scan_resilvering(vd->vdev_spa->spa_dsl_pool);
+		mm = vdev_my_mirror_map_alloc(vd->vdev_children, replacing,
+		    B_FALSE);
+		for (c = 0; c < mm->mm_children; c++) {
+			mc = &mm->mm_child[c];
+			mc->mc_vd = vd->vdev_child[c];
+			mc->mc_offset = zio->io_offset;
+
+			if (vdev_my_mirror_rebuilding(mc->mc_vd))
+				mm->mm_rebuilding = mc->mc_rebuilding = B_TRUE;
+		}
+	}
+
+	return (mm);
+}
+
+static int
+vdev_my_mirror_open(vdev_t *vd, uint64_t *asize, uint64_t *max_asize,
+    uint64_t *logical_ashift, uint64_t *physical_ashift)
+{
+	int numerrors = 0;
+	int lasterror = 0;
+
+	if (vd->vdev_children == 0) {
+		vd->vdev_stat.vs_aux = VDEV_AUX_BAD_LABEL;
+		return (SET_ERROR(EINVAL));
+	}
+
+	vdev_open_children(vd);
+
+	for (int c = 0; c < vd->vdev_children; c++) {
+		vdev_t *cvd = vd->vdev_child[c];
+
+		if (cvd->vdev_open_error) {
+			lasterror = cvd->vdev_open_error;
+			numerrors++;
+			continue;
+		}
+
+		*asize = MIN(*asize - 1, cvd->vdev_asize - 1) + 1;
+		*max_asize = MIN(*max_asize - 1, cvd->vdev_max_asize - 1) + 1;
+		*logical_ashift = MAX(*logical_ashift, cvd->vdev_ashift);
+	}
+	for (int c = 0; c < vd->vdev_children; c++) {
+		vdev_t *cvd = vd->vdev_child[c];
+
+		if (cvd->vdev_open_error)
+			continue;
+		*physical_ashift = vdev_best_ashift(*logical_ashift,
+		    *physical_ashift, cvd->vdev_physical_ashift);
+	}
+
+	if (numerrors == vd->vdev_children) {
+		if (vdev_children_are_offline(vd))
+			vd->vdev_stat.vs_aux = VDEV_AUX_CHILDREN_OFFLINE;
+		else
+			vd->vdev_stat.vs_aux = VDEV_AUX_NO_REPLICAS;
+		return (lasterror);
+	}
+
+	return (0);
+}
+
+static void
+vdev_my_mirror_close(vdev_t *vd)
+{
+	for (int c = 0; c < vd->vdev_children; c++)
+		vdev_close(vd->vdev_child[c]);
+}
+
+static void
+vdev_my_mirror_child_done(zio_t *zio)
+{
+	my_mirror_child_t *mc = zio->io_private;
+
+	mc->mc_error = zio->io_error;
+	mc->mc_tried = 1;
+	mc->mc_skipped = 0;
+}
+
+/*
+ * Check the other, lower-index DVAs to see if they're on the same
+ * vdev as the child we picked.  If they are, use them since they
+ * are likely to have been allocated from the primary metaslab in
+ * use at the time, and hence are more likely to have locality with
+ * single-copy data.
+ */
+static int
+vdev_my_mirror_dva_select(zio_t *zio, int p)
+{
+	dva_t *dva = zio->io_bp->blk_dva;
+	my_mirror_map_t *mm = zio->io_vsd;
+	int preferred;
+	int c;
+
+	preferred = mm->mm_preferred[p];
+	for (p--; p >= 0; p--) {
+		c = mm->mm_preferred[p];
+		if (DVA_GET_VDEV(&dva[c]) == DVA_GET_VDEV(&dva[preferred]))
+			preferred = c;
+	}
+	return (preferred);
+}
+
+static int
+vdev_my_mirror_preferred_child_randomize(zio_t *zio)
+{
+	my_mirror_map_t *mm = zio->io_vsd;
+	int p;
+
+	if (mm->mm_root) {
+		p = random_in_range(mm->mm_preferred_cnt);
+		return (vdev_my_mirror_dva_select(zio, p));
+	}
+
+	/*
+	 * To ensure we don't always favour the first matching vdev,
+	 * which could lead to wear leveling issues on SSD's, we
+	 * use the I/O offset as a pseudo random seed into the vdevs
+	 * which have the lowest load.
+	 */
+	p = (zio->io_offset >> vdev_my_mirror_shift) % mm->mm_preferred_cnt;
+	return (mm->mm_preferred[p]);
+}
+
+static boolean_t
+vdev_my_mirror_child_readable(my_mirror_child_t *mc)
+{
+	vdev_t *vd = mc->mc_vd;
+
+	if (vd->vdev_top != NULL && vd->vdev_top->vdev_ops == &vdev_draid_ops)
+		return (vdev_draid_readable(vd, mc->mc_offset));
+	else
+		return (vdev_readable(vd));
+}
+
+static boolean_t
+vdev_my_mirror_child_missing(my_mirror_child_t *mc, uint64_t txg, uint64_t size)
+{
+	vdev_t *vd = mc->mc_vd;
+
+	if (vd->vdev_top != NULL && vd->vdev_top->vdev_ops == &vdev_draid_ops)
+		return (vdev_draid_missing(vd, mc->mc_offset, txg, size));
+	else
+		return (vdev_dtl_contains(vd, DTL_MISSING, txg, size));
+}
+
+/*
+ * Try to find a vdev whose DTL doesn't contain the block we want to read
+ * preferring vdevs based on determined load. If we can't, try the read on
+ * any vdev we haven't already tried.
+ *
+ * Distributed spares are an exception to the above load rule. They are
+ * always preferred in order to detect gaps in the distributed spare which
+ * are created when another disk in the dRAID fails. In order to restore
+ * redundancy those gaps must be read to trigger the required repair IO.
+ */
+static int
+vdev_my_mirror_child_select(zio_t *zio)
+{
+	my_mirror_map_t *mm = zio->io_vsd;
+	uint64_t txg = zio->io_txg;
+	int c, lowest_load;
+
+	ASSERT(zio->io_bp == NULL || BP_PHYSICAL_BIRTH(zio->io_bp) == txg);
+
+	lowest_load = INT_MAX;
+	mm->mm_preferred_cnt = 0;
+	for (c = 0; c < mm->mm_children; c++) {
+		my_mirror_child_t *mc;
+
+		mc = &mm->mm_child[c];
+		if (mc->mc_tried || mc->mc_skipped)
+			continue;
+
+		if (mc->mc_vd == NULL ||
+		    !vdev_my_mirror_child_readable(mc)) {
+			mc->mc_error = SET_ERROR(ENXIO);
+			mc->mc_tried = 1;	/* don't even try */
+			mc->mc_skipped = 1;
+			continue;
+		}
+
+		if (vdev_my_mirror_child_missing(mc, txg, 1)) {
+			mc->mc_error = SET_ERROR(ESTALE);
+			mc->mc_skipped = 1;
+			mc->mc_speculative = 1;
+			continue;
+		}
+
+		if (mc->mc_vd->vdev_ops == &vdev_draid_spare_ops) {
+			mm->mm_preferred[0] = c;
+			mm->mm_preferred_cnt = 1;
+			break;
+		}
+
+		mc->mc_load = vdev_my_mirror_load(mm, mc->mc_vd, mc->mc_offset);
+		if (mc->mc_load > lowest_load)
+			continue;
+
+		if (mc->mc_load < lowest_load) {
+			lowest_load = mc->mc_load;
+			mm->mm_preferred_cnt = 0;
+		}
+		mm->mm_preferred[mm->mm_preferred_cnt] = c;
+		mm->mm_preferred_cnt++;
+	}
+
+	if (mm->mm_preferred_cnt == 1) {
+		my_mirror_BUMP(vdev_my_mirror_stat_preferred_found);
+		return (mm->mm_preferred[0]);
+	}
+
+	if (mm->mm_preferred_cnt > 1) {
+		my_mirror_BUMP(vdev_my_mirror_stat_preferred_not_found);
+		return (vdev_my_mirror_preferred_child_randomize(zio));
+	}
+
+	/*
+	 * Every device is either missing or has this txg in its DTL.
+	 * Look for any child we haven't already tried before giving up.
+	 */
+	for (c = 0; c < mm->mm_children; c++) {
+		if (!mm->mm_child[c].mc_tried)
+			return (c);
+	}
+
+	/*
+	 * Every child failed.  There's no place left to look.
+	 */
+	return (-1);
+}
+
+static void
+vdev_my_mirror_io_start(zio_t *zio)
+{
+	my_mirror_map_t *mm;
+	my_mirror_child_t *mc;
+	int c, children;
+
+	mm = vdev_my_mirror_map_init(zio);
+	zio->io_vsd = mm;
+	zio->io_vsd_ops = &vdev_my_mirror_vsd_ops;
+
+	if (mm == NULL) {
+		ASSERT(!spa_trust_config(zio->io_spa));
+		ASSERT(zio->io_type == ZIO_TYPE_READ);
+		zio_execute(zio);
+		return;
+	}
+
+	if (zio->io_type == ZIO_TYPE_READ) {
+		if ((zio->io_flags & ZIO_FLAG_SCRUB) && !mm->mm_resilvering) {
+			/*
+			 * For scrubbing reads we need to issue reads to all
+			 * children.  One child can reuse parent buffer, but
+			 * for others we have to allocate separate ones to
+			 * verify checksums if io_bp is non-NULL, or compare
+			 * them in vdev_my_mirror_io_done() otherwise.
+			 */
+			boolean_t first = B_TRUE;
+			for (c = 0; c < mm->mm_children; c++) {
+				mc = &mm->mm_child[c];
+
+				/* Don't issue ZIOs to offline children */
+				if (!vdev_my_mirror_child_readable(mc)) {
+					mc->mc_error = SET_ERROR(ENXIO);
+					mc->mc_tried = 1;
+					mc->mc_skipped = 1;
+					continue;
+				}
+
+				mc->mc_abd = first ? zio->io_abd :
+				    abd_alloc_sametype(zio->io_abd,
+				    zio->io_size);
+				zio_nowait(zio_vdev_child_io(zio, zio->io_bp,
+				    mc->mc_vd, mc->mc_offset, mc->mc_abd,
+				    zio->io_size, zio->io_type,
+				    zio->io_priority, 0,
+				    vdev_my_mirror_child_done, mc));
+				first = B_FALSE;
+			}
+			zio_execute(zio);
+			return;
+		}
+		/*
+		 * For normal reads just pick one child.
+		 */
+		c = vdev_my_mirror_child_select(zio);
+		children = (c >= 0);
+	} else {
+		ASSERT(zio->io_type == ZIO_TYPE_WRITE);
+
+		/*
+		 * Writes go to all children.
+		 */
+		c = 0;
+		children = mm->mm_children;
+	}
+
+	while (children--) {
+		mc = &mm->mm_child[c];
+		c++;
+
+		/*
+		 * When sequentially resilvering only issue write repair
+		 * IOs to the vdev which is being rebuilt since performance
+		 * is limited by the slowest child.  This is an issue for
+		 * faster replacement devices such as distributed spares.
+		 */
+		if ((zio->io_priority == ZIO_PRIORITY_REBUILD) &&
+		    (zio->io_flags & ZIO_FLAG_IO_REPAIR) &&
+		    !(zio->io_flags & ZIO_FLAG_SCRUB) &&
+		    mm->mm_rebuilding && !mc->mc_rebuilding) {
+			continue;
+		}
+
+		zio_nowait(zio_vdev_child_io(zio, zio->io_bp,
+		    mc->mc_vd, mc->mc_offset, zio->io_abd, zio->io_size,
+		    zio->io_type, zio->io_priority, 0,
+		    vdev_my_mirror_child_done, mc));
+	}
+
+	zio_execute(zio);
+}
+
+static int
+vdev_my_mirror_worst_error(my_mirror_map_t *mm)
+{
+	int error[2] = { 0, 0 };
+
+	for (int c = 0; c < mm->mm_children; c++) {
+		my_mirror_child_t *mc = &mm->mm_child[c];
+		int s = mc->mc_speculative;
+		error[s] = zio_worst_error(error[s], mc->mc_error);
+	}
+
+	return (error[0] ? error[0] : error[1]);
+}
+
+static void
+vdev_my_mirror_io_done(zio_t *zio)
+{
+	my_mirror_map_t *mm = zio->io_vsd;
+	my_mirror_child_t *mc;
+	int c;
+	int good_copies = 0;
+	int unexpected_errors = 0;
+	int last_good_copy = -1;
+
+	if (mm == NULL)
+		return;
+
+	for (c = 0; c < mm->mm_children; c++) {
+		mc = &mm->mm_child[c];
+
+		if (mc->mc_error) {
+			if (!mc->mc_skipped)
+				unexpected_errors++;
+		} else if (mc->mc_tried) {
+			last_good_copy = c;
+			good_copies++;
+		}
+	}
+
+	if (zio->io_type == ZIO_TYPE_WRITE) {
+		/*
+		 * XXX -- for now, treat partial writes as success.
+		 *
+		 * Now that we support write reallocation, it would be better
+		 * to treat partial failure as real failure unless there are
+		 * no non-degraded top-level vdevs left, and not update DTLs
+		 * if we intend to reallocate.
+		 */
+		if (good_copies != mm->mm_children) {
+			/*
+			 * Always require at least one good copy.
+			 *
+			 * For ditto blocks (io_vd == NULL), require
+			 * all copies to be good.
+			 *
+			 * XXX -- for replacing vdevs, there's no great answer.
+			 * If the old device is really dead, we may not even
+			 * be able to access it -- so we only want to
+			 * require good writes to the new device.  But if
+			 * the new device turns out to be flaky, we want
+			 * to be able to detach it -- which requires all
+			 * writes to the old device to have succeeded.
+			 */
+			if (good_copies == 0 || zio->io_vd == NULL)
+				zio->io_error = vdev_my_mirror_worst_error(mm);
+		}
+		return;
+	}
+
+	ASSERT(zio->io_type == ZIO_TYPE_READ);
+
+	/*
+	 * If we don't have a good copy yet, keep trying other children.
+	 */
+	if (good_copies == 0 && (c = vdev_my_mirror_child_select(zio)) != -1) {
+		ASSERT(c >= 0 && c < mm->mm_children);
+		mc = &mm->mm_child[c];
+		zio_vdev_io_redone(zio);
+		zio_nowait(zio_vdev_child_io(zio, zio->io_bp,
+		    mc->mc_vd, mc->mc_offset, zio->io_abd, zio->io_size,
+		    ZIO_TYPE_READ, zio->io_priority, 0,
+		    vdev_my_mirror_child_done, mc));
+		return;
+	}
+
+	if (zio->io_flags & ZIO_FLAG_SCRUB && !mm->mm_resilvering) {
+		abd_t *best_abd = NULL;
+		if (last_good_copy >= 0)
+			best_abd = mm->mm_child[last_good_copy].mc_abd;
+
+		/*
+		 * If we're scrubbing but don't have a BP available (because
+		 * this vdev is under a raidz or draid vdev) then the best we
+		 * can do is compare all of the copies read.  If they're not
+		 * identical then return a checksum error and the most likely
+		 * correct data.  The raidz code will issue a repair I/O if
+		 * possible.
+		 */
+		if (zio->io_bp == NULL) {
+			ASSERT(zio->io_vd->vdev_ops == &vdev_replacing_ops ||
+			    zio->io_vd->vdev_ops == &vdev_spare_ops);
+
+			abd_t *pref_abd = NULL;
+			for (c = 0; c < last_good_copy; c++) {
+				mc = &mm->mm_child[c];
+				if (mc->mc_error || !mc->mc_tried)
+					continue;
+
+				if (abd_cmp(mc->mc_abd, best_abd) != 0)
+					zio->io_error = SET_ERROR(ECKSUM);
+
+				/*
+				 * The distributed spare is always prefered
+				 * by vdev_my_mirror_child_select() so it's
+				 * considered to be the best candidate.
+				 */
+				if (pref_abd == NULL &&
+				    mc->mc_vd->vdev_ops ==
+				    &vdev_draid_spare_ops)
+					pref_abd = mc->mc_abd;
+
+				/*
+				 * In the absence of a preferred copy, use
+				 * the parent pointer to avoid a memory copy.
+				 */
+				if (mc->mc_abd == zio->io_abd)
+					best_abd = mc->mc_abd;
+			}
+			if (pref_abd)
+				best_abd = pref_abd;
+		} else {
+
+			/*
+			 * If we have a BP available, then checksums are
+			 * already verified and we just need a buffer
+			 * with valid data, preferring parent one to
+			 * avoid a memory copy.
+			 */
+			for (c = 0; c < last_good_copy; c++) {
+				mc = &mm->mm_child[c];
+				if (mc->mc_error || !mc->mc_tried)
+					continue;
+				if (mc->mc_abd == zio->io_abd) {
+					best_abd = mc->mc_abd;
+					break;
+				}
+			}
+		}
+
+		if (best_abd && best_abd != zio->io_abd)
+			abd_copy(zio->io_abd, best_abd, zio->io_size);
+		for (c = 0; c < mm->mm_children; c++) {
+			mc = &mm->mm_child[c];
+			if (mc->mc_abd != zio->io_abd)
+				abd_free(mc->mc_abd);
+			mc->mc_abd = NULL;
+		}
+	}
+
+	if (good_copies == 0) {
+		zio->io_error = vdev_my_mirror_worst_error(mm);
+		ASSERT(zio->io_error != 0);
+	}
+
+	if (good_copies && spa_writeable(zio->io_spa) &&
+	    (unexpected_errors ||
+	    (zio->io_flags & ZIO_FLAG_RESILVER) ||
+	    ((zio->io_flags & ZIO_FLAG_SCRUB) && mm->mm_resilvering))) {
+		/*
+		 * Use the good data we have in hand to repair damaged children.
+		 */
+		for (c = 0; c < mm->mm_children; c++) {
+			/*
+			 * Don't rewrite known good children.
+			 * Not only is it unnecessary, it could
+			 * actually be harmful: if the system lost
+			 * power while rewriting the only good copy,
+			 * there would be no good copies left!
+			 */
+			mc = &mm->mm_child[c];
+
+			if (mc->mc_error == 0) {
+				vdev_ops_t *ops = mc->mc_vd->vdev_ops;
+
+				if (mc->mc_tried)
+					continue;
+				/*
+				 * We didn't try this child.  We need to
+				 * repair it if:
+				 * 1. it's a scrub (in which case we have
+				 * tried everything that was healthy)
+				 *  - or -
+				 * 2. it's an indirect or distributed spare
+				 * vdev (in which case it could point to any
+				 * other vdev, which might have a bad DTL)
+				 *  - or -
+				 * 3. the DTL indicates that this data is
+				 * missing from this vdev
+				 */
+				if (!(zio->io_flags & ZIO_FLAG_SCRUB) &&
+				    ops != &vdev_indirect_ops &&
+				    ops != &vdev_draid_spare_ops &&
+				    !vdev_dtl_contains(mc->mc_vd, DTL_PARTIAL,
+				    zio->io_txg, 1))
+					continue;
+				mc->mc_error = SET_ERROR(ESTALE);
+			}
+
+			zio_nowait(zio_vdev_child_io(zio, zio->io_bp,
+			    mc->mc_vd, mc->mc_offset,
+			    zio->io_abd, zio->io_size, ZIO_TYPE_WRITE,
+			    zio->io_priority == ZIO_PRIORITY_REBUILD ?
+			    ZIO_PRIORITY_REBUILD : ZIO_PRIORITY_ASYNC_WRITE,
+			    ZIO_FLAG_IO_REPAIR | (unexpected_errors ?
+			    ZIO_FLAG_SELF_HEAL : 0), NULL, NULL));
+		}
+	}
+}
+
+static void
+vdev_my_mirror_state_change(vdev_t *vd, int faulted, int degraded)
+{
+	if (faulted == vd->vdev_children) {
+		if (vdev_children_are_offline(vd)) {
+			vdev_set_state(vd, B_FALSE, VDEV_STATE_OFFLINE,
+			    VDEV_AUX_CHILDREN_OFFLINE);
+		} else {
+			vdev_set_state(vd, B_FALSE, VDEV_STATE_CANT_OPEN,
+			    VDEV_AUX_NO_REPLICAS);
+		}
+	} else if (degraded + faulted != 0) {
+		vdev_set_state(vd, B_FALSE, VDEV_STATE_DEGRADED, VDEV_AUX_NONE);
+	} else {
+		vdev_set_state(vd, B_FALSE, VDEV_STATE_HEALTHY, VDEV_AUX_NONE);
+	}
+}
+
+/*
+ * Return the maximum asize for a rebuild zio in the provided range.
+ */
+static uint64_t
+vdev_my_mirror_rebuild_asize(vdev_t *vd, uint64_t start, uint64_t asize,
+    uint64_t max_segment)
+{
+	(void) start;
+
+	uint64_t psize = MIN(P2ROUNDUP(max_segment, 1 << vd->vdev_ashift),
+	    SPA_MAXBLOCKSIZE);
+
+	return (MIN(asize, vdev_psize_to_asize(vd, psize)));
+}
+
+vdev_ops_t vdev_my_mirror_ops = {
+	.vdev_op_init = NULL,
+	.vdev_op_fini = NULL,
+	.vdev_op_open = vdev_my_mirror_open,
+	.vdev_op_close = vdev_my_mirror_close,
+	.vdev_op_asize = vdev_default_asize,
+	.vdev_op_min_asize = vdev_default_min_asize,
+	.vdev_op_min_alloc = NULL,
+	.vdev_op_io_start = vdev_my_mirror_io_start,
+	.vdev_op_io_done = vdev_my_mirror_io_done,
+	.vdev_op_state_change = vdev_my_mirror_state_change,
+	.vdev_op_need_resilver = vdev_default_need_resilver,
+	.vdev_op_hold = NULL,
+	.vdev_op_rele = NULL,
+	.vdev_op_remap = NULL,
+	.vdev_op_xlate = vdev_default_xlate,
+	.vdev_op_rebuild_asize = vdev_my_mirror_rebuild_asize,
+	.vdev_op_metaslab_init = NULL,
+	.vdev_op_config_generate = NULL,
+	.vdev_op_nparity = NULL,
+	.vdev_op_ndisks = NULL,
+	.vdev_op_type = VDEV_TYPE_MY_MIRROR,	/* name of this vdev type */
+	.vdev_op_leaf = B_FALSE			/* not a leaf vdev */
+};
diff --git a/module/zfs/vdev_my_raidz.c b/module/zfs/vdev_my_raidz.c
new file mode 100644
index 000000000..b3caa0495
--- /dev/null
+++ b/module/zfs/vdev_my_raidz.c
@@ -0,0 +1,2593 @@
+/*
+ * CDDL HEADER START
+ *
+ * The contents of this file are subject to the terms of the
+ * Common Development and Distribution License (the "License").
+ * You may not use this file except in compliance with the License.
+ *
+ * You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+ * or http://www.opensolaris.org/os/licensing.
+ * See the License for the specific language governing permissions
+ * and limitations under the License.
+ *
+ * When distributing Covered Code, include this CDDL HEADER in each
+ * file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+ * If applicable, add the following below this CDDL HEADER, with the
+ * fields enclosed by brackets "[]" replaced with your own identifying
+ * information: Portions Copyright [yyyy] [name of copyright owner]
+ *
+ * CDDL HEADER END
+ */
+
+/*
+ * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2012, 2020 by Delphix. All rights reserved.
+ * Copyright (c) 2016 Gvozden Nešković. All rights reserved.
+ */
+
+#include <sys/zfs_context.h>
+#include <sys/spa.h>
+#include <sys/vdev_impl.h>
+#include <sys/zio.h>
+#include <sys/zio_checksum.h>
+#include <sys/abd.h>
+#include <sys/fs/zfs.h>
+#include <sys/fm/fs/zfs.h>
+#include <sys/vdev_raidz.h>
+#include <sys/vdev_raidz_impl.h>
+#include <sys/vdev_draid.h>
+
+#ifdef ZFS_DEBUG
+#include <sys/vdev.h>	/* For vdev_xlate() in vdev_raidz_io_verify() */
+#endif
+
+/*
+ * Virtual device vector for RAID-Z.
+ *
+ * This vdev supports single, double, and triple parity. For single parity,
+ * we use a simple XOR of all the data columns. For double or triple parity,
+ * we use a special case of Reed-Solomon coding. This extends the
+ * technique described in "The mathematics of RAID-6" by H. Peter Anvin by
+ * drawing on the system described in "A Tutorial on Reed-Solomon Coding for
+ * Fault-Tolerance in RAID-like Systems" by James S. Plank on which the
+ * former is also based. The latter is designed to provide higher performance
+ * for writes.
+ *
+ * Note that the Plank paper claimed to support arbitrary N+M, but was then
+ * amended six years later identifying a critical flaw that invalidates its
+ * claims. Nevertheless, the technique can be adapted to work for up to
+ * triple parity. For additional parity, the amendment "Note: Correction to
+ * the 1997 Tutorial on Reed-Solomon Coding" by James S. Plank and Ying Ding
+ * is viable, but the additional complexity means that write performance will
+ * suffer.
+ *
+ * All of the methods above operate on a Galois field, defined over the
+ * integers mod 2^N. In our case we choose N=8 for GF(8) so that all elements
+ * can be expressed with a single byte. Briefly, the operations on the
+ * field are defined as follows:
+ *
+ *   o addition (+) is represented by a bitwise XOR
+ *   o subtraction (-) is therefore identical to addition: A + B = A - B
+ *   o multiplication of A by 2 is defined by the following bitwise expression:
+ *
+ *	(A * 2)_7 = A_6
+ *	(A * 2)_6 = A_5
+ *	(A * 2)_5 = A_4
+ *	(A * 2)_4 = A_3 + A_7
+ *	(A * 2)_3 = A_2 + A_7
+ *	(A * 2)_2 = A_1 + A_7
+ *	(A * 2)_1 = A_0
+ *	(A * 2)_0 = A_7
+ *
+ * In C, multiplying by 2 is therefore ((a << 1) ^ ((a & 0x80) ? 0x1d : 0)).
+ * As an aside, this multiplication is derived from the error correcting
+ * primitive polynomial x^8 + x^4 + x^3 + x^2 + 1.
+ *
+ * Observe that any number in the field (except for 0) can be expressed as a
+ * power of 2 -- a generator for the field. We store a table of the powers of
+ * 2 and logs base 2 for quick look ups, and exploit the fact that A * B can
+ * be rewritten as 2^(log_2(A) + log_2(B)) (where '+' is normal addition rather
+ * than field addition). The inverse of a field element A (A^-1) is therefore
+ * A ^ (255 - 1) = A^254.
+ *
+ * The up-to-three parity columns, P, Q, R over several data columns,
+ * D_0, ... D_n-1, can be expressed by field operations:
+ *
+ *	P = D_0 + D_1 + ... + D_n-2 + D_n-1
+ *	Q = 2^n-1 * D_0 + 2^n-2 * D_1 + ... + 2^1 * D_n-2 + 2^0 * D_n-1
+ *	  = ((...((D_0) * 2 + D_1) * 2 + ...) * 2 + D_n-2) * 2 + D_n-1
+ *	R = 4^n-1 * D_0 + 4^n-2 * D_1 + ... + 4^1 * D_n-2 + 4^0 * D_n-1
+ *	  = ((...((D_0) * 4 + D_1) * 4 + ...) * 4 + D_n-2) * 4 + D_n-1
+ *
+ * We chose 1, 2, and 4 as our generators because 1 corresponds to the trivial
+ * XOR operation, and 2 and 4 can be computed quickly and generate linearly-
+ * independent coefficients. (There are no additional coefficients that have
+ * this property which is why the uncorrected Plank method breaks down.)
+ *
+ * See the reconstruction code below for how P, Q and R can used individually
+ * or in concert to recover missing data columns.
+ */
+
+#define	VDEV_RAIDZ_P		0
+#define	VDEV_RAIDZ_Q		1
+#define	VDEV_RAIDZ_R		2
+
+#define	VDEV_RAIDZ_MUL_2(x)	(((x) << 1) ^ (((x) & 0x80) ? 0x1d : 0))
+#define	VDEV_RAIDZ_MUL_4(x)	(VDEV_RAIDZ_MUL_2(VDEV_RAIDZ_MUL_2(x)))
+
+/*
+ * We provide a mechanism to perform the field multiplication operation on a
+ * 64-bit value all at once rather than a byte at a time. This works by
+ * creating a mask from the top bit in each byte and using that to
+ * conditionally apply the XOR of 0x1d.
+ */
+#define	VDEV_RAIDZ_64MUL_2(x, mask) \
+{ \
+	(mask) = (x) & 0x8080808080808080ULL; \
+	(mask) = ((mask) << 1) - ((mask) >> 7); \
+	(x) = (((x) << 1) & 0xfefefefefefefefeULL) ^ \
+	    ((mask) & 0x1d1d1d1d1d1d1d1dULL); \
+}
+
+#define	VDEV_RAIDZ_64MUL_4(x, mask) \
+{ \
+	VDEV_RAIDZ_64MUL_2((x), mask); \
+	VDEV_RAIDZ_64MUL_2((x), mask); \
+}
+
+static void
+vdev_raidz_row_free(raidz_row_t *rr)
+{
+	for (int c = 0; c < rr->rr_cols; c++) {
+		raidz_col_t *rc = &rr->rr_col[c];
+
+		if (rc->rc_size != 0)
+			abd_free(rc->rc_abd);
+		if (rc->rc_orig_data != NULL)
+			abd_free(rc->rc_orig_data);
+	}
+
+	if (rr->rr_abd_empty != NULL)
+		abd_free(rr->rr_abd_empty);
+
+	kmem_free(rr, offsetof(raidz_row_t, rr_col[rr->rr_scols]));
+}
+
+void
+vdev_my_raidz_map_free(raidz_map_t *rm)
+{
+	for (int i = 0; i < rm->rm_nrows; i++)
+		vdev_raidz_row_free(rm->rm_row[i]);
+
+	kmem_free(rm, offsetof(raidz_map_t, rm_row[rm->rm_nrows]));
+}
+
+static void
+vdev_my_raidz_map_free_vsd(zio_t *zio)
+{
+	raidz_map_t *rm = zio->io_vsd;
+
+	vdev_my_raidz_map_free(rm);
+}
+
+const zio_vsd_ops_t vdev_my_raidz_vsd_ops = {
+	.vsd_free = vdev_my_raidz_map_free_vsd,
+};
+
+/*
+ * Divides the IO evenly across all child vdevs; usually, dcols is
+ * the number of children in the target vdev.
+ *
+ * Avoid inlining the function to keep vdev_raidz_io_start(), which
+ * is this functions only caller, as small as possible on the stack.
+ */
+noinline raidz_map_t *
+vdev_my_raidz_map_alloc(zio_t *zio, uint64_t ashift, uint64_t dcols,
+    uint64_t nparity)
+{
+	raidz_row_t *rr;
+	/* The starting RAIDZ (parent) vdev sector of the block. */
+	uint64_t b = zio->io_offset >> ashift;
+	/* The zio's size in units of the vdev's minimum sector size. */
+	uint64_t s = zio->io_size >> ashift;
+	/* The first column for this stripe. */
+	uint64_t f = b % dcols;
+	/* The starting byte offset on each child vdev. */
+	uint64_t o = (b / dcols) << ashift;
+	uint64_t q, r, c, bc, col, acols, scols, coff, devidx, asize, tot;
+
+	raidz_map_t *rm =
+	    kmem_zalloc(offsetof(raidz_map_t, rm_row[1]), KM_SLEEP);
+	rm->rm_nrows = 1;
+
+	/*
+	 * "Quotient": The number of data sectors for this stripe on all but
+	 * the "big column" child vdevs that also contain "remainder" data.
+	 */
+	q = s / (dcols - nparity);
+
+	/*
+	 * "Remainder": The number of partial stripe data sectors in this I/O.
+	 * This will add a sector to some, but not all, child vdevs.
+	 */
+	r = s - q * (dcols - nparity);
+
+	/* The number of "big columns" - those which contain remainder data. */
+	bc = (r == 0 ? 0 : r + nparity);
+
+	/*
+	 * The total number of data and parity sectors associated with
+	 * this I/O.
+	 */
+	tot = s + nparity * (q + (r == 0 ? 0 : 1));
+
+	/*
+	 * acols: The columns that will be accessed.
+	 * scols: The columns that will be accessed or skipped.
+	 */
+	if (q == 0) {
+		/* Our I/O request doesn't span all child vdevs. */
+		acols = bc;
+		scols = MIN(dcols, roundup(bc, nparity + 1));
+	} else {
+		acols = dcols;
+		scols = dcols;
+	}
+
+	ASSERT3U(acols, <=, scols);
+
+	rr = kmem_alloc(offsetof(raidz_row_t, rr_col[scols]), KM_SLEEP);
+	rm->rm_row[0] = rr;
+
+	rr->rr_cols = acols;
+	rr->rr_scols = scols;
+	rr->rr_bigcols = bc;
+	rr->rr_missingdata = 0;
+	rr->rr_missingparity = 0;
+	rr->rr_firstdatacol = nparity;
+	rr->rr_abd_empty = NULL;
+	rr->rr_nempty = 0;
+#ifdef ZFS_DEBUG
+	rr->rr_offset = zio->io_offset;
+	rr->rr_size = zio->io_size;
+#endif
+
+	asize = 0;
+
+	for (c = 0; c < scols; c++) {
+		raidz_col_t *rc = &rr->rr_col[c];
+		col = f + c;
+		coff = o;
+		if (col >= dcols) {
+			col -= dcols;
+			coff += 1ULL << ashift;
+		}
+		rc->rc_devidx = col;
+		rc->rc_offset = coff;
+		rc->rc_abd = NULL;
+		rc->rc_orig_data = NULL;
+		rc->rc_error = 0;
+		rc->rc_tried = 0;
+		rc->rc_skipped = 0;
+		rc->rc_force_repair = 0;
+		rc->rc_allow_repair = 1;
+		rc->rc_need_orig_restore = B_FALSE;
+
+		if (c >= acols)
+			rc->rc_size = 0;
+		else if (c < bc)
+			rc->rc_size = (q + 1) << ashift;
+		else
+			rc->rc_size = q << ashift;
+
+		asize += rc->rc_size;
+	}
+
+	ASSERT3U(asize, ==, tot << ashift);
+	rm->rm_nskip = roundup(tot, nparity + 1) - tot;
+	rm->rm_skipstart = bc;
+
+	for (c = 0; c < rr->rr_firstdatacol; c++)
+		rr->rr_col[c].rc_abd =
+		    abd_alloc_linear(rr->rr_col[c].rc_size, B_FALSE);
+
+	for (uint64_t off = 0; c < acols; c++) {
+		raidz_col_t *rc = &rr->rr_col[c];
+		rc->rc_abd = abd_get_offset_struct(&rc->rc_abdstruct,
+		    zio->io_abd, off, rc->rc_size);
+		off += rc->rc_size;
+	}
+
+	/*
+	 * If all data stored spans all columns, there's a danger that parity
+	 * will always be on the same device and, since parity isn't read
+	 * during normal operation, that device's I/O bandwidth won't be
+	 * used effectively. We therefore switch the parity every 1MB.
+	 *
+	 * ... at least that was, ostensibly, the theory. As a practical
+	 * matter unless we juggle the parity between all devices evenly, we
+	 * won't see any benefit. Further, occasional writes that aren't a
+	 * multiple of the LCM of the number of children and the minimum
+	 * stripe width are sufficient to avoid pessimal behavior.
+	 * Unfortunately, this decision created an implicit on-disk format
+	 * requirement that we need to support for all eternity, but only
+	 * for single-parity RAID-Z.
+	 *
+	 * If we intend to skip a sector in the zeroth column for padding
+	 * we must make sure to note this swap. We will never intend to
+	 * skip the first column since at least one data and one parity
+	 * column must appear in each row.
+	 */
+	ASSERT(rr->rr_cols >= 2);
+	ASSERT(rr->rr_col[0].rc_size == rr->rr_col[1].rc_size);
+
+	if (rr->rr_firstdatacol == 1 && (zio->io_offset & (1ULL << 20))) {
+		devidx = rr->rr_col[0].rc_devidx;
+		o = rr->rr_col[0].rc_offset;
+		rr->rr_col[0].rc_devidx = rr->rr_col[1].rc_devidx;
+		rr->rr_col[0].rc_offset = rr->rr_col[1].rc_offset;
+		rr->rr_col[1].rc_devidx = devidx;
+		rr->rr_col[1].rc_offset = o;
+
+		if (rm->rm_skipstart == 0)
+			rm->rm_skipstart = 1;
+	}
+
+	/* init RAIDZ parity ops */
+	rm->rm_ops = vdev_raidz_math_get_ops();
+
+	return (rm);
+}
+
+struct pqr_struct {
+	uint64_t *p;
+	uint64_t *q;
+	uint64_t *r;
+};
+
+static int
+vdev_raidz_p_func(void *buf, size_t size, void *private)
+{
+	struct pqr_struct *pqr = private;
+	const uint64_t *src = buf;
+	int i, cnt = size / sizeof (src[0]);
+
+	ASSERT(pqr->p && !pqr->q && !pqr->r);
+
+	for (i = 0; i < cnt; i++, src++, pqr->p++)
+		*pqr->p ^= *src;
+
+	return (0);
+}
+
+static int
+vdev_raidz_pq_func(void *buf, size_t size, void *private)
+{
+	struct pqr_struct *pqr = private;
+	const uint64_t *src = buf;
+	uint64_t mask;
+	int i, cnt = size / sizeof (src[0]);
+
+	ASSERT(pqr->p && pqr->q && !pqr->r);
+
+	for (i = 0; i < cnt; i++, src++, pqr->p++, pqr->q++) {
+		*pqr->p ^= *src;
+		VDEV_RAIDZ_64MUL_2(*pqr->q, mask);
+		*pqr->q ^= *src;
+	}
+
+	return (0);
+}
+
+static int
+vdev_raidz_pqr_func(void *buf, size_t size, void *private)
+{
+	struct pqr_struct *pqr = private;
+	const uint64_t *src = buf;
+	uint64_t mask;
+	int i, cnt = size / sizeof (src[0]);
+
+	ASSERT(pqr->p && pqr->q && pqr->r);
+
+	for (i = 0; i < cnt; i++, src++, pqr->p++, pqr->q++, pqr->r++) {
+		*pqr->p ^= *src;
+		VDEV_RAIDZ_64MUL_2(*pqr->q, mask);
+		*pqr->q ^= *src;
+		VDEV_RAIDZ_64MUL_4(*pqr->r, mask);
+		*pqr->r ^= *src;
+	}
+
+	return (0);
+}
+
+static void
+vdev_raidz_generate_parity_p(raidz_row_t *rr)
+{
+	uint64_t *p = abd_to_buf(rr->rr_col[VDEV_RAIDZ_P].rc_abd);
+
+	for (int c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+		abd_t *src = rr->rr_col[c].rc_abd;
+
+		if (c == rr->rr_firstdatacol) {
+			abd_copy_to_buf(p, src, rr->rr_col[c].rc_size);
+		} else {
+			struct pqr_struct pqr = { p, NULL, NULL };
+			(void) abd_iterate_func(src, 0, rr->rr_col[c].rc_size,
+			    vdev_raidz_p_func, &pqr);
+		}
+	}
+}
+
+static void
+vdev_raidz_generate_parity_pq(raidz_row_t *rr)
+{
+	uint64_t *p = abd_to_buf(rr->rr_col[VDEV_RAIDZ_P].rc_abd);
+	uint64_t *q = abd_to_buf(rr->rr_col[VDEV_RAIDZ_Q].rc_abd);
+	uint64_t pcnt = rr->rr_col[VDEV_RAIDZ_P].rc_size / sizeof (p[0]);
+	ASSERT(rr->rr_col[VDEV_RAIDZ_P].rc_size ==
+	    rr->rr_col[VDEV_RAIDZ_Q].rc_size);
+
+	for (int c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+		abd_t *src = rr->rr_col[c].rc_abd;
+
+		uint64_t ccnt = rr->rr_col[c].rc_size / sizeof (p[0]);
+
+		if (c == rr->rr_firstdatacol) {
+			ASSERT(ccnt == pcnt || ccnt == 0);
+			abd_copy_to_buf(p, src, rr->rr_col[c].rc_size);
+			(void) memcpy(q, p, rr->rr_col[c].rc_size);
+
+			for (uint64_t i = ccnt; i < pcnt; i++) {
+				p[i] = 0;
+				q[i] = 0;
+			}
+		} else {
+			struct pqr_struct pqr = { p, q, NULL };
+
+			ASSERT(ccnt <= pcnt);
+			(void) abd_iterate_func(src, 0, rr->rr_col[c].rc_size,
+			    vdev_raidz_pq_func, &pqr);
+
+			/*
+			 * Treat short columns as though they are full of 0s.
+			 * Note that there's therefore nothing needed for P.
+			 */
+			uint64_t mask;
+			for (uint64_t i = ccnt; i < pcnt; i++) {
+				VDEV_RAIDZ_64MUL_2(q[i], mask);
+			}
+		}
+	}
+}
+
+static void
+vdev_raidz_generate_parity_pqr(raidz_row_t *rr)
+{
+	uint64_t *p = abd_to_buf(rr->rr_col[VDEV_RAIDZ_P].rc_abd);
+	uint64_t *q = abd_to_buf(rr->rr_col[VDEV_RAIDZ_Q].rc_abd);
+	uint64_t *r = abd_to_buf(rr->rr_col[VDEV_RAIDZ_R].rc_abd);
+	uint64_t pcnt = rr->rr_col[VDEV_RAIDZ_P].rc_size / sizeof (p[0]);
+	ASSERT(rr->rr_col[VDEV_RAIDZ_P].rc_size ==
+	    rr->rr_col[VDEV_RAIDZ_Q].rc_size);
+	ASSERT(rr->rr_col[VDEV_RAIDZ_P].rc_size ==
+	    rr->rr_col[VDEV_RAIDZ_R].rc_size);
+
+	for (int c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+		abd_t *src = rr->rr_col[c].rc_abd;
+
+		uint64_t ccnt = rr->rr_col[c].rc_size / sizeof (p[0]);
+
+		if (c == rr->rr_firstdatacol) {
+			ASSERT(ccnt == pcnt || ccnt == 0);
+			abd_copy_to_buf(p, src, rr->rr_col[c].rc_size);
+			(void) memcpy(q, p, rr->rr_col[c].rc_size);
+			(void) memcpy(r, p, rr->rr_col[c].rc_size);
+
+			for (uint64_t i = ccnt; i < pcnt; i++) {
+				p[i] = 0;
+				q[i] = 0;
+				r[i] = 0;
+			}
+		} else {
+			struct pqr_struct pqr = { p, q, r };
+
+			ASSERT(ccnt <= pcnt);
+			(void) abd_iterate_func(src, 0, rr->rr_col[c].rc_size,
+			    vdev_raidz_pqr_func, &pqr);
+
+			/*
+			 * Treat short columns as though they are full of 0s.
+			 * Note that there's therefore nothing needed for P.
+			 */
+			uint64_t mask;
+			for (uint64_t i = ccnt; i < pcnt; i++) {
+				VDEV_RAIDZ_64MUL_2(q[i], mask);
+				VDEV_RAIDZ_64MUL_4(r[i], mask);
+			}
+		}
+	}
+}
+
+/*
+ * Generate RAID parity in the first virtual columns according to the number of
+ * parity columns available.
+ */
+void
+vdev_my_raidz_generate_parity_row(raidz_map_t *rm, raidz_row_t *rr)
+{
+	ASSERT3U(rr->rr_cols, !=, 0);
+
+	/* Generate using the new math implementation */
+	if (vdev_raidz_math_generate(rm, rr) != RAIDZ_ORIGINAL_IMPL)
+		return;
+
+	switch (rr->rr_firstdatacol) {
+	case 1:
+		vdev_raidz_generate_parity_p(rr);
+		break;
+	case 2:
+		vdev_raidz_generate_parity_pq(rr);
+		break;
+	case 3:
+		vdev_raidz_generate_parity_pqr(rr);
+		break;
+	default:
+		cmn_err(CE_PANIC, "invalid RAID-Z configuration");
+	}
+}
+
+void
+vdev_my_raidz_generate_parity(raidz_map_t *rm)
+{
+	for (int i = 0; i < rm->rm_nrows; i++) {
+		raidz_row_t *rr = rm->rm_row[i];
+		vdev_my_raidz_generate_parity_row(rm, rr);
+	}
+}
+
+static int
+vdev_raidz_reconst_p_func(void *dbuf, void *sbuf, size_t size, void *private)
+{
+	(void) private;
+	uint64_t *dst = dbuf;
+	uint64_t *src = sbuf;
+	int cnt = size / sizeof (src[0]);
+
+	for (int i = 0; i < cnt; i++) {
+		dst[i] ^= src[i];
+	}
+
+	return (0);
+}
+
+static int
+vdev_raidz_reconst_q_pre_func(void *dbuf, void *sbuf, size_t size,
+    void *private)
+{
+	(void) private;
+	uint64_t *dst = dbuf;
+	uint64_t *src = sbuf;
+	uint64_t mask;
+	int cnt = size / sizeof (dst[0]);
+
+	for (int i = 0; i < cnt; i++, dst++, src++) {
+		VDEV_RAIDZ_64MUL_2(*dst, mask);
+		*dst ^= *src;
+	}
+
+	return (0);
+}
+
+static int
+vdev_raidz_reconst_q_pre_tail_func(void *buf, size_t size, void *private)
+{
+	(void) private;
+	uint64_t *dst = buf;
+	uint64_t mask;
+	int cnt = size / sizeof (dst[0]);
+
+	for (int i = 0; i < cnt; i++, dst++) {
+		/* same operation as vdev_raidz_reconst_q_pre_func() on dst */
+		VDEV_RAIDZ_64MUL_2(*dst, mask);
+	}
+
+	return (0);
+}
+
+struct reconst_q_struct {
+	uint64_t *q;
+	int exp;
+};
+
+static int
+vdev_raidz_reconst_q_post_func(void *buf, size_t size, void *private)
+{
+	struct reconst_q_struct *rq = private;
+	uint64_t *dst = buf;
+	int cnt = size / sizeof (dst[0]);
+
+	for (int i = 0; i < cnt; i++, dst++, rq->q++) {
+		int j;
+		uint8_t *b;
+
+		*dst ^= *rq->q;
+		for (j = 0, b = (uint8_t *)dst; j < 8; j++, b++) {
+			*b = vdev_raidz_exp2(*b, rq->exp);
+		}
+	}
+
+	return (0);
+}
+
+struct reconst_pq_struct {
+	uint8_t *p;
+	uint8_t *q;
+	uint8_t *pxy;
+	uint8_t *qxy;
+	int aexp;
+	int bexp;
+};
+
+static int
+vdev_raidz_reconst_pq_func(void *xbuf, void *ybuf, size_t size, void *private)
+{
+	struct reconst_pq_struct *rpq = private;
+	uint8_t *xd = xbuf;
+	uint8_t *yd = ybuf;
+
+	for (int i = 0; i < size;
+	    i++, rpq->p++, rpq->q++, rpq->pxy++, rpq->qxy++, xd++, yd++) {
+		*xd = vdev_raidz_exp2(*rpq->p ^ *rpq->pxy, rpq->aexp) ^
+		    vdev_raidz_exp2(*rpq->q ^ *rpq->qxy, rpq->bexp);
+		*yd = *rpq->p ^ *rpq->pxy ^ *xd;
+	}
+
+	return (0);
+}
+
+static int
+vdev_raidz_reconst_pq_tail_func(void *xbuf, size_t size, void *private)
+{
+	struct reconst_pq_struct *rpq = private;
+	uint8_t *xd = xbuf;
+
+	for (int i = 0; i < size;
+	    i++, rpq->p++, rpq->q++, rpq->pxy++, rpq->qxy++, xd++) {
+		/* same operation as vdev_raidz_reconst_pq_func() on xd */
+		*xd = vdev_raidz_exp2(*rpq->p ^ *rpq->pxy, rpq->aexp) ^
+		    vdev_raidz_exp2(*rpq->q ^ *rpq->qxy, rpq->bexp);
+	}
+
+	return (0);
+}
+
+static void
+vdev_raidz_reconstruct_p(raidz_row_t *rr, int *tgts, int ntgts)
+{
+	int x = tgts[0];
+	abd_t *dst, *src;
+
+	ASSERT3U(ntgts, ==, 1);
+	ASSERT3U(x, >=, rr->rr_firstdatacol);
+	ASSERT3U(x, <, rr->rr_cols);
+
+	ASSERT3U(rr->rr_col[x].rc_size, <=, rr->rr_col[VDEV_RAIDZ_P].rc_size);
+
+	src = rr->rr_col[VDEV_RAIDZ_P].rc_abd;
+	dst = rr->rr_col[x].rc_abd;
+
+	abd_copy_from_buf(dst, abd_to_buf(src), rr->rr_col[x].rc_size);
+
+	for (int c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+		uint64_t size = MIN(rr->rr_col[x].rc_size,
+		    rr->rr_col[c].rc_size);
+
+		src = rr->rr_col[c].rc_abd;
+
+		if (c == x)
+			continue;
+
+		(void) abd_iterate_func2(dst, src, 0, 0, size,
+		    vdev_raidz_reconst_p_func, NULL);
+	}
+}
+
+static void
+vdev_raidz_reconstruct_q(raidz_row_t *rr, int *tgts, int ntgts)
+{
+	int x = tgts[0];
+	int c, exp;
+	abd_t *dst, *src;
+
+	ASSERT(ntgts == 1);
+
+	ASSERT(rr->rr_col[x].rc_size <= rr->rr_col[VDEV_RAIDZ_Q].rc_size);
+
+	for (c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+		uint64_t size = (c == x) ? 0 : MIN(rr->rr_col[x].rc_size,
+		    rr->rr_col[c].rc_size);
+
+		src = rr->rr_col[c].rc_abd;
+		dst = rr->rr_col[x].rc_abd;
+
+		if (c == rr->rr_firstdatacol) {
+			abd_copy(dst, src, size);
+			if (rr->rr_col[x].rc_size > size) {
+				abd_zero_off(dst, size,
+				    rr->rr_col[x].rc_size - size);
+			}
+		} else {
+			ASSERT3U(size, <=, rr->rr_col[x].rc_size);
+			(void) abd_iterate_func2(dst, src, 0, 0, size,
+			    vdev_raidz_reconst_q_pre_func, NULL);
+			(void) abd_iterate_func(dst,
+			    size, rr->rr_col[x].rc_size - size,
+			    vdev_raidz_reconst_q_pre_tail_func, NULL);
+		}
+	}
+
+	src = rr->rr_col[VDEV_RAIDZ_Q].rc_abd;
+	dst = rr->rr_col[x].rc_abd;
+	exp = 255 - (rr->rr_cols - 1 - x);
+
+	struct reconst_q_struct rq = { abd_to_buf(src), exp };
+	(void) abd_iterate_func(dst, 0, rr->rr_col[x].rc_size,
+	    vdev_raidz_reconst_q_post_func, &rq);
+}
+
+static void
+vdev_raidz_reconstruct_pq(raidz_row_t *rr, int *tgts, int ntgts)
+{
+	uint8_t *p, *q, *pxy, *qxy, tmp, a, b, aexp, bexp;
+	abd_t *pdata, *qdata;
+	uint64_t xsize, ysize;
+	int x = tgts[0];
+	int y = tgts[1];
+	abd_t *xd, *yd;
+
+	ASSERT(ntgts == 2);
+	ASSERT(x < y);
+	ASSERT(x >= rr->rr_firstdatacol);
+	ASSERT(y < rr->rr_cols);
+
+	ASSERT(rr->rr_col[x].rc_size >= rr->rr_col[y].rc_size);
+
+	/*
+	 * Move the parity data aside -- we're going to compute parity as
+	 * though columns x and y were full of zeros -- Pxy and Qxy. We want to
+	 * reuse the parity generation mechanism without trashing the actual
+	 * parity so we make those columns appear to be full of zeros by
+	 * setting their lengths to zero.
+	 */
+	pdata = rr->rr_col[VDEV_RAIDZ_P].rc_abd;
+	qdata = rr->rr_col[VDEV_RAIDZ_Q].rc_abd;
+	xsize = rr->rr_col[x].rc_size;
+	ysize = rr->rr_col[y].rc_size;
+
+	rr->rr_col[VDEV_RAIDZ_P].rc_abd =
+	    abd_alloc_linear(rr->rr_col[VDEV_RAIDZ_P].rc_size, B_TRUE);
+	rr->rr_col[VDEV_RAIDZ_Q].rc_abd =
+	    abd_alloc_linear(rr->rr_col[VDEV_RAIDZ_Q].rc_size, B_TRUE);
+	rr->rr_col[x].rc_size = 0;
+	rr->rr_col[y].rc_size = 0;
+
+	vdev_raidz_generate_parity_pq(rr);
+
+	rr->rr_col[x].rc_size = xsize;
+	rr->rr_col[y].rc_size = ysize;
+
+	p = abd_to_buf(pdata);
+	q = abd_to_buf(qdata);
+	pxy = abd_to_buf(rr->rr_col[VDEV_RAIDZ_P].rc_abd);
+	qxy = abd_to_buf(rr->rr_col[VDEV_RAIDZ_Q].rc_abd);
+	xd = rr->rr_col[x].rc_abd;
+	yd = rr->rr_col[y].rc_abd;
+
+	/*
+	 * We now have:
+	 *	Pxy = P + D_x + D_y
+	 *	Qxy = Q + 2^(ndevs - 1 - x) * D_x + 2^(ndevs - 1 - y) * D_y
+	 *
+	 * We can then solve for D_x:
+	 *	D_x = A * (P + Pxy) + B * (Q + Qxy)
+	 * where
+	 *	A = 2^(x - y) * (2^(x - y) + 1)^-1
+	 *	B = 2^(ndevs - 1 - x) * (2^(x - y) + 1)^-1
+	 *
+	 * With D_x in hand, we can easily solve for D_y:
+	 *	D_y = P + Pxy + D_x
+	 */
+
+	a = vdev_raidz_pow2[255 + x - y];
+	b = vdev_raidz_pow2[255 - (rr->rr_cols - 1 - x)];
+	tmp = 255 - vdev_raidz_log2[a ^ 1];
+
+	aexp = vdev_raidz_log2[vdev_raidz_exp2(a, tmp)];
+	bexp = vdev_raidz_log2[vdev_raidz_exp2(b, tmp)];
+
+	ASSERT3U(xsize, >=, ysize);
+	struct reconst_pq_struct rpq = { p, q, pxy, qxy, aexp, bexp };
+
+	(void) abd_iterate_func2(xd, yd, 0, 0, ysize,
+	    vdev_raidz_reconst_pq_func, &rpq);
+	(void) abd_iterate_func(xd, ysize, xsize - ysize,
+	    vdev_raidz_reconst_pq_tail_func, &rpq);
+
+	abd_free(rr->rr_col[VDEV_RAIDZ_P].rc_abd);
+	abd_free(rr->rr_col[VDEV_RAIDZ_Q].rc_abd);
+
+	/*
+	 * Restore the saved parity data.
+	 */
+	rr->rr_col[VDEV_RAIDZ_P].rc_abd = pdata;
+	rr->rr_col[VDEV_RAIDZ_Q].rc_abd = qdata;
+}
+
+/* BEGIN CSTYLED */
+/*
+ * In the general case of reconstruction, we must solve the system of linear
+ * equations defined by the coefficients used to generate parity as well as
+ * the contents of the data and parity disks. This can be expressed with
+ * vectors for the original data (D) and the actual data (d) and parity (p)
+ * and a matrix composed of the identity matrix (I) and a dispersal matrix (V):
+ *
+ *            __   __                     __     __
+ *            |     |         __     __   |  p_0  |
+ *            |  V  |         |  D_0  |   | p_m-1 |
+ *            |     |    x    |   :   | = |  d_0  |
+ *            |  I  |         | D_n-1 |   |   :   |
+ *            |     |         ~~     ~~   | d_n-1 |
+ *            ~~   ~~                     ~~     ~~
+ *
+ * I is simply a square identity matrix of size n, and V is a vandermonde
+ * matrix defined by the coefficients we chose for the various parity columns
+ * (1, 2, 4). Note that these values were chosen both for simplicity, speedy
+ * computation as well as linear separability.
+ *
+ *      __               __               __     __
+ *      |   1   ..  1 1 1 |               |  p_0  |
+ *      | 2^n-1 ..  4 2 1 |   __     __   |   :   |
+ *      | 4^n-1 .. 16 4 1 |   |  D_0  |   | p_m-1 |
+ *      |   1   ..  0 0 0 |   |  D_1  |   |  d_0  |
+ *      |   0   ..  0 0 0 | x |  D_2  | = |  d_1  |
+ *      |   :       : : : |   |   :   |   |  d_2  |
+ *      |   0   ..  1 0 0 |   | D_n-1 |   |   :   |
+ *      |   0   ..  0 1 0 |   ~~     ~~   |   :   |
+ *      |   0   ..  0 0 1 |               | d_n-1 |
+ *      ~~               ~~               ~~     ~~
+ *
+ * Note that I, V, d, and p are known. To compute D, we must invert the
+ * matrix and use the known data and parity values to reconstruct the unknown
+ * data values. We begin by removing the rows in V|I and d|p that correspond
+ * to failed or missing columns; we then make V|I square (n x n) and d|p
+ * sized n by removing rows corresponding to unused parity from the bottom up
+ * to generate (V|I)' and (d|p)'. We can then generate the inverse of (V|I)'
+ * using Gauss-Jordan elimination. In the example below we use m=3 parity
+ * columns, n=8 data columns, with errors in d_1, d_2, and p_1:
+ *           __                               __
+ *           |  1   1   1   1   1   1   1   1  |
+ *           | 128  64  32  16  8   4   2   1  | <-----+-+-- missing disks
+ *           |  19 205 116  29  64  16  4   1  |      / /
+ *           |  1   0   0   0   0   0   0   0  |     / /
+ *           |  0   1   0   0   0   0   0   0  | <--' /
+ *  (V|I)  = |  0   0   1   0   0   0   0   0  | <---'
+ *           |  0   0   0   1   0   0   0   0  |
+ *           |  0   0   0   0   1   0   0   0  |
+ *           |  0   0   0   0   0   1   0   0  |
+ *           |  0   0   0   0   0   0   1   0  |
+ *           |  0   0   0   0   0   0   0   1  |
+ *           ~~                               ~~
+ *           __                               __
+ *           |  1   1   1   1   1   1   1   1  |
+ *           | 128  64  32  16  8   4   2   1  |
+ *           |  19 205 116  29  64  16  4   1  |
+ *           |  1   0   0   0   0   0   0   0  |
+ *           |  0   1   0   0   0   0   0   0  |
+ *  (V|I)' = |  0   0   1   0   0   0   0   0  |
+ *           |  0   0   0   1   0   0   0   0  |
+ *           |  0   0   0   0   1   0   0   0  |
+ *           |  0   0   0   0   0   1   0   0  |
+ *           |  0   0   0   0   0   0   1   0  |
+ *           |  0   0   0   0   0   0   0   1  |
+ *           ~~                               ~~
+ *
+ * Here we employ Gauss-Jordan elimination to find the inverse of (V|I)'. We
+ * have carefully chosen the seed values 1, 2, and 4 to ensure that this
+ * matrix is not singular.
+ * __                                                                 __
+ * |  1   1   1   1   1   1   1   1     1   0   0   0   0   0   0   0  |
+ * |  19 205 116  29  64  16  4   1     0   1   0   0   0   0   0   0  |
+ * |  1   0   0   0   0   0   0   0     0   0   1   0   0   0   0   0  |
+ * |  0   0   0   1   0   0   0   0     0   0   0   1   0   0   0   0  |
+ * |  0   0   0   0   1   0   0   0     0   0   0   0   1   0   0   0  |
+ * |  0   0   0   0   0   1   0   0     0   0   0   0   0   1   0   0  |
+ * |  0   0   0   0   0   0   1   0     0   0   0   0   0   0   1   0  |
+ * |  0   0   0   0   0   0   0   1     0   0   0   0   0   0   0   1  |
+ * ~~                                                                 ~~
+ * __                                                                 __
+ * |  1   0   0   0   0   0   0   0     0   0   1   0   0   0   0   0  |
+ * |  1   1   1   1   1   1   1   1     1   0   0   0   0   0   0   0  |
+ * |  19 205 116  29  64  16  4   1     0   1   0   0   0   0   0   0  |
+ * |  0   0   0   1   0   0   0   0     0   0   0   1   0   0   0   0  |
+ * |  0   0   0   0   1   0   0   0     0   0   0   0   1   0   0   0  |
+ * |  0   0   0   0   0   1   0   0     0   0   0   0   0   1   0   0  |
+ * |  0   0   0   0   0   0   1   0     0   0   0   0   0   0   1   0  |
+ * |  0   0   0   0   0   0   0   1     0   0   0   0   0   0   0   1  |
+ * ~~                                                                 ~~
+ * __                                                                 __
+ * |  1   0   0   0   0   0   0   0     0   0   1   0   0   0   0   0  |
+ * |  0   1   1   0   0   0   0   0     1   0   1   1   1   1   1   1  |
+ * |  0  205 116  0   0   0   0   0     0   1   19  29  64  16  4   1  |
+ * |  0   0   0   1   0   0   0   0     0   0   0   1   0   0   0   0  |
+ * |  0   0   0   0   1   0   0   0     0   0   0   0   1   0   0   0  |
+ * |  0   0   0   0   0   1   0   0     0   0   0   0   0   1   0   0  |
+ * |  0   0   0   0   0   0   1   0     0   0   0   0   0   0   1   0  |
+ * |  0   0   0   0   0   0   0   1     0   0   0   0   0   0   0   1  |
+ * ~~                                                                 ~~
+ * __                                                                 __
+ * |  1   0   0   0   0   0   0   0     0   0   1   0   0   0   0   0  |
+ * |  0   1   1   0   0   0   0   0     1   0   1   1   1   1   1   1  |
+ * |  0   0  185  0   0   0   0   0    205  1  222 208 141 221 201 204 |
+ * |  0   0   0   1   0   0   0   0     0   0   0   1   0   0   0   0  |
+ * |  0   0   0   0   1   0   0   0     0   0   0   0   1   0   0   0  |
+ * |  0   0   0   0   0   1   0   0     0   0   0   0   0   1   0   0  |
+ * |  0   0   0   0   0   0   1   0     0   0   0   0   0   0   1   0  |
+ * |  0   0   0   0   0   0   0   1     0   0   0   0   0   0   0   1  |
+ * ~~                                                                 ~~
+ * __                                                                 __
+ * |  1   0   0   0   0   0   0   0     0   0   1   0   0   0   0   0  |
+ * |  0   1   1   0   0   0   0   0     1   0   1   1   1   1   1   1  |
+ * |  0   0   1   0   0   0   0   0    166 100  4   40 158 168 216 209 |
+ * |  0   0   0   1   0   0   0   0     0   0   0   1   0   0   0   0  |
+ * |  0   0   0   0   1   0   0   0     0   0   0   0   1   0   0   0  |
+ * |  0   0   0   0   0   1   0   0     0   0   0   0   0   1   0   0  |
+ * |  0   0   0   0   0   0   1   0     0   0   0   0   0   0   1   0  |
+ * |  0   0   0   0   0   0   0   1     0   0   0   0   0   0   0   1  |
+ * ~~                                                                 ~~
+ * __                                                                 __
+ * |  1   0   0   0   0   0   0   0     0   0   1   0   0   0   0   0  |
+ * |  0   1   0   0   0   0   0   0    167 100  5   41 159 169 217 208 |
+ * |  0   0   1   0   0   0   0   0    166 100  4   40 158 168 216 209 |
+ * |  0   0   0   1   0   0   0   0     0   0   0   1   0   0   0   0  |
+ * |  0   0   0   0   1   0   0   0     0   0   0   0   1   0   0   0  |
+ * |  0   0   0   0   0   1   0   0     0   0   0   0   0   1   0   0  |
+ * |  0   0   0   0   0   0   1   0     0   0   0   0   0   0   1   0  |
+ * |  0   0   0   0   0   0   0   1     0   0   0   0   0   0   0   1  |
+ * ~~                                                                 ~~
+ *                   __                               __
+ *                   |  0   0   1   0   0   0   0   0  |
+ *                   | 167 100  5   41 159 169 217 208 |
+ *                   | 166 100  4   40 158 168 216 209 |
+ *       (V|I)'^-1 = |  0   0   0   1   0   0   0   0  |
+ *                   |  0   0   0   0   1   0   0   0  |
+ *                   |  0   0   0   0   0   1   0   0  |
+ *                   |  0   0   0   0   0   0   1   0  |
+ *                   |  0   0   0   0   0   0   0   1  |
+ *                   ~~                               ~~
+ *
+ * We can then simply compute D = (V|I)'^-1 x (d|p)' to discover the values
+ * of the missing data.
+ *
+ * As is apparent from the example above, the only non-trivial rows in the
+ * inverse matrix correspond to the data disks that we're trying to
+ * reconstruct. Indeed, those are the only rows we need as the others would
+ * only be useful for reconstructing data known or assumed to be valid. For
+ * that reason, we only build the coefficients in the rows that correspond to
+ * targeted columns.
+ */
+/* END CSTYLED */
+
+static void
+vdev_raidz_matrix_init(raidz_row_t *rr, int n, int nmap, int *map,
+    uint8_t **rows)
+{
+	int i, j;
+	int pow;
+
+	ASSERT(n == rr->rr_cols - rr->rr_firstdatacol);
+
+	/*
+	 * Fill in the missing rows of interest.
+	 */
+	for (i = 0; i < nmap; i++) {
+		ASSERT3S(0, <=, map[i]);
+		ASSERT3S(map[i], <=, 2);
+
+		pow = map[i] * n;
+		if (pow > 255)
+			pow -= 255;
+		ASSERT(pow <= 255);
+
+		for (j = 0; j < n; j++) {
+			pow -= map[i];
+			if (pow < 0)
+				pow += 255;
+			rows[i][j] = vdev_raidz_pow2[pow];
+		}
+	}
+}
+
+static void
+vdev_raidz_matrix_invert(raidz_row_t *rr, int n, int nmissing, int *missing,
+    uint8_t **rows, uint8_t **invrows, const uint8_t *used)
+{
+	int i, j, ii, jj;
+	uint8_t log;
+
+	/*
+	 * Assert that the first nmissing entries from the array of used
+	 * columns correspond to parity columns and that subsequent entries
+	 * correspond to data columns.
+	 */
+	for (i = 0; i < nmissing; i++) {
+		ASSERT3S(used[i], <, rr->rr_firstdatacol);
+	}
+	for (; i < n; i++) {
+		ASSERT3S(used[i], >=, rr->rr_firstdatacol);
+	}
+
+	/*
+	 * First initialize the storage where we'll compute the inverse rows.
+	 */
+	for (i = 0; i < nmissing; i++) {
+		for (j = 0; j < n; j++) {
+			invrows[i][j] = (i == j) ? 1 : 0;
+		}
+	}
+
+	/*
+	 * Subtract all trivial rows from the rows of consequence.
+	 */
+	for (i = 0; i < nmissing; i++) {
+		for (j = nmissing; j < n; j++) {
+			ASSERT3U(used[j], >=, rr->rr_firstdatacol);
+			jj = used[j] - rr->rr_firstdatacol;
+			ASSERT3S(jj, <, n);
+			invrows[i][j] = rows[i][jj];
+			rows[i][jj] = 0;
+		}
+	}
+
+	/*
+	 * For each of the rows of interest, we must normalize it and subtract
+	 * a multiple of it from the other rows.
+	 */
+	for (i = 0; i < nmissing; i++) {
+		for (j = 0; j < missing[i]; j++) {
+			ASSERT0(rows[i][j]);
+		}
+		ASSERT3U(rows[i][missing[i]], !=, 0);
+
+		/*
+		 * Compute the inverse of the first element and multiply each
+		 * element in the row by that value.
+		 */
+		log = 255 - vdev_raidz_log2[rows[i][missing[i]]];
+
+		for (j = 0; j < n; j++) {
+			rows[i][j] = vdev_raidz_exp2(rows[i][j], log);
+			invrows[i][j] = vdev_raidz_exp2(invrows[i][j], log);
+		}
+
+		for (ii = 0; ii < nmissing; ii++) {
+			if (i == ii)
+				continue;
+
+			ASSERT3U(rows[ii][missing[i]], !=, 0);
+
+			log = vdev_raidz_log2[rows[ii][missing[i]]];
+
+			for (j = 0; j < n; j++) {
+				rows[ii][j] ^=
+				    vdev_raidz_exp2(rows[i][j], log);
+				invrows[ii][j] ^=
+				    vdev_raidz_exp2(invrows[i][j], log);
+			}
+		}
+	}
+
+	/*
+	 * Verify that the data that is left in the rows are properly part of
+	 * an identity matrix.
+	 */
+	for (i = 0; i < nmissing; i++) {
+		for (j = 0; j < n; j++) {
+			if (j == missing[i]) {
+				ASSERT3U(rows[i][j], ==, 1);
+			} else {
+				ASSERT0(rows[i][j]);
+			}
+		}
+	}
+}
+
+static void
+vdev_raidz_matrix_reconstruct(raidz_row_t *rr, int n, int nmissing,
+    int *missing, uint8_t **invrows, const uint8_t *used)
+{
+	int i, j, x, cc, c;
+	uint8_t *src;
+	uint64_t ccount;
+	uint8_t *dst[VDEV_RAIDZ_MAXPARITY] = { NULL };
+	uint64_t dcount[VDEV_RAIDZ_MAXPARITY] = { 0 };
+	uint8_t log = 0;
+	uint8_t val;
+	int ll;
+	uint8_t *invlog[VDEV_RAIDZ_MAXPARITY];
+	uint8_t *p, *pp;
+	size_t psize;
+
+	psize = sizeof (invlog[0][0]) * n * nmissing;
+	p = kmem_alloc(psize, KM_SLEEP);
+
+	for (pp = p, i = 0; i < nmissing; i++) {
+		invlog[i] = pp;
+		pp += n;
+	}
+
+	for (i = 0; i < nmissing; i++) {
+		for (j = 0; j < n; j++) {
+			ASSERT3U(invrows[i][j], !=, 0);
+			invlog[i][j] = vdev_raidz_log2[invrows[i][j]];
+		}
+	}
+
+	for (i = 0; i < n; i++) {
+		c = used[i];
+		ASSERT3U(c, <, rr->rr_cols);
+
+		ccount = rr->rr_col[c].rc_size;
+		ASSERT(ccount >= rr->rr_col[missing[0]].rc_size || i > 0);
+		if (ccount == 0)
+			continue;
+		src = abd_to_buf(rr->rr_col[c].rc_abd);
+		for (j = 0; j < nmissing; j++) {
+			cc = missing[j] + rr->rr_firstdatacol;
+			ASSERT3U(cc, >=, rr->rr_firstdatacol);
+			ASSERT3U(cc, <, rr->rr_cols);
+			ASSERT3U(cc, !=, c);
+
+			dcount[j] = rr->rr_col[cc].rc_size;
+			if (dcount[j] != 0)
+				dst[j] = abd_to_buf(rr->rr_col[cc].rc_abd);
+		}
+
+		for (x = 0; x < ccount; x++, src++) {
+			if (*src != 0)
+				log = vdev_raidz_log2[*src];
+
+			for (cc = 0; cc < nmissing; cc++) {
+				if (x >= dcount[cc])
+					continue;
+
+				if (*src == 0) {
+					val = 0;
+				} else {
+					if ((ll = log + invlog[cc][i]) >= 255)
+						ll -= 255;
+					val = vdev_raidz_pow2[ll];
+				}
+
+				if (i == 0)
+					dst[cc][x] = val;
+				else
+					dst[cc][x] ^= val;
+			}
+		}
+	}
+
+	kmem_free(p, psize);
+}
+
+static void
+vdev_raidz_reconstruct_general(raidz_row_t *rr, int *tgts, int ntgts)
+{
+	int n, i, c, t, tt;
+	int nmissing_rows;
+	int missing_rows[VDEV_RAIDZ_MAXPARITY];
+	int parity_map[VDEV_RAIDZ_MAXPARITY];
+	uint8_t *p, *pp;
+	size_t psize;
+	uint8_t *rows[VDEV_RAIDZ_MAXPARITY];
+	uint8_t *invrows[VDEV_RAIDZ_MAXPARITY];
+	uint8_t *used;
+
+	abd_t **bufs = NULL;
+
+	/*
+	 * Matrix reconstruction can't use scatter ABDs yet, so we allocate
+	 * temporary linear ABDs if any non-linear ABDs are found.
+	 */
+	for (i = rr->rr_firstdatacol; i < rr->rr_cols; i++) {
+		if (!abd_is_linear(rr->rr_col[i].rc_abd)) {
+			bufs = kmem_alloc(rr->rr_cols * sizeof (abd_t *),
+			    KM_PUSHPAGE);
+
+			for (c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+				raidz_col_t *col = &rr->rr_col[c];
+
+				bufs[c] = col->rc_abd;
+				if (bufs[c] != NULL) {
+					col->rc_abd = abd_alloc_linear(
+					    col->rc_size, B_TRUE);
+					abd_copy(col->rc_abd, bufs[c],
+					    col->rc_size);
+				}
+			}
+
+			break;
+		}
+	}
+
+	n = rr->rr_cols - rr->rr_firstdatacol;
+
+	/*
+	 * Figure out which data columns are missing.
+	 */
+	nmissing_rows = 0;
+	for (t = 0; t < ntgts; t++) {
+		if (tgts[t] >= rr->rr_firstdatacol) {
+			missing_rows[nmissing_rows++] =
+			    tgts[t] - rr->rr_firstdatacol;
+		}
+	}
+
+	/*
+	 * Figure out which parity columns to use to help generate the missing
+	 * data columns.
+	 */
+	for (tt = 0, c = 0, i = 0; i < nmissing_rows; c++) {
+		ASSERT(tt < ntgts);
+		ASSERT(c < rr->rr_firstdatacol);
+
+		/*
+		 * Skip any targeted parity columns.
+		 */
+		if (c == tgts[tt]) {
+			tt++;
+			continue;
+		}
+
+		parity_map[i] = c;
+		i++;
+	}
+
+	psize = (sizeof (rows[0][0]) + sizeof (invrows[0][0])) *
+	    nmissing_rows * n + sizeof (used[0]) * n;
+	p = kmem_alloc(psize, KM_SLEEP);
+
+	for (pp = p, i = 0; i < nmissing_rows; i++) {
+		rows[i] = pp;
+		pp += n;
+		invrows[i] = pp;
+		pp += n;
+	}
+	used = pp;
+
+	for (i = 0; i < nmissing_rows; i++) {
+		used[i] = parity_map[i];
+	}
+
+	for (tt = 0, c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+		if (tt < nmissing_rows &&
+		    c == missing_rows[tt] + rr->rr_firstdatacol) {
+			tt++;
+			continue;
+		}
+
+		ASSERT3S(i, <, n);
+		used[i] = c;
+		i++;
+	}
+
+	/*
+	 * Initialize the interesting rows of the matrix.
+	 */
+	vdev_raidz_matrix_init(rr, n, nmissing_rows, parity_map, rows);
+
+	/*
+	 * Invert the matrix.
+	 */
+	vdev_raidz_matrix_invert(rr, n, nmissing_rows, missing_rows, rows,
+	    invrows, used);
+
+	/*
+	 * Reconstruct the missing data using the generated matrix.
+	 */
+	vdev_raidz_matrix_reconstruct(rr, n, nmissing_rows, missing_rows,
+	    invrows, used);
+
+	kmem_free(p, psize);
+
+	/*
+	 * copy back from temporary linear abds and free them
+	 */
+	if (bufs) {
+		for (c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+			raidz_col_t *col = &rr->rr_col[c];
+
+			if (bufs[c] != NULL) {
+				abd_copy(bufs[c], col->rc_abd, col->rc_size);
+				abd_free(col->rc_abd);
+			}
+			col->rc_abd = bufs[c];
+		}
+		kmem_free(bufs, rr->rr_cols * sizeof (abd_t *));
+	}
+}
+
+static void
+vdev_raidz_reconstruct_row(raidz_map_t *rm, raidz_row_t *rr,
+    const int *t, int nt)
+{
+	int tgts[VDEV_RAIDZ_MAXPARITY], *dt;
+	int ntgts;
+	int i, c, ret;
+	int nbadparity, nbaddata;
+	int parity_valid[VDEV_RAIDZ_MAXPARITY];
+
+	nbadparity = rr->rr_firstdatacol;
+	nbaddata = rr->rr_cols - nbadparity;
+	ntgts = 0;
+	for (i = 0, c = 0; c < rr->rr_cols; c++) {
+		if (c < rr->rr_firstdatacol)
+			parity_valid[c] = B_FALSE;
+
+		if (i < nt && c == t[i]) {
+			tgts[ntgts++] = c;
+			i++;
+		} else if (rr->rr_col[c].rc_error != 0) {
+			tgts[ntgts++] = c;
+		} else if (c >= rr->rr_firstdatacol) {
+			nbaddata--;
+		} else {
+			parity_valid[c] = B_TRUE;
+			nbadparity--;
+		}
+	}
+
+	ASSERT(ntgts >= nt);
+	ASSERT(nbaddata >= 0);
+	ASSERT(nbaddata + nbadparity == ntgts);
+
+	dt = &tgts[nbadparity];
+
+	/* Reconstruct using the new math implementation */
+	ret = vdev_raidz_math_reconstruct(rm, rr, parity_valid, dt, nbaddata);
+	if (ret != RAIDZ_ORIGINAL_IMPL)
+		return;
+
+	/*
+	 * See if we can use any of our optimized reconstruction routines.
+	 */
+	switch (nbaddata) {
+	case 1:
+		if (parity_valid[VDEV_RAIDZ_P]) {
+			vdev_raidz_reconstruct_p(rr, dt, 1);
+			return;
+		}
+
+		ASSERT(rr->rr_firstdatacol > 1);
+
+		if (parity_valid[VDEV_RAIDZ_Q]) {
+			vdev_raidz_reconstruct_q(rr, dt, 1);
+			return;
+		}
+
+		ASSERT(rr->rr_firstdatacol > 2);
+		break;
+
+	case 2:
+		ASSERT(rr->rr_firstdatacol > 1);
+
+		if (parity_valid[VDEV_RAIDZ_P] &&
+		    parity_valid[VDEV_RAIDZ_Q]) {
+			vdev_raidz_reconstruct_pq(rr, dt, 2);
+			return;
+		}
+
+		ASSERT(rr->rr_firstdatacol > 2);
+
+		break;
+	}
+
+	vdev_raidz_reconstruct_general(rr, tgts, ntgts);
+}
+
+static int
+vdev_my_raidz_open(vdev_t *vd, uint64_t *asize, uint64_t *max_asize,
+    uint64_t *logical_ashift, uint64_t *physical_ashift)
+{
+	vdev_raidz_t *vdrz = vd->vdev_tsd;
+	uint64_t nparity = vdrz->vd_nparity;
+	int c;
+	int lasterror = 0;
+	int numerrors = 0;
+
+	ASSERT(nparity > 0);
+
+	if (nparity > VDEV_RAIDZ_MAXPARITY ||
+	    vd->vdev_children < nparity + 1) {
+		vd->vdev_stat.vs_aux = VDEV_AUX_BAD_LABEL;
+		return (SET_ERROR(EINVAL));
+	}
+
+	vdev_open_children(vd);
+
+	for (c = 0; c < vd->vdev_children; c++) {
+		vdev_t *cvd = vd->vdev_child[c];
+
+		if (cvd->vdev_open_error != 0) {
+			lasterror = cvd->vdev_open_error;
+			numerrors++;
+			continue;
+		}
+
+		*asize = MIN(*asize - 1, cvd->vdev_asize - 1) + 1;
+		*max_asize = MIN(*max_asize - 1, cvd->vdev_max_asize - 1) + 1;
+		*logical_ashift = MAX(*logical_ashift, cvd->vdev_ashift);
+	}
+	for (c = 0; c < vd->vdev_children; c++) {
+		vdev_t *cvd = vd->vdev_child[c];
+
+		if (cvd->vdev_open_error != 0)
+			continue;
+		*physical_ashift = vdev_best_ashift(*logical_ashift,
+		    *physical_ashift, cvd->vdev_physical_ashift);
+	}
+
+	*asize *= vd->vdev_children;
+	*max_asize *= vd->vdev_children;
+
+	if (numerrors > nparity) {
+		vd->vdev_stat.vs_aux = VDEV_AUX_NO_REPLICAS;
+		return (lasterror);
+	}
+
+	return (0);
+}
+
+static void
+vdev_my_raidz_close(vdev_t *vd)
+{
+	for (int c = 0; c < vd->vdev_children; c++) {
+		if (vd->vdev_child[c] != NULL)
+			vdev_close(vd->vdev_child[c]);
+	}
+}
+
+static uint64_t
+vdev_my_raidz_asize(vdev_t *vd, uint64_t psize)
+{
+	vdev_raidz_t *vdrz = vd->vdev_tsd;
+	uint64_t asize;
+	uint64_t ashift = vd->vdev_top->vdev_ashift;
+	uint64_t cols = vdrz->vd_logical_width;
+	uint64_t nparity = vdrz->vd_nparity;
+
+	asize = ((psize - 1) >> ashift) + 1;
+	asize += nparity * ((asize + cols - nparity - 1) / (cols - nparity));
+	asize = roundup(asize, nparity + 1) << ashift;
+
+	return (asize);
+}
+
+/*
+ * The allocatable space for a raidz vdev is N * sizeof(smallest child)
+ * so each child must provide at least 1/Nth of its asize.
+ */
+static uint64_t
+vdev_my_raidz_min_asize(vdev_t *vd)
+{
+	return ((vd->vdev_min_asize + vd->vdev_children - 1) /
+	    vd->vdev_children);
+}
+
+void
+vdev_my_raidz_child_done(zio_t *zio)
+{
+	raidz_col_t *rc = zio->io_private;
+
+	rc->rc_error = zio->io_error;
+	rc->rc_tried = 1;
+	rc->rc_skipped = 0;
+}
+
+static void
+vdev_raidz_io_verify(vdev_t *vd, raidz_row_t *rr, int col)
+{
+#ifdef ZFS_DEBUG
+	vdev_t *tvd = vd->vdev_top;
+
+	range_seg64_t logical_rs, physical_rs, remain_rs;
+	logical_rs.rs_start = rr->rr_offset;
+	logical_rs.rs_end = logical_rs.rs_start +
+	    vdev_my_raidz_asize(vd, rr->rr_size);
+
+	raidz_col_t *rc = &rr->rr_col[col];
+	vdev_t *cvd = vd->vdev_child[rc->rc_devidx];
+
+	vdev_xlate(cvd, &logical_rs, &physical_rs, &remain_rs);
+	ASSERT(vdev_xlate_is_empty(&remain_rs));
+	ASSERT3U(rc->rc_offset, ==, physical_rs.rs_start);
+	ASSERT3U(rc->rc_offset, <, physical_rs.rs_end);
+	/*
+	 * It would be nice to assert that rs_end is equal
+	 * to rc_offset + rc_size but there might be an
+	 * optional I/O at the end that is not accounted in
+	 * rc_size.
+	 */
+	if (physical_rs.rs_end > rc->rc_offset + rc->rc_size) {
+		ASSERT3U(physical_rs.rs_end, ==, rc->rc_offset +
+		    rc->rc_size + (1 << tvd->vdev_ashift));
+	} else {
+		ASSERT3U(physical_rs.rs_end, ==, rc->rc_offset + rc->rc_size);
+	}
+#endif
+}
+
+static void
+vdev_raidz_io_start_write(zio_t *zio, raidz_row_t *rr, uint64_t ashift)
+{
+	vdev_t *vd = zio->io_vd;
+	raidz_map_t *rm = zio->io_vsd;
+	int c, i;
+
+	vdev_my_raidz_generate_parity_row(rm, rr);
+
+	for (int c = 0; c < rr->rr_cols; c++) {
+		raidz_col_t *rc = &rr->rr_col[c];
+		if (rc->rc_size == 0)
+			continue;
+
+		/* Verify physical to logical translation */
+		vdev_raidz_io_verify(vd, rr, c);
+
+		zio_nowait(zio_vdev_child_io(zio, NULL,
+		    vd->vdev_child[rc->rc_devidx], rc->rc_offset,
+		    rc->rc_abd, rc->rc_size, zio->io_type, zio->io_priority,
+		    0, vdev_raidz_child_done, rc));
+	}
+
+	/*
+	 * Generate optional I/Os for skip sectors to improve aggregation
+	 * contiguity.
+	 */
+	for (c = rm->rm_skipstart, i = 0; i < rm->rm_nskip; c++, i++) {
+		ASSERT(c <= rr->rr_scols);
+		if (c == rr->rr_scols)
+			c = 0;
+
+		raidz_col_t *rc = &rr->rr_col[c];
+		vdev_t *cvd = vd->vdev_child[rc->rc_devidx];
+
+		zio_nowait(zio_vdev_child_io(zio, NULL, cvd,
+		    rc->rc_offset + rc->rc_size, NULL, 1ULL << ashift,
+		    zio->io_type, zio->io_priority,
+		    ZIO_FLAG_NODATA | ZIO_FLAG_OPTIONAL, NULL, NULL));
+	}
+}
+
+static void
+vdev_raidz_io_start_read(zio_t *zio, raidz_row_t *rr)
+{
+	vdev_t *vd = zio->io_vd;
+
+	/*
+	 * Iterate over the columns in reverse order so that we hit the parity
+	 * last -- any errors along the way will force us to read the parity.
+	 */
+	for (int c = rr->rr_cols - 1; c >= 0; c--) {
+		raidz_col_t *rc = &rr->rr_col[c];
+		if (rc->rc_size == 0)
+			continue;
+		vdev_t *cvd = vd->vdev_child[rc->rc_devidx];
+		if (!vdev_readable(cvd)) {
+			if (c >= rr->rr_firstdatacol)
+				rr->rr_missingdata++;
+			else
+				rr->rr_missingparity++;
+			rc->rc_error = SET_ERROR(ENXIO);
+			rc->rc_tried = 1;	/* don't even try */
+			rc->rc_skipped = 1;
+			continue;
+		}
+		if (vdev_dtl_contains(cvd, DTL_MISSING, zio->io_txg, 1)) {
+			if (c >= rr->rr_firstdatacol)
+				rr->rr_missingdata++;
+			else
+				rr->rr_missingparity++;
+			rc->rc_error = SET_ERROR(ESTALE);
+			rc->rc_skipped = 1;
+			continue;
+		}
+		if (c >= rr->rr_firstdatacol || rr->rr_missingdata > 0 ||
+		    (zio->io_flags & (ZIO_FLAG_SCRUB | ZIO_FLAG_RESILVER))) {
+			zio_nowait(zio_vdev_child_io(zio, NULL, cvd,
+			    rc->rc_offset, rc->rc_abd, rc->rc_size,
+			    zio->io_type, zio->io_priority, 0,
+			    vdev_raidz_child_done, rc));
+		}
+	}
+}
+
+/*
+ * Start an IO operation on a RAIDZ VDev
+ *
+ * Outline:
+ * - For write operations:
+ *   1. Generate the parity data
+ *   2. Create child zio write operations to each column's vdev, for both
+ *      data and parity.
+ *   3. If the column skips any sectors for padding, create optional dummy
+ *      write zio children for those areas to improve aggregation continuity.
+ * - For read operations:
+ *   1. Create child zio read operations to each data column's vdev to read
+ *      the range of data required for zio.
+ *   2. If this is a scrub or resilver operation, or if any of the data
+ *      vdevs have had errors, then create zio read operations to the parity
+ *      columns' VDevs as well.
+ */
+static void
+vdev_my_raidz_io_start(zio_t *zio)
+{
+	zfs_dbgmsg("vdev_my_raidz_io_start() called\n");
+	vdev_t *vd = zio->io_vd;
+	vdev_t *tvd = vd->vdev_top;
+	vdev_raidz_t *vdrz = vd->vdev_tsd;
+
+	raidz_map_t *rm = vdev_my_raidz_map_alloc(zio, tvd->vdev_ashift,
+	    vdrz->vd_logical_width, vdrz->vd_nparity);
+	zio->io_vsd = rm;
+	zio->io_vsd_ops = &vdev_raidz_vsd_ops;
+
+	/*
+	 * Until raidz expansion is implemented all maps for a raidz vdev
+	 * contain a single row.
+	 */
+	ASSERT3U(rm->rm_nrows, ==, 1);
+	raidz_row_t *rr = rm->rm_row[0];
+
+	if (zio->io_type == ZIO_TYPE_WRITE) {
+		vdev_raidz_io_start_write(zio, rr, tvd->vdev_ashift);
+	} else {
+		ASSERT(zio->io_type == ZIO_TYPE_READ);
+		vdev_raidz_io_start_read(zio, rr);
+	}
+
+	zio_execute(zio);
+}
+
+/*
+ * Report a checksum error for a child of a RAID-Z device.
+ */
+void
+vdev_my_raidz_checksum_error(zio_t *zio, raidz_col_t *rc, abd_t *bad_data)
+{
+	vdev_t *vd = zio->io_vd->vdev_child[rc->rc_devidx];
+
+	if (!(zio->io_flags & ZIO_FLAG_SPECULATIVE) &&
+	    zio->io_priority != ZIO_PRIORITY_REBUILD) {
+		zio_bad_cksum_t zbc;
+		raidz_map_t *rm = zio->io_vsd;
+
+		zbc.zbc_has_cksum = 0;
+		zbc.zbc_injected = rm->rm_ecksuminjected;
+
+		(void) zfs_ereport_post_checksum(zio->io_spa, vd,
+		    &zio->io_bookmark, zio, rc->rc_offset, rc->rc_size,
+		    rc->rc_abd, bad_data, &zbc);
+		mutex_enter(&vd->vdev_stat_lock);
+		vd->vdev_stat.vs_checksum_errors++;
+		mutex_exit(&vd->vdev_stat_lock);
+	}
+}
+
+/*
+ * We keep track of whether or not there were any injected errors, so that
+ * any ereports we generate can note it.
+ */
+static int
+raidz_checksum_verify(zio_t *zio)
+{
+	zio_bad_cksum_t zbc;
+	raidz_map_t *rm = zio->io_vsd;
+
+	bzero(&zbc, sizeof (zio_bad_cksum_t));
+
+	int ret = zio_checksum_error(zio, &zbc);
+	if (ret != 0 && zbc.zbc_injected != 0)
+		rm->rm_ecksuminjected = 1;
+
+	return (ret);
+}
+
+/*
+ * Generate the parity from the data columns. If we tried and were able to
+ * read the parity without error, verify that the generated parity matches the
+ * data we read. If it doesn't, we fire off a checksum error. Return the
+ * number of such failures.
+ */
+static int
+raidz_parity_verify(zio_t *zio, raidz_row_t *rr)
+{
+	abd_t *orig[VDEV_RAIDZ_MAXPARITY];
+	int c, ret = 0;
+	raidz_map_t *rm = zio->io_vsd;
+	raidz_col_t *rc;
+
+	blkptr_t *bp = zio->io_bp;
+	enum zio_checksum checksum = (bp == NULL ? zio->io_prop.zp_checksum :
+	    (BP_IS_GANG(bp) ? ZIO_CHECKSUM_GANG_HEADER : BP_GET_CHECKSUM(bp)));
+
+	if (checksum == ZIO_CHECKSUM_NOPARITY)
+		return (ret);
+
+	for (c = 0; c < rr->rr_firstdatacol; c++) {
+		rc = &rr->rr_col[c];
+		if (!rc->rc_tried || rc->rc_error != 0)
+			continue;
+
+		orig[c] = rc->rc_abd;
+		ASSERT3U(abd_get_size(rc->rc_abd), ==, rc->rc_size);
+		rc->rc_abd = abd_alloc_linear(rc->rc_size, B_FALSE);
+	}
+
+	/*
+	 * Verify any empty sectors are zero filled to ensure the parity
+	 * is calculated correctly even if these non-data sectors are damaged.
+	 */
+	if (rr->rr_nempty && rr->rr_abd_empty != NULL)
+		ret += vdev_draid_map_verify_empty(zio, rr);
+
+	/*
+	 * Regenerates parity even for !tried||rc_error!=0 columns.  This
+	 * isn't harmful but it does have the side effect of fixing stuff
+	 * we didn't realize was necessary (i.e. even if we return 0).
+	 */
+	vdev_my_raidz_generate_parity_row(rm, rr);
+
+	for (c = 0; c < rr->rr_firstdatacol; c++) {
+		rc = &rr->rr_col[c];
+
+		if (!rc->rc_tried || rc->rc_error != 0)
+			continue;
+
+		if (abd_cmp(orig[c], rc->rc_abd) != 0) {
+			vdev_my_raidz_checksum_error(zio, rc, orig[c]);
+			rc->rc_error = SET_ERROR(ECKSUM);
+			ret++;
+		}
+		abd_free(orig[c]);
+	}
+
+	return (ret);
+}
+
+static int
+vdev_raidz_worst_error(raidz_row_t *rr)
+{
+	int error = 0;
+
+	for (int c = 0; c < rr->rr_cols; c++)
+		error = zio_worst_error(error, rr->rr_col[c].rc_error);
+
+	return (error);
+}
+
+static void
+vdev_raidz_io_done_verified(zio_t *zio, raidz_row_t *rr)
+{
+	int unexpected_errors = 0;
+	int parity_errors = 0;
+	int parity_untried = 0;
+	int data_errors = 0;
+
+	ASSERT3U(zio->io_type, ==, ZIO_TYPE_READ);
+
+	for (int c = 0; c < rr->rr_cols; c++) {
+		raidz_col_t *rc = &rr->rr_col[c];
+
+		if (rc->rc_error) {
+			if (c < rr->rr_firstdatacol)
+				parity_errors++;
+			else
+				data_errors++;
+
+			if (!rc->rc_skipped)
+				unexpected_errors++;
+		} else if (c < rr->rr_firstdatacol && !rc->rc_tried) {
+			parity_untried++;
+		}
+
+		if (rc->rc_force_repair)
+			unexpected_errors++;
+	}
+
+	/*
+	 * If we read more parity disks than were used for
+	 * reconstruction, confirm that the other parity disks produced
+	 * correct data.
+	 *
+	 * Note that we also regenerate parity when resilvering so we
+	 * can write it out to failed devices later.
+	 */
+	if (parity_errors + parity_untried <
+	    rr->rr_firstdatacol - data_errors ||
+	    (zio->io_flags & ZIO_FLAG_RESILVER)) {
+		int n = raidz_parity_verify(zio, rr);
+		unexpected_errors += n;
+	}
+
+	if (zio->io_error == 0 && spa_writeable(zio->io_spa) &&
+	    (unexpected_errors > 0 || (zio->io_flags & ZIO_FLAG_RESILVER))) {
+		/*
+		 * Use the good data we have in hand to repair damaged children.
+		 */
+		for (int c = 0; c < rr->rr_cols; c++) {
+			raidz_col_t *rc = &rr->rr_col[c];
+			vdev_t *vd = zio->io_vd;
+			vdev_t *cvd = vd->vdev_child[rc->rc_devidx];
+
+			if (!rc->rc_allow_repair) {
+				continue;
+			} else if (!rc->rc_force_repair &&
+			    (rc->rc_error == 0 || rc->rc_size == 0)) {
+				continue;
+			}
+
+			zio_nowait(zio_vdev_child_io(zio, NULL, cvd,
+			    rc->rc_offset, rc->rc_abd, rc->rc_size,
+			    ZIO_TYPE_WRITE,
+			    zio->io_priority == ZIO_PRIORITY_REBUILD ?
+			    ZIO_PRIORITY_REBUILD : ZIO_PRIORITY_ASYNC_WRITE,
+			    ZIO_FLAG_IO_REPAIR | (unexpected_errors ?
+			    ZIO_FLAG_SELF_HEAL : 0), NULL, NULL));
+		}
+	}
+}
+
+static void
+raidz_restore_orig_data(raidz_map_t *rm)
+{
+	for (int i = 0; i < rm->rm_nrows; i++) {
+		raidz_row_t *rr = rm->rm_row[i];
+		for (int c = 0; c < rr->rr_cols; c++) {
+			raidz_col_t *rc = &rr->rr_col[c];
+			if (rc->rc_need_orig_restore) {
+				abd_copy(rc->rc_abd,
+				    rc->rc_orig_data, rc->rc_size);
+				rc->rc_need_orig_restore = B_FALSE;
+			}
+		}
+	}
+}
+
+/*
+ * returns EINVAL if reconstruction of the block will not be possible
+ * returns ECKSUM if this specific reconstruction failed
+ * returns 0 on successful reconstruction
+ */
+static int
+raidz_reconstruct(zio_t *zio, int *ltgts, int ntgts, int nparity)
+{
+	raidz_map_t *rm = zio->io_vsd;
+
+	/* Reconstruct each row */
+	for (int r = 0; r < rm->rm_nrows; r++) {
+		raidz_row_t *rr = rm->rm_row[r];
+		int my_tgts[VDEV_RAIDZ_MAXPARITY]; /* value is child id */
+		int t = 0;
+		int dead = 0;
+		int dead_data = 0;
+
+		for (int c = 0; c < rr->rr_cols; c++) {
+			raidz_col_t *rc = &rr->rr_col[c];
+			ASSERT0(rc->rc_need_orig_restore);
+			if (rc->rc_error != 0) {
+				dead++;
+				if (c >= nparity)
+					dead_data++;
+				continue;
+			}
+			if (rc->rc_size == 0)
+				continue;
+			for (int lt = 0; lt < ntgts; lt++) {
+				if (rc->rc_devidx == ltgts[lt]) {
+					if (rc->rc_orig_data == NULL) {
+						rc->rc_orig_data =
+						    abd_alloc_linear(
+						    rc->rc_size, B_TRUE);
+						abd_copy(rc->rc_orig_data,
+						    rc->rc_abd, rc->rc_size);
+					}
+					rc->rc_need_orig_restore = B_TRUE;
+
+					dead++;
+					if (c >= nparity)
+						dead_data++;
+					my_tgts[t++] = c;
+					break;
+				}
+			}
+		}
+		if (dead > nparity) {
+			/* reconstruction not possible */
+			raidz_restore_orig_data(rm);
+			return (EINVAL);
+		}
+		if (dead_data > 0)
+			vdev_raidz_reconstruct_row(rm, rr, my_tgts, t);
+	}
+
+	/* Check for success */
+	if (raidz_checksum_verify(zio) == 0) {
+
+		/* Reconstruction succeeded - report errors */
+		for (int i = 0; i < rm->rm_nrows; i++) {
+			raidz_row_t *rr = rm->rm_row[i];
+
+			for (int c = 0; c < rr->rr_cols; c++) {
+				raidz_col_t *rc = &rr->rr_col[c];
+				if (rc->rc_need_orig_restore) {
+					/*
+					 * Note: if this is a parity column,
+					 * we don't really know if it's wrong.
+					 * We need to let
+					 * vdev_raidz_io_done_verified() check
+					 * it, and if we set rc_error, it will
+					 * think that it is a "known" error
+					 * that doesn't need to be checked
+					 * or corrected.
+					 */
+					if (rc->rc_error == 0 &&
+					    c >= rr->rr_firstdatacol) {
+						vdev_my_raidz_checksum_error(zio,
+						    rc, rc->rc_orig_data);
+						rc->rc_error =
+						    SET_ERROR(ECKSUM);
+					}
+					rc->rc_need_orig_restore = B_FALSE;
+				}
+			}
+
+			vdev_raidz_io_done_verified(zio, rr);
+		}
+
+		zio_checksum_verified(zio);
+
+		return (0);
+	}
+
+	/* Reconstruction failed - restore original data */
+	raidz_restore_orig_data(rm);
+	return (ECKSUM);
+}
+
+/*
+ * Iterate over all combinations of N bad vdevs and attempt a reconstruction.
+ * Note that the algorithm below is non-optimal because it doesn't take into
+ * account how reconstruction is actually performed. For example, with
+ * triple-parity RAID-Z the reconstruction procedure is the same if column 4
+ * is targeted as invalid as if columns 1 and 4 are targeted since in both
+ * cases we'd only use parity information in column 0.
+ *
+ * The order that we find the various possible combinations of failed
+ * disks is dictated by these rules:
+ * - Examine each "slot" (the "i" in tgts[i])
+ *   - Try to increment this slot (tgts[i] = tgts[i] + 1)
+ *   - if we can't increment because it runs into the next slot,
+ *     reset our slot to the minimum, and examine the next slot
+ *
+ *  For example, with a 6-wide RAIDZ3, and no known errors (so we have to choose
+ *  3 columns to reconstruct), we will generate the following sequence:
+ *
+ *  STATE        ACTION
+ *  0 1 2        special case: skip since these are all parity
+ *  0 1   3      first slot: reset to 0; middle slot: increment to 2
+ *  0   2 3      first slot: increment to 1
+ *    1 2 3      first: reset to 0; middle: reset to 1; last: increment to 4
+ *  0 1     4    first: reset to 0; middle: increment to 2
+ *  0   2   4    first: increment to 1
+ *    1 2   4    first: reset to 0; middle: increment to 3
+ *  0     3 4    first: increment to 1
+ *    1   3 4    first: increment to 2
+ *      2 3 4    first: reset to 0; middle: reset to 1; last: increment to 5
+ *  0 1       5  first: reset to 0; middle: increment to 2
+ *  0   2     5  first: increment to 1
+ *    1 2     5  first: reset to 0; middle: increment to 3
+ *  0     3   5  first: increment to 1
+ *    1   3   5  first: increment to 2
+ *      2 3   5  first: reset to 0; middle: increment to 4
+ *  0       4 5  first: increment to 1
+ *    1     4 5  first: increment to 2
+ *      2   4 5  first: increment to 3
+ *        3 4 5  done
+ *
+ * This strategy works for dRAID but is less efficient when there are a large
+ * number of child vdevs and therefore permutations to check. Furthermore,
+ * since the raidz_map_t rows likely do not overlap reconstruction would be
+ * possible as long as there are no more than nparity data errors per row.
+ * These additional permutations are not currently checked but could be as
+ * a future improvement.
+ */
+static int
+vdev_raidz_combrec(zio_t *zio)
+{
+	int nparity = vdev_get_nparity(zio->io_vd);
+	raidz_map_t *rm = zio->io_vsd;
+
+	/* Check if there's enough data to attempt reconstrution. */
+	for (int i = 0; i < rm->rm_nrows; i++) {
+		raidz_row_t *rr = rm->rm_row[i];
+		int total_errors = 0;
+
+		for (int c = 0; c < rr->rr_cols; c++) {
+			if (rr->rr_col[c].rc_error)
+				total_errors++;
+		}
+
+		if (total_errors > nparity)
+			return (vdev_raidz_worst_error(rr));
+	}
+
+	for (int num_failures = 1; num_failures <= nparity; num_failures++) {
+		int tstore[VDEV_RAIDZ_MAXPARITY + 2];
+		int *ltgts = &tstore[1]; /* value is logical child ID */
+
+		/* Determine number of logical children, n */
+		int n = zio->io_vd->vdev_children;
+
+		ASSERT3U(num_failures, <=, nparity);
+		ASSERT3U(num_failures, <=, VDEV_RAIDZ_MAXPARITY);
+
+		/* Handle corner cases in combrec logic */
+		ltgts[-1] = -1;
+		for (int i = 0; i < num_failures; i++) {
+			ltgts[i] = i;
+		}
+		ltgts[num_failures] = n;
+
+		for (;;) {
+			int err = raidz_reconstruct(zio, ltgts, num_failures,
+			    nparity);
+			if (err == EINVAL) {
+				/*
+				 * Reconstruction not possible with this #
+				 * failures; try more failures.
+				 */
+				break;
+			} else if (err == 0)
+				return (0);
+
+			/* Compute next targets to try */
+			for (int t = 0; ; t++) {
+				ASSERT3U(t, <, num_failures);
+				ltgts[t]++;
+				if (ltgts[t] == n) {
+					/* try more failures */
+					ASSERT3U(t, ==, num_failures - 1);
+					break;
+				}
+
+				ASSERT3U(ltgts[t], <, n);
+				ASSERT3U(ltgts[t], <=, ltgts[t + 1]);
+
+				/*
+				 * If that spot is available, we're done here.
+				 * Try the next combination.
+				 */
+				if (ltgts[t] != ltgts[t + 1])
+					break;
+
+				/*
+				 * Otherwise, reset this tgt to the minimum,
+				 * and move on to the next tgt.
+				 */
+				ltgts[t] = ltgts[t - 1] + 1;
+				ASSERT3U(ltgts[t], ==, t);
+			}
+
+			/* Increase the number of failures and keep trying. */
+			if (ltgts[num_failures - 1] == n)
+				break;
+		}
+	}
+
+	return (ECKSUM);
+}
+
+void
+vdev_my_raidz_reconstruct(raidz_map_t *rm, const int *t, int nt)
+{
+	for (uint64_t row = 0; row < rm->rm_nrows; row++) {
+		raidz_row_t *rr = rm->rm_row[row];
+		vdev_raidz_reconstruct_row(rm, rr, t, nt);
+	}
+}
+
+/*
+ * Complete a write IO operation on a RAIDZ VDev
+ *
+ * Outline:
+ *   1. Check for errors on the child IOs.
+ *   2. Return, setting an error code if too few child VDevs were written
+ *      to reconstruct the data later.  Note that partial writes are
+ *      considered successful if they can be reconstructed at all.
+ */
+static void
+vdev_raidz_io_done_write_impl(zio_t *zio, raidz_row_t *rr)
+{
+	int total_errors = 0;
+
+	ASSERT3U(rr->rr_missingparity, <=, rr->rr_firstdatacol);
+	ASSERT3U(rr->rr_missingdata, <=, rr->rr_cols - rr->rr_firstdatacol);
+	ASSERT3U(zio->io_type, ==, ZIO_TYPE_WRITE);
+
+	for (int c = 0; c < rr->rr_cols; c++) {
+		raidz_col_t *rc = &rr->rr_col[c];
+
+		if (rc->rc_error) {
+			ASSERT(rc->rc_error != ECKSUM);	/* child has no bp */
+
+			total_errors++;
+		}
+	}
+
+	/*
+	 * Treat partial writes as a success. If we couldn't write enough
+	 * columns to reconstruct the data, the I/O failed.  Otherwise,
+	 * good enough.
+	 *
+	 * Now that we support write reallocation, it would be better
+	 * to treat partial failure as real failure unless there are
+	 * no non-degraded top-level vdevs left, and not update DTLs
+	 * if we intend to reallocate.
+	 */
+	if (total_errors > rr->rr_firstdatacol) {
+		zio->io_error = zio_worst_error(zio->io_error,
+		    vdev_raidz_worst_error(rr));
+	}
+}
+
+static void
+vdev_raidz_io_done_reconstruct_known_missing(zio_t *zio, raidz_map_t *rm,
+    raidz_row_t *rr)
+{
+	int parity_errors = 0;
+	int parity_untried = 0;
+	int data_errors = 0;
+	int total_errors = 0;
+
+	ASSERT3U(rr->rr_missingparity, <=, rr->rr_firstdatacol);
+	ASSERT3U(rr->rr_missingdata, <=, rr->rr_cols - rr->rr_firstdatacol);
+	ASSERT3U(zio->io_type, ==, ZIO_TYPE_READ);
+
+	for (int c = 0; c < rr->rr_cols; c++) {
+		raidz_col_t *rc = &rr->rr_col[c];
+
+		/*
+		 * If scrubbing and a replacing/sparing child vdev determined
+		 * that not all of its children have an identical copy of the
+		 * data, then clear the error so the column is treated like
+		 * any other read and force a repair to correct the damage.
+		 */
+		if (rc->rc_error == ECKSUM) {
+			ASSERT(zio->io_flags & ZIO_FLAG_SCRUB);
+			vdev_my_raidz_checksum_error(zio, rc, rc->rc_abd);
+			rc->rc_force_repair = 1;
+			rc->rc_error = 0;
+		}
+
+		if (rc->rc_error) {
+			if (c < rr->rr_firstdatacol)
+				parity_errors++;
+			else
+				data_errors++;
+
+			total_errors++;
+		} else if (c < rr->rr_firstdatacol && !rc->rc_tried) {
+			parity_untried++;
+		}
+	}
+
+	/*
+	 * If there were data errors and the number of errors we saw was
+	 * correctable -- less than or equal to the number of parity disks read
+	 * -- reconstruct based on the missing data.
+	 */
+	if (data_errors != 0 &&
+	    total_errors <= rr->rr_firstdatacol - parity_untried) {
+		/*
+		 * We either attempt to read all the parity columns or
+		 * none of them. If we didn't try to read parity, we
+		 * wouldn't be here in the correctable case. There must
+		 * also have been fewer parity errors than parity
+		 * columns or, again, we wouldn't be in this code path.
+		 */
+		ASSERT(parity_untried == 0);
+		ASSERT(parity_errors < rr->rr_firstdatacol);
+
+		/*
+		 * Identify the data columns that reported an error.
+		 */
+		int n = 0;
+		int tgts[VDEV_RAIDZ_MAXPARITY];
+		for (int c = rr->rr_firstdatacol; c < rr->rr_cols; c++) {
+			raidz_col_t *rc = &rr->rr_col[c];
+			if (rc->rc_error != 0) {
+				ASSERT(n < VDEV_RAIDZ_MAXPARITY);
+				tgts[n++] = c;
+			}
+		}
+
+		ASSERT(rr->rr_firstdatacol >= n);
+
+		vdev_raidz_reconstruct_row(rm, rr, tgts, n);
+	}
+}
+
+/*
+ * Return the number of reads issued.
+ */
+static int
+vdev_raidz_read_all(zio_t *zio, raidz_row_t *rr)
+{
+	vdev_t *vd = zio->io_vd;
+	int nread = 0;
+
+	rr->rr_missingdata = 0;
+	rr->rr_missingparity = 0;
+
+	/*
+	 * If this rows contains empty sectors which are not required
+	 * for a normal read then allocate an ABD for them now so they
+	 * may be read, verified, and any needed repairs performed.
+	 */
+	if (rr->rr_nempty && rr->rr_abd_empty == NULL)
+		vdev_draid_map_alloc_empty(zio, rr);
+
+	for (int c = 0; c < rr->rr_cols; c++) {
+		raidz_col_t *rc = &rr->rr_col[c];
+		if (rc->rc_tried || rc->rc_size == 0)
+			continue;
+
+		zio_nowait(zio_vdev_child_io(zio, NULL,
+		    vd->vdev_child[rc->rc_devidx],
+		    rc->rc_offset, rc->rc_abd, rc->rc_size,
+		    zio->io_type, zio->io_priority, 0,
+		    vdev_raidz_child_done, rc));
+		nread++;
+	}
+	return (nread);
+}
+
+/*
+ * We're here because either there were too many errors to even attempt
+ * reconstruction (total_errors == rm_first_datacol), or vdev_*_combrec()
+ * failed. In either case, there is enough bad data to prevent reconstruction.
+ * Start checksum ereports for all children which haven't failed.
+ */
+static void
+vdev_raidz_io_done_unrecoverable(zio_t *zio)
+{
+	zfs_dbgmsg("vdev_raidz_io_done_unrecoverable() called\n");
+	raidz_map_t *rm = zio->io_vsd;
+
+	for (int i = 0; i < rm->rm_nrows; i++) {
+		raidz_row_t *rr = rm->rm_row[i];
+
+		for (int c = 0; c < rr->rr_cols; c++) {
+			raidz_col_t *rc = &rr->rr_col[c];
+			vdev_t *cvd = zio->io_vd->vdev_child[rc->rc_devidx];
+
+			if (rc->rc_error != 0)
+				continue;
+
+			zio_bad_cksum_t zbc;
+			zbc.zbc_has_cksum = 0;
+			zbc.zbc_injected = rm->rm_ecksuminjected;
+
+			(void) zfs_ereport_start_checksum(zio->io_spa,
+			    cvd, &zio->io_bookmark, zio, rc->rc_offset,
+			    rc->rc_size, &zbc);
+			mutex_enter(&cvd->vdev_stat_lock);
+			cvd->vdev_stat.vs_checksum_errors++;
+			mutex_exit(&cvd->vdev_stat_lock);
+		}
+	}
+}
+
+void
+vdev_my_raidz_io_done(zio_t *zio)
+{
+	zfs_dbgmsg("vdev_my_raidz_io_done\n");
+	raidz_map_t *rm = zio->io_vsd;
+
+	if (zio->io_type == ZIO_TYPE_WRITE) {
+		for (int i = 0; i < rm->rm_nrows; i++) {
+			vdev_raidz_io_done_write_impl(zio, rm->rm_row[i]);
+		}
+	} else {
+		
+		for (int i = 0; i < rm->rm_nrows; i++) {
+			raidz_row_t *rr = rm->rm_row[i];
+			vdev_raidz_io_done_reconstruct_known_missing(zio,
+			    rm, rr);
+		}
+
+		if (raidz_checksum_verify(zio) == 0) {
+			zfs_dbgmsg("raidz_checksum_verify return good\n");
+			for (int i = 0; i < rm->rm_nrows; i++) {
+				raidz_row_t *rr = rm->rm_row[i];
+				vdev_raidz_io_done_verified(zio, rr);
+			}
+			zio_checksum_verified(zio);
+		} else {
+			zfs_dbgmsg("raidz_checksum_verify return bad\n");
+			/*
+			 * A sequential resilver has no checksum which makes
+			 * combinatoral reconstruction impossible. This code
+			 * path is unreachable since raidz_checksum_verify()
+			 * has no checksum to verify and must succeed.
+			 */
+			ASSERT3U(zio->io_priority, !=, ZIO_PRIORITY_REBUILD);
+
+			/*
+			 * This isn't a typical situation -- either we got a
+			 * read error or a child silently returned bad data.
+			 * Read every block so we can try again with as much
+			 * data and parity as we can track down. If we've
+			 * already been through once before, all children will
+			 * be marked as tried so we'll proceed to combinatorial
+			 * reconstruction.
+			 */
+			int nread = 0;
+			for (int i = 0; i < rm->rm_nrows; i++) {
+				nread += vdev_raidz_read_all(zio,
+				    rm->rm_row[i]);
+			}
+			zfs_dbgmsg("nread = %d\n", nread);
+			if (nread != 0) {
+				/*
+				 * Normally our stage is VDEV_IO_DONE, but if
+				 * we've already called redone(), it will have
+				 * changed to VDEV_IO_START, in which case we
+				 * don't want to call redone() again.
+				 */
+				if (zio->io_stage != ZIO_STAGE_VDEV_IO_START)
+					zio_vdev_io_redone(zio);
+				return;
+			}
+
+			zio->io_error = vdev_raidz_combrec(zio);
+
+			zfs_dbgmsg("vdev_raidz_combrec return %d\n", zio->io_error);
+			if (zio->io_error == ECKSUM &&
+			    !(zio->io_flags & ZIO_FLAG_SPECULATIVE)) {
+				zfs_dbgmsg("Calling vdev_raidz_io_done_unrecoverable\n");
+				vdev_raidz_io_done_unrecoverable(zio);
+			}
+		}
+	}
+}
+
+static void
+vdev_my_raidz_state_change(vdev_t *vd, int faulted, int degraded)
+{
+	vdev_raidz_t *vdrz = vd->vdev_tsd;
+	if (faulted > vdrz->vd_nparity)
+		vdev_set_state(vd, B_FALSE, VDEV_STATE_CANT_OPEN,
+		    VDEV_AUX_NO_REPLICAS);
+	else if (degraded + faulted != 0)
+		vdev_set_state(vd, B_FALSE, VDEV_STATE_DEGRADED, VDEV_AUX_NONE);
+	else
+		vdev_set_state(vd, B_FALSE, VDEV_STATE_HEALTHY, VDEV_AUX_NONE);
+}
+
+/*
+ * Determine if any portion of the provided block resides on a child vdev
+ * with a dirty DTL and therefore needs to be resilvered.  The function
+ * assumes that at least one DTL is dirty which implies that full stripe
+ * width blocks must be resilvered.
+ */
+static boolean_t
+vdev_my_raidz_need_resilver(vdev_t *vd, const dva_t *dva, size_t psize,
+    uint64_t phys_birth)
+{
+	vdev_raidz_t *vdrz = vd->vdev_tsd;
+	uint64_t dcols = vd->vdev_children;
+	uint64_t nparity = vdrz->vd_nparity;
+	uint64_t ashift = vd->vdev_top->vdev_ashift;
+	/* The starting RAIDZ (parent) vdev sector of the block. */
+	uint64_t b = DVA_GET_OFFSET(dva) >> ashift;
+	/* The zio's size in units of the vdev's minimum sector size. */
+	uint64_t s = ((psize - 1) >> ashift) + 1;
+	/* The first column for this stripe. */
+	uint64_t f = b % dcols;
+
+	/* Unreachable by sequential resilver. */
+	ASSERT3U(phys_birth, !=, TXG_UNKNOWN);
+
+	if (!vdev_dtl_contains(vd, DTL_PARTIAL, phys_birth, 1))
+		return (B_FALSE);
+
+	if (s + nparity >= dcols)
+		return (B_TRUE);
+
+	for (uint64_t c = 0; c < s + nparity; c++) {
+		uint64_t devidx = (f + c) % dcols;
+		vdev_t *cvd = vd->vdev_child[devidx];
+
+		/*
+		 * dsl_scan_need_resilver() already checked vd with
+		 * vdev_dtl_contains(). So here just check cvd with
+		 * vdev_dtl_empty(), cheaper and a good approximation.
+		 */
+		if (!vdev_dtl_empty(cvd, DTL_PARTIAL))
+			return (B_TRUE);
+	}
+
+	return (B_FALSE);
+}
+
+static void
+vdev_my_raidz_xlate(vdev_t *cvd, const range_seg64_t *logical_rs,
+    range_seg64_t *physical_rs, range_seg64_t *remain_rs)
+{
+	(void) remain_rs;
+
+	vdev_t *raidvd = cvd->vdev_parent;
+	ASSERT(raidvd->vdev_ops == &vdev_raidz_ops);
+
+	uint64_t width = raidvd->vdev_children;
+	uint64_t tgt_col = cvd->vdev_id;
+	uint64_t ashift = raidvd->vdev_top->vdev_ashift;
+
+	/* make sure the offsets are block-aligned */
+	ASSERT0(logical_rs->rs_start % (1 << ashift));
+	ASSERT0(logical_rs->rs_end % (1 << ashift));
+	uint64_t b_start = logical_rs->rs_start >> ashift;
+	uint64_t b_end = logical_rs->rs_end >> ashift;
+
+	uint64_t start_row = 0;
+	if (b_start > tgt_col) /* avoid underflow */
+		start_row = ((b_start - tgt_col - 1) / width) + 1;
+
+	uint64_t end_row = 0;
+	if (b_end > tgt_col)
+		end_row = ((b_end - tgt_col - 1) / width) + 1;
+
+	physical_rs->rs_start = start_row << ashift;
+	physical_rs->rs_end = end_row << ashift;
+
+	ASSERT3U(physical_rs->rs_start, <=, logical_rs->rs_start);
+	ASSERT3U(physical_rs->rs_end - physical_rs->rs_start, <=,
+	    logical_rs->rs_end - logical_rs->rs_start);
+}
+
+/*
+ * Initialize private RAIDZ specific fields from the nvlist.
+ */
+static int
+vdev_my_raidz_init(spa_t *spa, nvlist_t *nv, void **tsd)
+{
+	vdev_raidz_t *vdrz;
+	uint64_t nparity;
+
+	uint_t children;
+	nvlist_t **child;
+	int error = nvlist_lookup_nvlist_array(nv,
+	    ZPOOL_CONFIG_CHILDREN, &child, &children);
+	if (error != 0)
+		return (SET_ERROR(EINVAL));
+
+	zfs_dbgmsg("%d", error);
+	if (nvlist_lookup_uint64(nv, ZPOOL_CONFIG_NPARITY, &nparity) == 0) {
+		zfs_dbgmsg("parity level, %llu", (u_longlong_t)nparity);
+		if (nparity == 0 || nparity > VDEV_RAIDZ_MAXPARITY)
+			return (SET_ERROR(EINVAL));
+
+		/*
+		 * Previous versions could only support 1 or 2 parity
+		 * device.
+		 */
+		if (nparity > 1 && spa_version(spa) < SPA_VERSION_RAIDZ2)
+			return (SET_ERROR(EINVAL));
+		else if (nparity > 2 && spa_version(spa) < SPA_VERSION_RAIDZ3)
+			return (SET_ERROR(EINVAL));
+	} else {
+		/*
+		 * We require the parity to be specified for SPAs that
+		 * support multiple parity levels.
+		 */
+		zfs_dbgmsg("nnnnnn");
+		if (spa_version(spa) >= SPA_VERSION_RAIDZ2)
+			return (SET_ERROR(EINVAL));
+
+		/*
+		 * Otherwise, we default to 1 parity device for RAID-Z.
+		 */
+		nparity = 1;
+	}
+
+	zfs_dbgmsg("op successful");
+	vdrz = kmem_zalloc(sizeof (*vdrz), KM_SLEEP);
+	vdrz->vd_logical_width = children;
+	vdrz->vd_nparity = nparity;
+
+	*tsd = vdrz;
+
+	return (0);
+}
+
+static void
+vdev_my_raidz_fini(vdev_t *vd)
+{
+	kmem_free(vd->vdev_tsd, sizeof (vdev_raidz_t));
+}
+
+/*
+ * Add RAIDZ specific fields to the config nvlist.
+ */
+static void
+vdev_my_raidz_config_generate(vdev_t *vd, nvlist_t *nv)
+{
+	ASSERT3P(vd->vdev_ops, ==, &vdev_raidz_ops);
+	vdev_raidz_t *vdrz = vd->vdev_tsd;
+
+	/*
+	 * Make sure someone hasn't managed to sneak a fancy new vdev
+	 * into a crufty old storage pool.
+	 */
+	ASSERT(vdrz->vd_nparity == 1 ||
+	    (vdrz->vd_nparity <= 2 &&
+	    spa_version(vd->vdev_spa) >= SPA_VERSION_RAIDZ2) ||
+	    (vdrz->vd_nparity <= 3 &&
+	    spa_version(vd->vdev_spa) >= SPA_VERSION_RAIDZ3));
+
+	/*
+	 * Note that we'll add these even on storage pools where they
+	 * aren't strictly required -- older software will just ignore
+	 * it.
+	 */
+	fnvlist_add_uint64(nv, ZPOOL_CONFIG_NPARITY, vdrz->vd_nparity);
+}
+
+static uint64_t
+vdev_my_raidz_nparity(vdev_t *vd)
+{
+	vdev_raidz_t *vdrz = vd->vdev_tsd;
+	return (vdrz->vd_nparity);
+}
+
+static uint64_t
+vdev_my_raidz_ndisks(vdev_t *vd)
+{
+	return (vd->vdev_children);
+}
+
+vdev_ops_t vdev_my_raidz_ops = {
+	.vdev_op_init = vdev_my_raidz_init,
+	.vdev_op_fini = vdev_my_raidz_fini,
+	.vdev_op_open = vdev_my_raidz_open,
+	.vdev_op_close = vdev_my_raidz_close,
+	.vdev_op_asize = vdev_my_raidz_asize,
+	.vdev_op_min_asize = vdev_my_raidz_min_asize,
+	.vdev_op_min_alloc = NULL,
+	.vdev_op_io_start = vdev_my_raidz_io_start,
+	.vdev_op_io_done = vdev_my_raidz_io_done,
+	.vdev_op_state_change = vdev_my_raidz_state_change,
+	.vdev_op_need_resilver = vdev_my_raidz_need_resilver,
+	.vdev_op_hold = NULL,
+	.vdev_op_rele = NULL,
+	.vdev_op_remap = NULL,
+	.vdev_op_xlate = vdev_my_raidz_xlate,
+	.vdev_op_rebuild_asize = NULL,
+	.vdev_op_metaslab_init = NULL,
+	.vdev_op_config_generate = vdev_my_raidz_config_generate,
+	.vdev_op_nparity = vdev_my_raidz_nparity,
+	.vdev_op_ndisks = vdev_my_raidz_ndisks,
+	.vdev_op_type = VDEV_TYPE_MY_RAIDZ,	/* name of this vdev type */
+	.vdev_op_leaf = B_FALSE			/* not a leaf vdev */
+};
diff --git a/module/zfs/vdev_queue.c b/module/zfs/vdev_queue.c
index cc5b15b8c..3120c385a 100644
--- a/module/zfs/vdev_queue.c
+++ b/module/zfs/vdev_queue.c
@@ -258,10 +258,11 @@ vdev_queue_class_tree(vdev_queue_t *vq, zio_priority_t p)
 static inline avl_tree_t *
 vdev_queue_type_tree(vdev_queue_t *vq, zio_type_t t)
 {
-	ASSERT(t == ZIO_TYPE_READ || t == ZIO_TYPE_WRITE || t == ZIO_TYPE_TRIM);
+	ASSERT(t == ZIO_TYPE_READ || t == ZIO_TYPE_WRITE || t == ZIO_TYPE_TRIM || t == ZIO_TYPE_MLEC_WRITE_DATA);
 	if (t == ZIO_TYPE_READ)
 		return (&vq->vq_read_offset_tree);
-	else if (t == ZIO_TYPE_WRITE)
+	// TODO: make a mlec specific offset
+	else if (t == ZIO_TYPE_WRITE || t == ZIO_TYPE_MLEC_WRITE_DATA)
 		return (&vq->vq_write_offset_tree);
 	else
 		return (&vq->vq_trim_offset_tree);
@@ -875,6 +876,7 @@ vdev_queue_io(zio_t *zio)
 	if (zio->io_flags & ZIO_FLAG_DONT_QUEUE)
 		return (zio);
 
+	zfs_dbgmsg("Going to queue in vdev_queue_io");
 	/*
 	 * Children i/os inherent their parent's priority, which might
 	 * not match the child's i/o type.  Fix it up here.
@@ -890,7 +892,7 @@ vdev_queue_io(zio_t *zio)
 		    zio->io_priority != ZIO_PRIORITY_REBUILD) {
 			zio->io_priority = ZIO_PRIORITY_ASYNC_READ;
 		}
-	} else if (zio->io_type == ZIO_TYPE_WRITE) {
+	} else if (zio->io_type == ZIO_TYPE_WRITE || zio->io_type == ZIO_TYPE_MLEC_WRITE_DATA) {
 		ASSERT(zio->io_priority != ZIO_PRIORITY_TRIM);
 
 		if (zio->io_priority != ZIO_PRIORITY_SYNC_WRITE &&
@@ -908,6 +910,7 @@ vdev_queue_io(zio_t *zio)
 	zio->io_flags |= ZIO_FLAG_DONT_CACHE | ZIO_FLAG_DONT_QUEUE;
 	zio->io_timestamp = gethrtime();
 
+	zfs_dbgmsg("Adding to queue for io type %d", zio->io_type);
 	mutex_enter(&vq->vq_lock);
 	vdev_queue_io_add(vq, zio);
 	nio = vdev_queue_io_to_issue(vq);
diff --git a/module/zfs/vdev_raidz.c b/module/zfs/vdev_raidz.c
index 5c25007f1..d6b31724c 100644
--- a/module/zfs/vdev_raidz.c
+++ b/module/zfs/vdev_raidz.c
@@ -187,18 +187,25 @@ vdev_raidz_map_alloc(zio_t *zio, uint64_t ashift, uint64_t dcols,
 {
 	raidz_row_t *rr;
 	/* The starting RAIDZ (parent) vdev sector of the block. */
+	// b is in the unit of sector, the parent vdev sector, which sector in parent should we start reading
 	uint64_t b = zio->io_offset >> ashift;
 	/* The zio's size in units of the vdev's minimum sector size. */
 	uint64_t s = zio->io_size >> ashift;
 	/* The first column for this stripe. */
 	uint64_t f = b % dcols;
 	/* The starting byte offset on each child vdev. */
+	// Question? why is this / dcols, not (dcols - 1)? 
 	uint64_t o = (b / dcols) << ashift;
 	uint64_t q, r, c, bc, col, acols, scols, coff, devidx, asize, tot;
 
+	zfs_dbgmsg("zio size %llu", (u_longlong_t)zio->io_size);
+	zfs_dbgmsg("dcols, nparity: %llu, %llu", (u_longlong_t)dcols, (u_longlong_t)nparity);
+	zfs_dbgmsg("b, s, f, o: %llu, %llu, %llu, %llu", (u_longlong_t)b, (u_longlong_t)s, (u_longlong_t)f, (u_longlong_t)o);
+	zfs_dbgmsg("allocating row");
 	raidz_map_t *rm =
 	    kmem_zalloc(offsetof(raidz_map_t, rm_row[1]), KM_SLEEP);
 	rm->rm_nrows = 1;
+	zfs_dbgmsg("allocated row");
 
 	/*
 	 * "Quotient": The number of data sectors for this stripe on all but
@@ -236,8 +243,10 @@ vdev_raidz_map_alloc(zio_t *zio, uint64_t ashift, uint64_t dcols,
 
 	ASSERT3U(acols, <=, scols);
 
+	zfs_dbgmsg("allocating columns");
 	rr = kmem_alloc(offsetof(raidz_row_t, rr_col[scols]), KM_SLEEP);
 	rm->rm_row[0] = rr;
+	zfs_dbgmsg("allocated columns");
 
 	rr->rr_cols = acols;
 	rr->rr_scols = scols;
@@ -254,6 +263,7 @@ vdev_raidz_map_alloc(zio_t *zio, uint64_t ashift, uint64_t dcols,
 
 	asize = 0;
 
+	zfs_dbgmsg("Total number of columns %llu", (u_longlong_t)scols);
 	for (c = 0; c < scols; c++) {
 		raidz_col_t *rc = &rr->rr_col[c];
 		col = f + c;
@@ -280,16 +290,21 @@ vdev_raidz_map_alloc(zio_t *zio, uint64_t ashift, uint64_t dcols,
 		else
 			rc->rc_size = q << ashift;
 
+		zfs_dbgmsg("column %llu has rc_size %llu", (u_longlong_t)c, (u_longlong_t)rc->rc_size);
 		asize += rc->rc_size;
 	}
 
+	zfs_dbgmsg("popualted columns");
+
 	ASSERT3U(asize, ==, tot << ashift);
 	rm->rm_nskip = roundup(tot, nparity + 1) - tot;
 	rm->rm_skipstart = bc;
 
-	for (c = 0; c < rr->rr_firstdatacol; c++)
+	for (c = 0; c < rr->rr_firstdatacol; c++) {
+		zfs_dbgmsg("Allocating abd of size %llu", (u_longlong_t)rr->rr_col[c].rc_size);
 		rr->rr_col[c].rc_abd =
 		    abd_alloc_linear(rr->rr_col[c].rc_size, B_FALSE);
+	}
 
 	for (uint64_t off = 0; c < acols; c++) {
 		raidz_col_t *rc = &rr->rr_col[c];
@@ -298,6 +313,8 @@ vdev_raidz_map_alloc(zio_t *zio, uint64_t ashift, uint64_t dcols,
 		off += rc->rc_size;
 	}
 
+	zfs_dbgmsg("allocated adb for columns");
+
 	/*
 	 * If all data stored spans all columns, there's a danger that parity
 	 * will always be on the same device and, since parity isn't read
@@ -1526,9 +1543,24 @@ vdev_raidz_io_verify(vdev_t *vd, raidz_row_t *rr, int col)
 #endif
 }
 
+static void vdev_raidz_mlec_write(zio_t * zio, raidz_row_t *rr, uint64_t ashift) {
+	uint64_t col_idx = zio->mlec_write_col_idx;
+	vdev_t *vd = zio->io_vd;
+	// raidz_map_t *rm = zio->io_vsd;
+	raidz_col_t *rc = &rr->rr_col[col_idx];
+
+	zfs_dbgmsg("vdev_raidz_mlec_write called with col_idx %llu", (u_longlong_t)col_idx);
+	zfs_dbgmsg("Calling child %llu with offset %llu, and rc size %llu", (u_longlong_t)col_idx, (u_longlong_t)rc->rc_offset, (u_longlong_t)rc->rc_size);
+
+	// Test print the abd content
+	zfs_dbgmsg("The abd content is %s with size %llu", (char *) abd_to_buf(zio->io_abd), (u_longlong_t)zio->io_size);
+	zio_nowait(zio_vdev_child_io(zio, zio->mlec_write_target, vd->vdev_child[col_idx], rc->rc_offset, rc->rc_abd, rc->rc_size, zio->io_type, zio->io_priority, zio->io_flags, NULL, rc));
+}
+
 static void
 vdev_raidz_io_start_write(zio_t *zio, raidz_row_t *rr, uint64_t ashift)
 {
+	zfs_dbgmsg("vdev_raidz_io_start_write() called");
 	vdev_t *vd = zio->io_vd;
 	raidz_map_t *rm = zio->io_vsd;
 	int c, i;
@@ -1571,6 +1603,7 @@ vdev_raidz_io_start_write(zio_t *zio, raidz_row_t *rr, uint64_t ashift)
 static void
 vdev_raidz_io_start_read(zio_t *zio, raidz_row_t *rr)
 {
+	zfs_dbgmsg("vdev_raidz_io_start_read()");
 	vdev_t *vd = zio->io_vd;
 
 	/*
@@ -1582,6 +1615,7 @@ vdev_raidz_io_start_read(zio_t *zio, raidz_row_t *rr)
 		if (rc->rc_size == 0)
 			continue;
 		vdev_t *cvd = vd->vdev_child[rc->rc_devidx];
+
 		if (!vdev_readable(cvd)) {
 			if (c >= rr->rr_firstdatacol)
 				rr->rr_missingdata++;
@@ -1592,6 +1626,7 @@ vdev_raidz_io_start_read(zio_t *zio, raidz_row_t *rr)
 			rc->rc_skipped = 1;
 			continue;
 		}
+
 		if (vdev_dtl_contains(cvd, DTL_MISSING, zio->io_txg, 1)) {
 			if (c >= rr->rr_firstdatacol)
 				rr->rr_missingdata++;
@@ -1601,6 +1636,8 @@ vdev_raidz_io_start_read(zio_t *zio, raidz_row_t *rr)
 			rc->rc_skipped = 1;
 			continue;
 		}
+
+		zfs_dbgmsg("Reading column %d, devidx %llu", c, (u_longlong_t) rc->rc_devidx);
 		if (c >= rr->rr_firstdatacol || rr->rr_missingdata > 0 ||
 		    (zio->io_flags & (ZIO_FLAG_SCRUB | ZIO_FLAG_RESILVER))) {
 			zio_nowait(zio_vdev_child_io(zio, NULL, cvd,
@@ -1634,11 +1671,15 @@ vdev_raidz_io_start(zio_t *zio)
 	vdev_t *vd = zio->io_vd;
 	vdev_t *tvd = vd->vdev_top;
 	vdev_raidz_t *vdrz = vd->vdev_tsd;
+	zfs_dbgmsg("vdev_raidz_io_start, logical width %d, n parity %d", vdrz->vd_logical_width, vdrz->vd_nparity);
+
 
+	zfs_dbgmsg("allocating raidz map");
 	raidz_map_t *rm = vdev_raidz_map_alloc(zio, tvd->vdev_ashift,
 	    vdrz->vd_logical_width, vdrz->vd_nparity);
 	zio->io_vsd = rm;
 	zio->io_vsd_ops = &vdev_raidz_vsd_ops;
+	zfs_dbgmsg("raidz map allocated");
 
 	/*
 	 * Until raidz expansion is implemented all maps for a raidz vdev
@@ -1647,10 +1688,16 @@ vdev_raidz_io_start(zio_t *zio)
 	ASSERT3U(rm->rm_nrows, ==, 1);
 	raidz_row_t *rr = rm->rm_row[0];
 
-	if (zio->io_type == ZIO_TYPE_WRITE) {
+	zfs_dbgmsg("io_type raw %d", zio->io_type);
+	if (zio->io_type == ZIO_TYPE_MLEC_WRITE_DATA) {
+		zfs_dbgmsg("MLEC data write");
+		vdev_raidz_mlec_write(zio, rr, tvd->vdev_ashift);
+	} else if (zio->io_type == ZIO_TYPE_WRITE) {
+		zfs_dbgmsg("raidz write");
 		vdev_raidz_io_start_write(zio, rr, tvd->vdev_ashift);
 	} else {
 		ASSERT(zio->io_type == ZIO_TYPE_READ);
+		zfs_dbgmsg("raidz read");
 		vdev_raidz_io_start_read(zio, rr);
 	}
 
@@ -2270,6 +2317,8 @@ vdev_raidz_read_all(zio_t *zio, raidz_row_t *rr)
 static void
 vdev_raidz_io_done_unrecoverable(zio_t *zio)
 {
+	zfs_dbgmsg("vdev_raidz_io_done_unrecoverable() called\n");
+
 	raidz_map_t *rm = zio->io_vsd;
 
 	for (int i = 0; i < rm->rm_nrows; i++) {
@@ -2299,26 +2348,35 @@ vdev_raidz_io_done_unrecoverable(zio_t *zio)
 void
 vdev_raidz_io_done(zio_t *zio)
 {
+	zfs_dbgmsg("vdev_raidz_io_done(): %s", zio_type_name[zio->io_type]);
 	raidz_map_t *rm = zio->io_vsd;
 
 	if (zio->io_type == ZIO_TYPE_WRITE) {
+		zfs_dbgmsg("zio type is write\n");
 		for (int i = 0; i < rm->rm_nrows; i++) {
 			vdev_raidz_io_done_write_impl(zio, rm->rm_row[i]);
 		}
 	} else {
+		zfs_dbgmsg("zio type is read, reconstruct with known missing\n");
 		for (int i = 0; i < rm->rm_nrows; i++) {
 			raidz_row_t *rr = rm->rm_row[i];
 			vdev_raidz_io_done_reconstruct_known_missing(zio,
 			    rm, rr);
 		}
 
+		zfs_dbgmsg("Calling raidz_checksum_verify\n");
+
 		if (raidz_checksum_verify(zio) == 0) {
+			zfs_dbgmsg("raidz_checksum_verify return good\n");
+
 			for (int i = 0; i < rm->rm_nrows; i++) {
 				raidz_row_t *rr = rm->rm_row[i];
 				vdev_raidz_io_done_verified(zio, rr);
 			}
 			zio_checksum_verified(zio);
 		} else {
+			zfs_dbgmsg("raidz_checksum_verify return bad\n");
+
 			/*
 			 * A sequential resilver has no checksum which makes
 			 * combinatoral reconstruction impossible. This code
@@ -2341,6 +2399,8 @@ vdev_raidz_io_done(zio_t *zio)
 				nread += vdev_raidz_read_all(zio,
 				    rm->rm_row[i]);
 			}
+			zfs_dbgmsg("nread = %d\n", nread);
+
 			if (nread != 0) {
 				/*
 				 * Normally our stage is VDEV_IO_DONE, but if
@@ -2354,8 +2414,13 @@ vdev_raidz_io_done(zio_t *zio)
 			}
 
 			zio->io_error = vdev_raidz_combrec(zio);
+
+			zfs_dbgmsg("vdev_raidz_combrec return %d\n", zio->io_error);
+
 			if (zio->io_error == ECKSUM &&
 			    !(zio->io_flags & ZIO_FLAG_SPECULATIVE)) {
+				zfs_dbgmsg("Calling vdev_raidz_io_done_unrecoverable\n");
+
 				vdev_raidz_io_done_unrecoverable(zio);
 			}
 		}
diff --git a/module/zfs/zfs_ioctl.c b/module/zfs/zfs_ioctl.c
index f441328f3..023361f3f 100644
--- a/module/zfs/zfs_ioctl.c
+++ b/module/zfs/zfs_ioctl.c
@@ -221,6 +221,11 @@
 #include <sys/lua/lauxlib.h>
 #include <sys/zfs_ioctl_impl.h>
 
+// MLEC include
+#include <sys/vdev_raidz.h>
+#include <sys/vdev_raidz_impl.h>
+#include <sys/zfs_sa.h>
+
 kmutex_t zfsdev_state_lock;
 zfsdev_state_t *zfsdev_state_list;
 
@@ -242,19 +247,22 @@ uint_t zfs_fsyncer_key;
 uint_t zfs_allow_log_key;
 
 /* DATA_TYPE_ANY is used when zkey_type can vary. */
-#define	DATA_TYPE_ANY	DATA_TYPE_UNKNOWN
-
-typedef struct zfs_ioc_vec {
-	zfs_ioc_legacy_func_t	*zvec_legacy_func;
-	zfs_ioc_func_t		*zvec_func;
-	zfs_secpolicy_func_t	*zvec_secpolicy;
-	zfs_ioc_namecheck_t	zvec_namecheck;
-	boolean_t		zvec_allow_log;
-	zfs_ioc_poolcheck_t	zvec_pool_check;
-	boolean_t		zvec_smush_outnvlist;
-	const char		*zvec_name;
-	const zfs_ioc_key_t	*zvec_nvl_keys;
-	size_t			zvec_nvl_key_count;
+#define DATA_TYPE_ANY DATA_TYPE_UNKNOWN
+#define DATA_TYPE_ANY DATA_TYPE_UNKNOWN
+
+
+typedef struct zfs_ioc_vec
+{
+	zfs_ioc_legacy_func_t *zvec_legacy_func;
+	zfs_ioc_func_t *zvec_func;
+	zfs_secpolicy_func_t *zvec_secpolicy;
+	zfs_ioc_namecheck_t zvec_namecheck;
+	boolean_t zvec_allow_log;
+	zfs_ioc_poolcheck_t zvec_pool_check;
+	boolean_t zvec_smush_outnvlist;
+	const char *zvec_name;
+	const zfs_ioc_key_t *zvec_nvl_keys;
+	size_t zvec_nvl_key_count;
 } zfs_ioc_vec_t;
 
 /* This array is indexed by zfs_userquota_prop_t */
@@ -276,11 +284,11 @@ static const char *userquota_perms[] = {
 static int zfs_ioc_userspace_upgrade(zfs_cmd_t *zc);
 static int zfs_ioc_id_quota_upgrade(zfs_cmd_t *zc);
 static int zfs_check_settable(const char *name, nvpair_t *property,
-    cred_t *cr);
+							  cred_t *cr);
 static int zfs_check_clearable(const char *dataset, nvlist_t *props,
-    nvlist_t **errors);
+							   nvlist_t **errors);
 static int zfs_fill_zplprops_root(uint64_t, nvlist_t *, nvlist_t *,
-    boolean_t *);
+								  boolean_t *);
 int zfs_set_prop_nvlist(const char *, zprop_source_t, nvlist_t *, nvlist_t *);
 static int get_nvlist(uint64_t nvl, uint64_t size, int iflag, nvlist_t **nvp);
 
@@ -300,12 +308,13 @@ history_str_get(zfs_cmd_t *zc)
 
 	buf = kmem_alloc(HIS_MAX_RECORD_LEN, KM_SLEEP);
 	if (copyinstr((void *)(uintptr_t)zc->zc_history,
-	    buf, HIS_MAX_RECORD_LEN, NULL) != 0) {
+				  buf, HIS_MAX_RECORD_LEN, NULL) != 0)
+	{
 		history_str_free(buf);
 		return (NULL);
 	}
 
-	buf[HIS_MAX_RECORD_LEN -1] = '\0';
+	buf[HIS_MAX_RECORD_LEN - 1] = '\0';
 
 	return (buf);
 }
@@ -318,8 +327,10 @@ zfs_earlier_version(const char *name, int version)
 {
 	spa_t *spa;
 
-	if (spa_open(name, &spa, FTAG) == 0) {
-		if (spa_version(spa) < version) {
+	if (spa_open(name, &spa, FTAG) == 0)
+	{
+		if (spa_version(spa) < version)
+		{
 			spa_close(spa, FTAG);
 			return (1);
 		}
@@ -337,10 +348,12 @@ zpl_earlier_version(const char *name, int version)
 	objset_t *os;
 	boolean_t rc = B_TRUE;
 
-	if (dmu_objset_hold(name, FTAG, &os) == 0) {
+	if (dmu_objset_hold(name, FTAG, &os) == 0)
+	{
 		uint64_t zplversion;
 
-		if (dmu_objset_type(os) != DMU_OST_ZFS) {
+		if (dmu_objset_type(os) != DMU_OST_ZFS)
+		{
 			dmu_objset_rele(os, FTAG);
 			return (B_TRUE);
 		}
@@ -361,9 +374,10 @@ zfs_log_history(zfs_cmd_t *zc)
 	if ((buf = history_str_get(zc)) == NULL)
 		return;
 
-	if (spa_open(zc->zc_name, &spa, FTAG) == 0) {
+	if (spa_open(zc->zc_name, &spa, FTAG) == 0)
+	{
 		if (spa_version(spa) >= SPA_VERSION_ZPOOL_HISTORY)
-			(void) spa_history_log(spa, buf);
+			(void)spa_history_log(spa, buf);
 		spa_close(spa, FTAG);
 	}
 	history_str_free(buf);
@@ -389,7 +403,7 @@ static int
 zfs_secpolicy_read(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
 	if (INGLOBALZONE(curproc) ||
-	    zone_dataset_visible(zc->zc_name, NULL))
+		zone_dataset_visible(zc->zc_name, NULL))
 		return (0);
 
 	return (SET_ERROR(ENOENT));
@@ -405,17 +419,20 @@ zfs_dozonecheck_impl(const char *dataset, uint64_t zoned, cred_t *cr)
 	 * so they don't see EPERM on something they shouldn't know about.
 	 */
 	if (!INGLOBALZONE(curproc) &&
-	    !zone_dataset_visible(dataset, &writable))
+		!zone_dataset_visible(dataset, &writable))
 		return (SET_ERROR(ENOENT));
 
-	if (INGLOBALZONE(curproc)) {
+	if (INGLOBALZONE(curproc))
+	{
 		/*
 		 * If the fs is zoned, only root can access it from the
 		 * global zone.
 		 */
 		if (secpolicy_zfs(cr) && zoned)
 			return (SET_ERROR(EPERM));
-	} else {
+	}
+	else
+	{
 		/*
 		 * If we are in a local zone, the 'zoned' property must be set.
 		 */
@@ -435,7 +452,7 @@ zfs_dozonecheck(const char *dataset, cred_t *cr)
 	uint64_t zoned;
 
 	if (dsl_prop_get_integer(dataset, zfs_prop_to_name(ZFS_PROP_ZONED),
-	    &zoned, NULL))
+							 &zoned, NULL))
 		return (SET_ERROR(ENOENT));
 
 	return (zfs_dozonecheck_impl(dataset, zoned, cr));
@@ -454,12 +471,13 @@ zfs_dozonecheck_ds(const char *dataset, dsl_dataset_t *ds, cred_t *cr)
 
 static int
 zfs_secpolicy_write_perms_ds(const char *name, dsl_dataset_t *ds,
-    const char *perm, cred_t *cr)
+							 const char *perm, cred_t *cr)
 {
 	int error;
 
 	error = zfs_dozonecheck_ds(name, ds, cr);
-	if (error == 0) {
+	if (error == 0)
+	{
 		error = secpolicy_zfs(cr);
 		if (error != 0)
 			error = dsl_deleg_access_impl(ds, perm, cr);
@@ -487,7 +505,8 @@ zfs_secpolicy_write_perms(const char *name, const char *perm, cred_t *cr)
 		return (error);
 
 	error = dsl_dataset_hold(dp, name, FTAG, &ds);
-	if (error != 0) {
+	if (error != 0)
+	{
 		dsl_pool_rele(dp, FTAG);
 		return (error);
 	}
@@ -508,16 +527,16 @@ static int
 zfs_set_slabel_policy(const char *name, const char *strval, cred_t *cr)
 {
 #ifdef HAVE_MLSLABEL
-	char		ds_hexsl[MAXNAMELEN];
-	bslabel_t	ds_sl, new_sl;
-	boolean_t	new_default = FALSE;
-	uint64_t	zoned;
-	int		needed_priv = -1;
-	int		error;
+	char ds_hexsl[MAXNAMELEN];
+	bslabel_t ds_sl, new_sl;
+	boolean_t new_default = FALSE;
+	uint64_t zoned;
+	int needed_priv = -1;
+	int error;
 
 	/* First get the existing dataset label. */
 	error = dsl_prop_get(name, zfs_prop_to_name(ZFS_PROP_MLSLABEL),
-	    1, sizeof (ds_hexsl), &ds_hexsl, NULL);
+						 1, sizeof(ds_hexsl), &ds_hexsl, NULL);
 	if (error != 0)
 		return (SET_ERROR(EPERM));
 
@@ -533,7 +552,8 @@ zfs_set_slabel_policy(const char *name, const char *strval, cred_t *cr)
 	 * doesn't match that of the zone; otherwise no other checks
 	 * are needed.
 	 */
-	if (!INGLOBALZONE(curproc)) {
+	if (!INGLOBALZONE(curproc))
+	{
 		if (new_default || !blequal(&new_sl, CR_SL(CRED())))
 			return (SET_ERROR(EPERM));
 		return (0);
@@ -545,9 +565,10 @@ zfs_set_slabel_policy(const char *name, const char *strval, cred_t *cr)
 	 * global zone.
 	 */
 	if (dsl_prop_get_integer(name,
-	    zfs_prop_to_name(ZFS_PROP_ZONED), &zoned, NULL))
+							 zfs_prop_to_name(ZFS_PROP_ZONED), &zoned, NULL))
 		return (SET_ERROR(EPERM));
-	if (!zoned) {
+	if (!zoned)
+	{
 		if (zfs_check_global_label(name, strval) != 0)
 			return (SET_ERROR(EPERM));
 	}
@@ -558,7 +579,8 @@ zfs_set_slabel_policy(const char *name, const char *strval, cred_t *cr)
 	 * Get the zfsvfs_t; if there isn't one, then the dataset isn't
 	 * mounted (or isn't a dataset, doesn't exist, ...).
 	 */
-	if (strcasecmp(ds_hexsl, ZFS_MLSLABEL_DEFAULT) != 0) {
+	if (strcasecmp(ds_hexsl, ZFS_MLSLABEL_DEFAULT) != 0)
+	{
 		objset_t *os;
 		static const char *setsl_tag = "setsl_tag";
 
@@ -567,13 +589,14 @@ zfs_set_slabel_policy(const char *name, const char *strval, cred_t *cr)
 		 * (e.g., already mounted, in use, or other error).
 		 */
 		error = dmu_objset_own(name, DMU_OST_ZFS, B_TRUE, B_TRUE,
-		    setsl_tag, &os);
+							   setsl_tag, &os);
 		if (error != 0)
 			return (SET_ERROR(EPERM));
 
 		dmu_objset_disown(os, B_TRUE, setsl_tag);
 
-		if (new_default) {
+		if (new_default)
+		{
 			needed_priv = PRIV_FILE_DOWNGRADE_SL;
 			goto out_check;
 		}
@@ -585,7 +608,9 @@ zfs_set_slabel_policy(const char *name, const char *strval, cred_t *cr)
 			needed_priv = PRIV_FILE_DOWNGRADE_SL;
 		else if (blstrictdom(&new_sl, &ds_sl))
 			needed_priv = PRIV_FILE_UPGRADE_SL;
-	} else {
+	}
+	else
+	{
 		/* dataset currently has a default label */
 		if (!new_default)
 			needed_priv = PRIV_FILE_UPGRADE_SL;
@@ -602,14 +627,15 @@ out_check:
 
 static int
 zfs_secpolicy_setprop(const char *dsname, zfs_prop_t prop, nvpair_t *propval,
-    cred_t *cr)
+					  cred_t *cr)
 {
 	char *strval;
 
 	/*
 	 * Check permissions for special properties.
 	 */
-	switch (prop) {
+	switch (prop)
+	{
 	default:
 		break;
 	case ZFS_PROP_ZONED:
@@ -623,7 +649,8 @@ zfs_secpolicy_setprop(const char *dsname, zfs_prop_t prop, nvpair_t *propval,
 	case ZFS_PROP_QUOTA:
 	case ZFS_PROP_FILESYSTEM_LIMIT:
 	case ZFS_PROP_SNAPSHOT_LIMIT:
-		if (!INGLOBALZONE(curproc)) {
+		if (!INGLOBALZONE(curproc))
+		{
 			uint64_t zoned;
 			char setpoint[ZFS_MAX_DATASET_NAME_LEN];
 			/*
@@ -632,7 +659,7 @@ zfs_secpolicy_setprop(const char *dsname, zfs_prop_t prop, nvpair_t *propval,
 			 * the thing they own.
 			 */
 			if (dsl_prop_get_integer(dsname,
-			    zfs_prop_to_name(ZFS_PROP_ZONED), &zoned, setpoint))
+									 zfs_prop_to_name(ZFS_PROP_ZONED), &zoned, setpoint))
 				return (SET_ERROR(EPERM));
 			if (!zoned || strlen(dsname) <= strlen(setpoint))
 				return (SET_ERROR(EPERM));
@@ -643,7 +670,8 @@ zfs_secpolicy_setprop(const char *dsname, zfs_prop_t prop, nvpair_t *propval,
 		if (!is_system_labeled())
 			return (SET_ERROR(EPERM));
 
-		if (nvpair_value_string(propval, &strval) == 0) {
+		if (nvpair_value_string(propval, &strval) == 0)
+		{
 			int err;
 
 			err = zfs_set_slabel_policy(dsname, strval, CRED());
@@ -678,7 +706,7 @@ static int
 zfs_secpolicy_rollback(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
 	return (zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_ROLLBACK, cr));
+									  ZFS_DELEG_PERM_ROLLBACK, cr));
 }
 
 /* ARGSUSED */
@@ -702,7 +730,8 @@ zfs_secpolicy_send(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 		return (error);
 
 	error = dsl_dataset_hold_obj(dp, zc->zc_sendobj, FTAG, &ds);
-	if (error != 0) {
+	if (error != 0)
+	{
 		dsl_pool_rele(dp, FTAG);
 		return (error);
 	}
@@ -710,7 +739,7 @@ zfs_secpolicy_send(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	dsl_dataset_name(ds, zc->zc_name);
 
 	error = zfs_secpolicy_write_perms_ds(zc->zc_name, ds,
-	    ZFS_DELEG_PERM_SEND, cr);
+										 ZFS_DELEG_PERM_SEND, cr);
 	dsl_dataset_rele(ds, FTAG);
 	dsl_pool_rele(dp, FTAG);
 
@@ -722,7 +751,7 @@ static int
 zfs_secpolicy_send_new(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
 	return (zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_SEND, cr));
+									  ZFS_DELEG_PERM_SEND, cr));
 }
 
 static int
@@ -745,11 +774,14 @@ zfs_get_parent(const char *datasetname, char *parent, int parentsize)
 	/*
 	 * Remove the @bla or /bla from the end of the name to get the parent.
 	 */
-	(void) strncpy(parent, datasetname, parentsize);
+	(void)strncpy(parent, datasetname, parentsize);
 	cp = strrchr(parent, '@');
-	if (cp != NULL) {
+	if (cp != NULL)
+	{
 		cp[0] = '\0';
-	} else {
+	}
+	else
+	{
 		cp = strrchr(parent, '/');
 		if (cp == NULL)
 			return (SET_ERROR(ENOENT));
@@ -759,13 +791,12 @@ zfs_get_parent(const char *datasetname, char *parent, int parentsize)
 	return (0);
 }
 
-int
-zfs_secpolicy_destroy_perms(const char *name, cred_t *cr)
+int zfs_secpolicy_destroy_perms(const char *name, cred_t *cr)
 {
 	int error;
 
 	if ((error = zfs_secpolicy_write_perms(name,
-	    ZFS_DELEG_PERM_MOUNT, cr)) != 0)
+										   ZFS_DELEG_PERM_MOUNT, cr)) != 0)
 		return (error);
 
 	return (zfs_secpolicy_write_perms(name, ZFS_DELEG_PERM_DESTROY, cr));
@@ -793,10 +824,12 @@ zfs_secpolicy_destroy_snaps(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	snaps = fnvlist_lookup_nvlist(innvl, "snaps");
 
 	for (pair = nvlist_next_nvpair(snaps, NULL); pair != NULL;
-	    pair = nextpair) {
+		 pair = nextpair)
+	{
 		nextpair = nvlist_next_nvpair(snaps, pair);
 		error = zfs_secpolicy_destroy_perms(nvpair_name(pair), cr);
-		if (error == ENOENT) {
+		if (error == ENOENT)
+		{
 			/*
 			 * Ignore any snapshots that don't exist (we consider
 			 * them "already destroyed").  Remove the name from the
@@ -815,30 +848,29 @@ zfs_secpolicy_destroy_snaps(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	return (error);
 }
 
-int
-zfs_secpolicy_rename_perms(const char *from, const char *to, cred_t *cr)
+int zfs_secpolicy_rename_perms(const char *from, const char *to, cred_t *cr)
 {
-	char	parentname[ZFS_MAX_DATASET_NAME_LEN];
-	int	error;
+	char parentname[ZFS_MAX_DATASET_NAME_LEN];
+	int error;
 
 	if ((error = zfs_secpolicy_write_perms(from,
-	    ZFS_DELEG_PERM_RENAME, cr)) != 0)
+										   ZFS_DELEG_PERM_RENAME, cr)) != 0)
 		return (error);
 
 	if ((error = zfs_secpolicy_write_perms(from,
-	    ZFS_DELEG_PERM_MOUNT, cr)) != 0)
+										   ZFS_DELEG_PERM_MOUNT, cr)) != 0)
 		return (error);
 
 	if ((error = zfs_get_parent(to, parentname,
-	    sizeof (parentname))) != 0)
+								sizeof(parentname))) != 0)
 		return (error);
 
 	if ((error = zfs_secpolicy_write_perms(parentname,
-	    ZFS_DELEG_PERM_CREATE, cr)) != 0)
+										   ZFS_DELEG_PERM_CREATE, cr)) != 0)
 		return (error);
 
 	if ((error = zfs_secpolicy_write_perms(parentname,
-	    ZFS_DELEG_PERM_MOUNT, cr)) != 0)
+										   ZFS_DELEG_PERM_MOUNT, cr)) != 0)
 		return (error);
 
 	return (error);
@@ -860,7 +892,7 @@ zfs_secpolicy_promote(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	int error;
 
 	error = zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_PROMOTE, cr);
+									  ZFS_DELEG_PERM_PROMOTE, cr);
 	if (error != 0)
 		return (error);
 
@@ -870,27 +902,30 @@ zfs_secpolicy_promote(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 
 	error = dsl_dataset_hold(dp, zc->zc_name, FTAG, &clone);
 
-	if (error == 0) {
+	if (error == 0)
+	{
 		char parentname[ZFS_MAX_DATASET_NAME_LEN];
 		dsl_dataset_t *origin = NULL;
 		dsl_dir_t *dd;
 		dd = clone->ds_dir;
 
 		error = dsl_dataset_hold_obj(dd->dd_pool,
-		    dsl_dir_phys(dd)->dd_origin_obj, FTAG, &origin);
-		if (error != 0) {
+									 dsl_dir_phys(dd)->dd_origin_obj, FTAG, &origin);
+		if (error != 0)
+		{
 			dsl_dataset_rele(clone, FTAG);
 			dsl_pool_rele(dp, FTAG);
 			return (error);
 		}
 
 		error = zfs_secpolicy_write_perms_ds(zc->zc_name, clone,
-		    ZFS_DELEG_PERM_MOUNT, cr);
+											 ZFS_DELEG_PERM_MOUNT, cr);
 
 		dsl_dataset_name(origin, parentname);
-		if (error == 0) {
+		if (error == 0)
+		{
 			error = zfs_secpolicy_write_perms_ds(parentname, origin,
-			    ZFS_DELEG_PERM_PROMOTE, cr);
+												 ZFS_DELEG_PERM_PROMOTE, cr);
 		}
 		dsl_dataset_rele(clone, FTAG);
 		dsl_dataset_rele(origin, FTAG);
@@ -906,15 +941,15 @@ zfs_secpolicy_recv(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	int error;
 
 	if ((error = zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_RECEIVE, cr)) != 0)
+										   ZFS_DELEG_PERM_RECEIVE, cr)) != 0)
 		return (error);
 
 	if ((error = zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_MOUNT, cr)) != 0)
+										   ZFS_DELEG_PERM_MOUNT, cr)) != 0)
 		return (error);
 
 	return (zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_CREATE, cr));
+									  ZFS_DELEG_PERM_CREATE, cr));
 }
 
 /* ARGSUSED */
@@ -924,11 +959,10 @@ zfs_secpolicy_recv_new(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	return (zfs_secpolicy_recv(zc, innvl, cr));
 }
 
-int
-zfs_secpolicy_snapshot_perms(const char *name, cred_t *cr)
+int zfs_secpolicy_snapshot_perms(const char *name, cred_t *cr)
 {
 	return (zfs_secpolicy_write_perms(name,
-	    ZFS_DELEG_PERM_SNAPSHOT, cr));
+									  ZFS_DELEG_PERM_SNAPSHOT, cr));
 }
 
 /*
@@ -945,11 +979,13 @@ zfs_secpolicy_snapshot(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	snaps = fnvlist_lookup_nvlist(innvl, "snaps");
 
 	for (pair = nvlist_next_nvpair(snaps, NULL); pair != NULL;
-	    pair = nvlist_next_nvpair(snaps, pair)) {
+		 pair = nvlist_next_nvpair(snaps, pair))
+	{
 		char *name = nvpair_name(pair);
 		char *atp = strchr(name, '@');
 
-		if (atp == NULL) {
+		if (atp == NULL)
+		{
 			error = SET_ERROR(EINVAL);
 			break;
 		}
@@ -972,17 +1008,19 @@ zfs_secpolicy_bookmark(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	int error = 0;
 
 	for (nvpair_t *pair = nvlist_next_nvpair(innvl, NULL);
-	    pair != NULL; pair = nvlist_next_nvpair(innvl, pair)) {
+		 pair != NULL; pair = nvlist_next_nvpair(innvl, pair))
+	{
 		char *name = nvpair_name(pair);
 		char *hashp = strchr(name, '#');
 
-		if (hashp == NULL) {
+		if (hashp == NULL)
+		{
 			error = SET_ERROR(EINVAL);
 			break;
 		}
 		*hashp = '\0';
 		error = zfs_secpolicy_write_perms(name,
-		    ZFS_DELEG_PERM_BOOKMARK, cr);
+										  ZFS_DELEG_PERM_BOOKMARK, cr);
 		*hashp = '#';
 		if (error != 0)
 			break;
@@ -998,21 +1036,24 @@ zfs_secpolicy_destroy_bookmarks(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	int error = 0;
 
 	for (pair = nvlist_next_nvpair(innvl, NULL); pair != NULL;
-	    pair = nextpair) {
+		 pair = nextpair)
+	{
 		char *name = nvpair_name(pair);
 		char *hashp = strchr(name, '#');
 		nextpair = nvlist_next_nvpair(innvl, pair);
 
-		if (hashp == NULL) {
+		if (hashp == NULL)
+		{
 			error = SET_ERROR(EINVAL);
 			break;
 		}
 
 		*hashp = '\0';
 		error = zfs_secpolicy_write_perms(name,
-		    ZFS_DELEG_PERM_DESTROY, cr);
+										  ZFS_DELEG_PERM_DESTROY, cr);
 		*hashp = '#';
-		if (error == ENOENT) {
+		if (error == ENOENT)
+		{
 			/*
 			 * Ignore any filesystems that don't exist (we consider
 			 * their bookmarks "already destroyed").  Remove
@@ -1047,25 +1088,25 @@ zfs_secpolicy_log_history(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 static int
 zfs_secpolicy_create_clone(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
-	char	parentname[ZFS_MAX_DATASET_NAME_LEN];
-	int	error;
-	char	*origin;
+	char parentname[ZFS_MAX_DATASET_NAME_LEN];
+	int error;
+	char *origin;
 
 	if ((error = zfs_get_parent(zc->zc_name, parentname,
-	    sizeof (parentname))) != 0)
+								sizeof(parentname))) != 0)
 		return (error);
 
 	if (nvlist_lookup_string(innvl, "origin", &origin) == 0 &&
-	    (error = zfs_secpolicy_write_perms(origin,
-	    ZFS_DELEG_PERM_CLONE, cr)) != 0)
+		(error = zfs_secpolicy_write_perms(origin,
+										   ZFS_DELEG_PERM_CLONE, cr)) != 0)
 		return (error);
 
 	if ((error = zfs_secpolicy_write_perms(parentname,
-	    ZFS_DELEG_PERM_CREATE, cr)) != 0)
+										   ZFS_DELEG_PERM_CREATE, cr)) != 0)
 		return (error);
 
 	return (zfs_secpolicy_write_perms(parentname,
-	    ZFS_DELEG_PERM_MOUNT, cr));
+									  ZFS_DELEG_PERM_MOUNT, cr));
 }
 
 /*
@@ -1073,8 +1114,7 @@ zfs_secpolicy_create_clone(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
  * SYS_CONFIG privilege, which is not available in a local zone.
  */
 /* ARGSUSED */
-int
-zfs_secpolicy_config(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
+int zfs_secpolicy_config(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
 	if (secpolicy_sys_config(cr, B_FALSE) != 0)
 		return (SET_ERROR(EPERM));
@@ -1114,14 +1154,17 @@ zfs_secpolicy_inherit_prop(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
 	zfs_prop_t prop = zfs_name_to_prop(zc->zc_value);
 
-	if (prop == ZPROP_INVAL) {
+	if (prop == ZPROP_INVAL)
+	{
 		if (!zfs_prop_user(zc->zc_value))
 			return (SET_ERROR(EINVAL));
 		return (zfs_secpolicy_write_perms(zc->zc_name,
-		    ZFS_DELEG_PERM_USERPROP, cr));
-	} else {
+										  ZFS_DELEG_PERM_USERPROP, cr));
+	}
+	else
+	{
 		return (zfs_secpolicy_setprop(zc->zc_name, prop,
-		    NULL, cr));
+									  NULL, cr));
 	}
 }
 
@@ -1135,21 +1178,25 @@ zfs_secpolicy_userspace_one(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	if (zc->zc_objset_type >= ZFS_NUM_USERQUOTA_PROPS)
 		return (SET_ERROR(EINVAL));
 
-	if (zc->zc_value[0] == 0) {
+	if (zc->zc_value[0] == 0)
+	{
 		/*
 		 * They are asking about a posix uid/gid.  If it's
 		 * themself, allow it.
 		 */
 		if (zc->zc_objset_type == ZFS_PROP_USERUSED ||
-		    zc->zc_objset_type == ZFS_PROP_USERQUOTA ||
-		    zc->zc_objset_type == ZFS_PROP_USEROBJUSED ||
-		    zc->zc_objset_type == ZFS_PROP_USEROBJQUOTA) {
+			zc->zc_objset_type == ZFS_PROP_USERQUOTA ||
+			zc->zc_objset_type == ZFS_PROP_USEROBJUSED ||
+			zc->zc_objset_type == ZFS_PROP_USEROBJQUOTA)
+		{
 			if (zc->zc_guid == crgetuid(cr))
 				return (0);
-		} else if (zc->zc_objset_type == ZFS_PROP_GROUPUSED ||
-		    zc->zc_objset_type == ZFS_PROP_GROUPQUOTA ||
-		    zc->zc_objset_type == ZFS_PROP_GROUPOBJUSED ||
-		    zc->zc_objset_type == ZFS_PROP_GROUPOBJQUOTA) {
+		}
+		else if (zc->zc_objset_type == ZFS_PROP_GROUPUSED ||
+				 zc->zc_objset_type == ZFS_PROP_GROUPQUOTA ||
+				 zc->zc_objset_type == ZFS_PROP_GROUPOBJUSED ||
+				 zc->zc_objset_type == ZFS_PROP_GROUPOBJQUOTA)
+		{
 			if (groupmember(zc->zc_guid, cr))
 				return (0);
 		}
@@ -1157,7 +1204,7 @@ zfs_secpolicy_userspace_one(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	}
 
 	return (zfs_secpolicy_write_perms(zc->zc_name,
-	    userquota_perms[zc->zc_objset_type], cr));
+									  userquota_perms[zc->zc_objset_type], cr));
 }
 
 static int
@@ -1171,7 +1218,7 @@ zfs_secpolicy_userspace_many(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 		return (SET_ERROR(EINVAL));
 
 	return (zfs_secpolicy_write_perms(zc->zc_name,
-	    userquota_perms[zc->zc_objset_type], cr));
+									  userquota_perms[zc->zc_objset_type], cr));
 }
 
 /* ARGSUSED */
@@ -1179,7 +1226,7 @@ static int
 zfs_secpolicy_userspace_upgrade(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
 	return (zfs_secpolicy_setprop(zc->zc_name, ZFS_PROP_VERSION,
-	    NULL, cr));
+								  NULL, cr));
 }
 
 /* ARGSUSED */
@@ -1193,13 +1240,14 @@ zfs_secpolicy_hold(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	holds = fnvlist_lookup_nvlist(innvl, "holds");
 
 	for (pair = nvlist_next_nvpair(holds, NULL); pair != NULL;
-	    pair = nvlist_next_nvpair(holds, pair)) {
+		 pair = nvlist_next_nvpair(holds, pair))
+	{
 		char fsname[ZFS_MAX_DATASET_NAME_LEN];
 		error = dmu_fsname(nvpair_name(pair), fsname);
 		if (error != 0)
 			return (error);
 		error = zfs_secpolicy_write_perms(fsname,
-		    ZFS_DELEG_PERM_HOLD, cr);
+										  ZFS_DELEG_PERM_HOLD, cr);
 		if (error != 0)
 			return (error);
 	}
@@ -1214,13 +1262,14 @@ zfs_secpolicy_release(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	int error;
 
 	for (pair = nvlist_next_nvpair(innvl, NULL); pair != NULL;
-	    pair = nvlist_next_nvpair(innvl, pair)) {
+		 pair = nvlist_next_nvpair(innvl, pair))
+	{
 		char fsname[ZFS_MAX_DATASET_NAME_LEN];
 		error = dmu_fsname(nvpair_name(pair), fsname);
 		if (error != 0)
 			return (error);
 		error = zfs_secpolicy_write_perms(fsname,
-		    ZFS_DELEG_PERM_RELEASE, cr);
+										  ZFS_DELEG_PERM_RELEASE, cr);
 		if (error != 0)
 			return (error);
 	}
@@ -1241,12 +1290,13 @@ zfs_secpolicy_tmp_snapshot(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 	int error;
 
 	if ((error = zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_DIFF, cr)) == 0)
+										   ZFS_DELEG_PERM_DIFF, cr)) == 0)
 		return (0);
 
 	error = zfs_secpolicy_snapshot_perms(zc->zc_name, cr);
 
-	if (innvl != NULL) {
+	if (innvl != NULL)
+	{
 		if (error == 0)
 			error = zfs_secpolicy_hold(zc, innvl, cr);
 		if (error == 0)
@@ -1261,14 +1311,14 @@ static int
 zfs_secpolicy_load_key(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
 	return (zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_LOAD_KEY, cr));
+									  ZFS_DELEG_PERM_LOAD_KEY, cr));
 }
 
 static int
 zfs_secpolicy_change_key(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
 {
 	return (zfs_secpolicy_write_perms(zc->zc_name,
-	    ZFS_DELEG_PERM_CHANGE_KEY, cr));
+									  ZFS_DELEG_PERM_CHANGE_KEY, cr));
 }
 
 /*
@@ -1290,12 +1340,14 @@ get_nvlist(uint64_t nvl, uint64_t size, int iflag, nvlist_t **nvp)
 	packed = vmem_alloc(size, KM_SLEEP);
 
 	if ((error = ddi_copyin((void *)(uintptr_t)nvl, packed, size,
-	    iflag)) != 0) {
+							iflag)) != 0)
+	{
 		vmem_free(packed, size);
 		return (SET_ERROR(EFAULT));
 	}
 
-	if ((error = nvlist_unpack(packed, size, &list, 0)) != 0) {
+	if ((error = nvlist_unpack(packed, size, &list, 0)) != 0)
+	{
 		vmem_free(packed, size);
 		return (error);
 	}
@@ -1319,7 +1371,8 @@ nvlist_smush(nvlist_t *errors, size_t max)
 
 	size = fnvlist_size(errors);
 
-	if (size > max) {
+	if (size > max)
+	{
 		nvpair_t *more_errors;
 		int n = 0;
 
@@ -1329,9 +1382,10 @@ nvlist_smush(nvlist_t *errors, size_t max)
 		fnvlist_add_int32(errors, ZPROP_N_MORE_ERRORS, 0);
 		more_errors = nvlist_prev_nvpair(errors, NULL);
 
-		do {
+		do
+		{
 			nvpair_t *pair = nvlist_prev_nvpair(errors,
-			    more_errors);
+												more_errors);
 			fnvlist_remove_nvpair(errors, pair);
 			n++;
 			size = fnvlist_size(errors);
@@ -1354,12 +1408,15 @@ put_nvlist(zfs_cmd_t *zc, nvlist_t *nvl)
 
 	size = fnvlist_size(nvl);
 
-	if (size > zc->zc_nvlist_dst_size) {
+	if (size > zc->zc_nvlist_dst_size)
+	{
 		error = SET_ERROR(ENOMEM);
-	} else {
+	}
+	else
+	{
 		packed = fnvlist_pack(nvl, &size);
 		if (ddi_copyout(packed, (void *)(uintptr_t)zc->zc_nvlist_dst,
-		    size, zc->zc_iflags) != 0)
+						size, zc->zc_iflags) != 0)
 			error = SET_ERROR(EFAULT);
 		fnvlist_pack_free(packed, size);
 	}
@@ -1369,11 +1426,11 @@ put_nvlist(zfs_cmd_t *zc, nvlist_t *nvl)
 	return (error);
 }
 
-int
-getzfsvfs_impl(objset_t *os, zfsvfs_t **zfvp)
+int getzfsvfs_impl(objset_t *os, zfsvfs_t **zfvp)
 {
 	int error = 0;
-	if (dmu_objset_type(os) != DMU_OST_ZFS) {
+	if (dmu_objset_type(os) != DMU_OST_ZFS)
+	{
 		return (SET_ERROR(EINVAL));
 	}
 
@@ -1385,8 +1442,7 @@ getzfsvfs_impl(objset_t *os, zfsvfs_t **zfvp)
 	return (error);
 }
 
-int
-getzfsvfs(const char *dsname, zfsvfs_t **zfvp)
+int getzfsvfs(const char *dsname, zfsvfs_t **zfvp)
 {
 	objset_t *os;
 	int error;
@@ -1413,12 +1469,14 @@ zfsvfs_hold(const char *name, void *tag, zfsvfs_t **zfvp, boolean_t writer)
 
 	if (getzfsvfs(name, zfvp) != 0)
 		error = zfsvfs_create(name, B_FALSE, zfvp);
-	if (error == 0) {
+	if (error == 0)
+	{
 		if (writer)
 			ZFS_TEARDOWN_ENTER_WRITE(*zfvp, tag);
 		else
 			ZFS_TEARDOWN_ENTER_READ(*zfvp, tag);
-		if ((*zfvp)->z_unmounted) {
+		if ((*zfvp)->z_unmounted)
+		{
 			/*
 			 * XXX we could probably try again, since the unmounting
 			 * thread should be just about to disassociate the
@@ -1436,9 +1494,12 @@ zfsvfs_rele(zfsvfs_t *zfsvfs, void *tag)
 {
 	ZFS_TEARDOWN_EXIT(zfsvfs, tag);
 
-	if (zfs_vfs_held(zfsvfs)) {
+	if (zfs_vfs_held(zfsvfs))
+	{
 		zfs_vfs_rele(zfsvfs);
-	} else {
+	}
+	else
+	{
 		dmu_objset_disown(zfsvfs->z_os, B_TRUE, zfsvfs);
 		zfsvfs_free(zfsvfs);
 	}
@@ -1456,52 +1517,56 @@ zfs_ioc_pool_create(zfs_cmd_t *zc)
 	boolean_t unload_wkey = B_TRUE;
 
 	if ((error = get_nvlist(zc->zc_nvlist_conf, zc->zc_nvlist_conf_size,
-	    zc->zc_iflags, &config)))
+							zc->zc_iflags, &config)))
 		return (error);
 
 	if (zc->zc_nvlist_src_size != 0 && (error =
-	    get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-	    zc->zc_iflags, &props))) {
+											get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
+													   zc->zc_iflags, &props)))
+	{
 		nvlist_free(config);
 		return (error);
 	}
 
-	if (props) {
+	if (props)
+	{
 		nvlist_t *nvl = NULL;
 		nvlist_t *hidden_args = NULL;
 		uint64_t version = SPA_VERSION;
 		char *tname;
 
-		(void) nvlist_lookup_uint64(props,
-		    zpool_prop_to_name(ZPOOL_PROP_VERSION), &version);
-		if (!SPA_VERSION_IS_SUPPORTED(version)) {
+		(void)nvlist_lookup_uint64(props,
+								   zpool_prop_to_name(ZPOOL_PROP_VERSION), &version);
+		if (!SPA_VERSION_IS_SUPPORTED(version))
+		{
 			error = SET_ERROR(EINVAL);
 			goto pool_props_bad;
 		}
-		(void) nvlist_lookup_nvlist(props, ZPOOL_ROOTFS_PROPS, &nvl);
-		if (nvl) {
+		(void)nvlist_lookup_nvlist(props, ZPOOL_ROOTFS_PROPS, &nvl);
+		if (nvl)
+		{
 			error = nvlist_dup(nvl, &rootprops, KM_SLEEP);
 			if (error != 0)
 				goto pool_props_bad;
-			(void) nvlist_remove_all(props, ZPOOL_ROOTFS_PROPS);
+			(void)nvlist_remove_all(props, ZPOOL_ROOTFS_PROPS);
 		}
 
-		(void) nvlist_lookup_nvlist(props, ZPOOL_HIDDEN_ARGS,
-		    &hidden_args);
+		(void)nvlist_lookup_nvlist(props, ZPOOL_HIDDEN_ARGS,
+								   &hidden_args);
 		error = dsl_crypto_params_create_nvlist(DCP_CMD_NONE,
-		    rootprops, hidden_args, &dcp);
+												rootprops, hidden_args, &dcp);
 		if (error != 0)
 			goto pool_props_bad;
-		(void) nvlist_remove_all(props, ZPOOL_HIDDEN_ARGS);
+		(void)nvlist_remove_all(props, ZPOOL_HIDDEN_ARGS);
 
 		VERIFY(nvlist_alloc(&zplprops, NV_UNIQUE_NAME, KM_SLEEP) == 0);
 		error = zfs_fill_zplprops_root(version, rootprops,
-		    zplprops, NULL);
+									   zplprops, NULL);
 		if (error != 0)
 			goto pool_props_bad;
 
 		if (nvlist_lookup_string(props,
-		    zpool_prop_to_name(ZPOOL_PROP_TNAME), &tname) == 0)
+								 zpool_prop_to_name(ZPOOL_PROP_TNAME), &tname) == 0)
 			spa_name = tname;
 	}
 
@@ -1511,8 +1576,9 @@ zfs_ioc_pool_create(zfs_cmd_t *zc)
 	 * Set the remaining root properties
 	 */
 	if (!error && (error = zfs_set_prop_nvlist(spa_name,
-	    ZPROP_SRC_LOCAL, rootprops, NULL)) != 0) {
-		(void) spa_destroy(spa_name);
+											   ZPROP_SRC_LOCAL, rootprops, NULL)) != 0)
+	{
+		(void)spa_destroy(spa_name);
 		unload_wkey = B_FALSE; /* spa_destroy() unloads wrapping keys */
 	}
 
@@ -1544,23 +1610,25 @@ zfs_ioc_pool_import(zfs_cmd_t *zc)
 	int error;
 
 	if ((error = get_nvlist(zc->zc_nvlist_conf, zc->zc_nvlist_conf_size,
-	    zc->zc_iflags, &config)) != 0)
+							zc->zc_iflags, &config)) != 0)
 		return (error);
 
 	if (zc->zc_nvlist_src_size != 0 && (error =
-	    get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-	    zc->zc_iflags, &props))) {
+											get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
+													   zc->zc_iflags, &props)))
+	{
 		nvlist_free(config);
 		return (error);
 	}
 
 	if (nvlist_lookup_uint64(config, ZPOOL_CONFIG_POOL_GUID, &guid) != 0 ||
-	    guid != zc->zc_guid)
+		guid != zc->zc_guid)
 		error = SET_ERROR(EINVAL);
 	else
 		error = spa_import(zc->zc_name, config, props, zc->zc_cookie);
 
-	if (zc->zc_nvlist_dst != 0) {
+	if (zc->zc_nvlist_dst != 0)
+	{
 		int err;
 
 		if ((err = put_nvlist(zc, config)) != 0)
@@ -1619,9 +1687,10 @@ zfs_ioc_pool_stats(zfs_cmd_t *zc)
 	int ret = 0;
 
 	error = spa_get_stats(zc->zc_name, &config, zc->zc_value,
-	    sizeof (zc->zc_value));
+						  sizeof(zc->zc_value));
 
-	if (config != NULL) {
+	if (config != NULL)
+	{
 		ret = put_nvlist(zc, config);
 		nvlist_free(config);
 
@@ -1631,7 +1700,9 @@ zfs_ioc_pool_stats(zfs_cmd_t *zc)
 		 * in 'zc_cookie'.
 		 */
 		zc->zc_cookie = error;
-	} else {
+	}
+	else
+	{
 		ret = error;
 	}
 
@@ -1649,7 +1720,7 @@ zfs_ioc_pool_tryimport(zfs_cmd_t *zc)
 	int error;
 
 	if ((error = get_nvlist(zc->zc_nvlist_conf, zc->zc_nvlist_conf_size,
-	    zc->zc_iflags, &tryconfig)) != 0)
+							zc->zc_iflags, &tryconfig)) != 0)
 		return (error);
 
 	config = spa_tryimport(tryconfig);
@@ -1702,7 +1773,8 @@ zfs_ioc_pool_freeze(zfs_cmd_t *zc)
 	int error;
 
 	error = spa_open(zc->zc_name, &spa, FTAG);
-	if (error == 0) {
+	if (error == 0)
+	{
 		spa_freeze(spa);
 		spa_close(spa, FTAG);
 	}
@@ -1719,7 +1791,8 @@ zfs_ioc_pool_upgrade(zfs_cmd_t *zc)
 		return (error);
 
 	if (zc->zc_cookie < spa_version(spa) ||
-	    !SPA_VERSION_IS_SUPPORTED(zc->zc_cookie)) {
+		!SPA_VERSION_IS_SUPPORTED(zc->zc_cookie))
+	{
 		spa_close(spa, FTAG);
 		return (SET_ERROR(EINVAL));
 	}
@@ -1744,17 +1817,19 @@ zfs_ioc_pool_get_history(zfs_cmd_t *zc)
 	if ((error = spa_open(zc->zc_name, &spa, FTAG)) != 0)
 		return (error);
 
-	if (spa_version(spa) < SPA_VERSION_ZPOOL_HISTORY) {
+	if (spa_version(spa) < SPA_VERSION_ZPOOL_HISTORY)
+	{
 		spa_close(spa, FTAG);
 		return (SET_ERROR(ENOTSUP));
 	}
 
 	hist_buf = vmem_alloc(size, KM_SLEEP);
 	if ((error = spa_history_get(spa, &zc->zc_history_offset,
-	    &zc->zc_history_len, hist_buf)) == 0) {
+								 &zc->zc_history_len, hist_buf)) == 0)
+	{
 		error = ddi_copyout(hist_buf,
-		    (void *)(uintptr_t)zc->zc_history,
-		    zc->zc_history_len, zc->zc_iflags);
+							(void *)(uintptr_t)zc->zc_history,
+							zc->zc_history_len, zc->zc_iflags);
 	}
 
 	spa_close(spa, FTAG);
@@ -1769,7 +1844,8 @@ zfs_ioc_pool_reguid(zfs_cmd_t *zc)
 	int error;
 
 	error = spa_open(zc->zc_name, &spa, FTAG);
-	if (error == 0) {
+	if (error == 0)
+	{
 		error = spa_change_guid(spa);
 		spa_close(spa, FTAG);
 	}
@@ -1798,14 +1874,15 @@ zfs_ioc_obj_to_path(zfs_cmd_t *zc)
 
 	/* XXX reading from objset not owned */
 	if ((error = dmu_objset_hold_flags(zc->zc_name, B_TRUE,
-	    FTAG, &os)) != 0)
+									   FTAG, &os)) != 0)
 		return (error);
-	if (dmu_objset_type(os) != DMU_OST_ZFS) {
+	if (dmu_objset_type(os) != DMU_OST_ZFS)
+	{
 		dmu_objset_rele_flags(os, B_TRUE, FTAG);
 		return (SET_ERROR(EINVAL));
 	}
 	error = zfs_obj_to_path(os, zc->zc_obj, zc->zc_value,
-	    sizeof (zc->zc_value));
+							sizeof(zc->zc_value));
 	dmu_objset_rele_flags(os, B_TRUE, FTAG);
 
 	return (error);
@@ -1828,14 +1905,15 @@ zfs_ioc_obj_to_stats(zfs_cmd_t *zc)
 
 	/* XXX reading from objset not owned */
 	if ((error = dmu_objset_hold_flags(zc->zc_name, B_TRUE,
-	    FTAG, &os)) != 0)
+									   FTAG, &os)) != 0)
 		return (error);
-	if (dmu_objset_type(os) != DMU_OST_ZFS) {
+	if (dmu_objset_type(os) != DMU_OST_ZFS)
+	{
 		dmu_objset_rele_flags(os, B_TRUE, FTAG);
 		return (SET_ERROR(EINVAL));
 	}
 	error = zfs_obj_to_stats(os, zc->zc_obj, &zc->zc_stat, zc->zc_value,
-	    sizeof (zc->zc_value));
+							 sizeof(zc->zc_value));
 	dmu_objset_rele_flags(os, B_TRUE, FTAG);
 
 	return (error);
@@ -1853,8 +1931,9 @@ zfs_ioc_vdev_add(zfs_cmd_t *zc)
 		return (error);
 
 	error = get_nvlist(zc->zc_nvlist_conf, zc->zc_nvlist_conf_size,
-	    zc->zc_iflags, &config);
-	if (error == 0) {
+					   zc->zc_iflags, &config);
+	if (error == 0)
+	{
 		error = spa_vdev_add(spa, config);
 		nvlist_free(config);
 	}
@@ -1877,9 +1956,12 @@ zfs_ioc_vdev_remove(zfs_cmd_t *zc)
 	error = spa_open(zc->zc_name, &spa, FTAG);
 	if (error != 0)
 		return (error);
-	if (zc->zc_cookie != 0) {
+	if (zc->zc_cookie != 0)
+	{
 		error = spa_vdev_remove_cancel(spa);
-	} else {
+	}
+	else
+	{
 		error = spa_vdev_remove(spa, zc->zc_guid, B_FALSE);
 	}
 	spa_close(spa, FTAG);
@@ -1895,7 +1977,8 @@ zfs_ioc_vdev_set_state(zfs_cmd_t *zc)
 
 	if ((error = spa_open(zc->zc_name, &spa, FTAG)) != 0)
 		return (error);
-	switch (zc->zc_cookie) {
+	switch (zc->zc_cookie)
+	{
 	case VDEV_STATE_ONLINE:
 		error = vdev_online(spa, zc->zc_guid, zc->zc_obj, &newstate);
 		break;
@@ -1906,8 +1989,8 @@ zfs_ioc_vdev_set_state(zfs_cmd_t *zc)
 
 	case VDEV_STATE_FAULTED:
 		if (zc->zc_obj != VDEV_AUX_ERR_EXCEEDED &&
-		    zc->zc_obj != VDEV_AUX_EXTERNAL &&
-		    zc->zc_obj != VDEV_AUX_EXTERNAL_PERSIST)
+			zc->zc_obj != VDEV_AUX_EXTERNAL &&
+			zc->zc_obj != VDEV_AUX_EXTERNAL_PERSIST)
 			zc->zc_obj = VDEV_AUX_ERR_EXCEEDED;
 
 		error = vdev_fault(spa, zc->zc_guid, zc->zc_obj);
@@ -1915,7 +1998,7 @@ zfs_ioc_vdev_set_state(zfs_cmd_t *zc)
 
 	case VDEV_STATE_DEGRADED:
 		if (zc->zc_obj != VDEV_AUX_ERR_EXCEEDED &&
-		    zc->zc_obj != VDEV_AUX_EXTERNAL)
+			zc->zc_obj != VDEV_AUX_EXTERNAL)
 			zc->zc_obj = VDEV_AUX_ERR_EXCEEDED;
 
 		error = vdev_degrade(spa, zc->zc_guid, zc->zc_obj);
@@ -1946,9 +2029,10 @@ zfs_ioc_vdev_attach(zfs_cmd_t *zc)
 		return (error);
 
 	if ((error = get_nvlist(zc->zc_nvlist_conf, zc->zc_nvlist_conf_size,
-	    zc->zc_iflags, &config)) == 0) {
+							zc->zc_iflags, &config)) == 0)
+	{
 		error = spa_vdev_attach(spa, zc->zc_guid, config, replacing,
-		    rebuild);
+								rebuild);
 		nvlist_free(config);
 	}
 
@@ -1983,14 +2067,16 @@ zfs_ioc_vdev_split(zfs_cmd_t *zc)
 		return (error);
 
 	if ((error = get_nvlist(zc->zc_nvlist_conf, zc->zc_nvlist_conf_size,
-	    zc->zc_iflags, &config))) {
+							zc->zc_iflags, &config)))
+	{
 		spa_close(spa, FTAG);
 		return (error);
 	}
 
 	if (zc->zc_nvlist_src_size != 0 && (error =
-	    get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-	    zc->zc_iflags, &props))) {
+											get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
+													   zc->zc_iflags, &props)))
+	{
 		spa_close(spa, FTAG);
 		nvlist_free(config);
 		return (error);
@@ -2049,7 +2135,8 @@ zfs_ioc_objset_stats_impl(zfs_cmd_t *zc, objset_t *os)
 	dmu_objset_fast_stat(os, &zc->zc_objset_stats);
 
 	if (zc->zc_nvlist_dst != 0 &&
-	    (error = dsl_prop_get_all(os, &nv)) == 0) {
+		(error = dsl_prop_get_all(os, &nv)) == 0)
+	{
 		dmu_objset_stats(os, nv);
 		/*
 		 * NB: zvol_get_stats() will read the objset contents,
@@ -2059,9 +2146,11 @@ zfs_ioc_objset_stats_impl(zfs_cmd_t *zc, objset_t *os)
 		 * XXX reading without owning
 		 */
 		if (!zc->zc_objset_stats.dds_inconsistent &&
-		    dmu_objset_type(os) == DMU_OST_ZVOL) {
+			dmu_objset_type(os) == DMU_OST_ZVOL)
+		{
 			error = zvol_get_stats(os, nv);
-			if (error == EIO) {
+			if (error == EIO)
+			{
 				nvlist_free(nv);
 				return (error);
 			}
@@ -2092,7 +2181,8 @@ zfs_ioc_objset_stats(zfs_cmd_t *zc)
 	int error;
 
 	error = dmu_objset_hold(zc->zc_name, FTAG, &os);
-	if (error == 0) {
+	if (error == 0)
+	{
 		error = zfs_ioc_objset_stats_impl(zc, os);
 		dmu_objset_rele(os, FTAG);
 	}
@@ -2128,7 +2218,8 @@ zfs_ioc_objset_recvd_props(zfs_cmd_t *zc)
 		return (SET_ERROR(ENOTSUP));
 
 	if (zc->zc_nvlist_dst != 0 &&
-	    (error = dsl_prop_get_received(zc->zc_name, &nv)) == 0) {
+		(error = dsl_prop_get_received(zc->zc_name, &nv)) == 0)
+	{
 		error = put_nvlist(zc, nv);
 		nvlist_free(nv);
 	}
@@ -2179,18 +2270,21 @@ zfs_ioc_objset_zplprops(zfs_cmd_t *zc)
 	 * hold, because it could be inconsistent.
 	 */
 	if (zc->zc_nvlist_dst != 0 &&
-	    !zc->zc_objset_stats.dds_inconsistent &&
-	    dmu_objset_type(os) == DMU_OST_ZFS) {
+		!zc->zc_objset_stats.dds_inconsistent &&
+		dmu_objset_type(os) == DMU_OST_ZFS)
+	{
 		nvlist_t *nv;
 
 		VERIFY(nvlist_alloc(&nv, NV_UNIQUE_NAME, KM_SLEEP) == 0);
 		if ((err = nvl_add_zplprop(os, nv, ZFS_PROP_VERSION)) == 0 &&
-		    (err = nvl_add_zplprop(os, nv, ZFS_PROP_NORMALIZE)) == 0 &&
-		    (err = nvl_add_zplprop(os, nv, ZFS_PROP_UTF8ONLY)) == 0 &&
-		    (err = nvl_add_zplprop(os, nv, ZFS_PROP_CASE)) == 0)
+			(err = nvl_add_zplprop(os, nv, ZFS_PROP_NORMALIZE)) == 0 &&
+			(err = nvl_add_zplprop(os, nv, ZFS_PROP_UTF8ONLY)) == 0 &&
+			(err = nvl_add_zplprop(os, nv, ZFS_PROP_CASE)) == 0)
 			err = put_nvlist(zc, nv);
 		nvlist_free(nv);
-	} else {
+	}
+	else
+	{
 		err = SET_ERROR(ENOENT);
 	}
 	dmu_objset_rele(os, FTAG);
@@ -2219,7 +2313,8 @@ zfs_ioc_dataset_list_next(zfs_cmd_t *zc)
 	size_t orig_len = strlen(zc->zc_name);
 
 top:
-	if ((error = dmu_objset_hold(zc->zc_name, FTAG, &os))) {
+	if ((error = dmu_objset_hold(zc->zc_name, FTAG, &os)))
+	{
 		if (error == ENOENT)
 			error = SET_ERROR(ESRCH);
 		return (error);
@@ -2227,13 +2322,14 @@ top:
 
 	p = strrchr(zc->zc_name, '/');
 	if (p == NULL || p[1] != '\0')
-		(void) strlcat(zc->zc_name, "/", sizeof (zc->zc_name));
+		(void)strlcat(zc->zc_name, "/", sizeof(zc->zc_name));
 	p = zc->zc_name + strlen(zc->zc_name);
 
-	do {
+	do
+	{
 		error = dmu_dir_list_next(os,
-		    sizeof (zc->zc_name) - (p - zc->zc_name), p,
-		    NULL, &zc->zc_cookie);
+								  sizeof(zc->zc_name) - (p - zc->zc_name), p,
+								  NULL, &zc->zc_cookie);
 		if (error == ENOENT)
 			error = SET_ERROR(ESRCH);
 	} while (error == 0 && zfs_dataset_name_hidden(zc->zc_name));
@@ -2243,9 +2339,11 @@ top:
 	 * If it's an internal dataset (ie. with a '$' in its name),
 	 * don't try to get stats for it, otherwise we'll return ENOENT.
 	 */
-	if (error == 0 && strchr(zc->zc_name, '$') == NULL) {
+	if (error == 0 && strchr(zc->zc_name, '$') == NULL)
+	{
 		error = zfs_ioc_objset_stats(zc); /* fill in the stats */
-		if (error == ENOENT) {
+		if (error == ENOENT)
+		{
 			/* We lost a race with destroy, get the next one. */
 			zc->zc_name[orig_len] = '\0';
 			goto top;
@@ -2275,21 +2373,23 @@ zfs_ioc_snapshot_list_next(zfs_cmd_t *zc)
 	dsl_dataset_t *ds;
 	uint64_t min_txg = 0, max_txg = 0;
 
-	if (zc->zc_nvlist_src_size != 0) {
+	if (zc->zc_nvlist_src_size != 0)
+	{
 		nvlist_t *props = NULL;
 		error = get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-		    zc->zc_iflags, &props);
+						   zc->zc_iflags, &props);
 		if (error != 0)
 			return (error);
-		(void) nvlist_lookup_uint64(props, SNAP_ITER_MIN_TXG,
-		    &min_txg);
-		(void) nvlist_lookup_uint64(props, SNAP_ITER_MAX_TXG,
-		    &max_txg);
+		(void)nvlist_lookup_uint64(props, SNAP_ITER_MIN_TXG,
+								   &min_txg);
+		(void)nvlist_lookup_uint64(props, SNAP_ITER_MAX_TXG,
+								   &max_txg);
 		nvlist_free(props);
 	}
 
 	error = dmu_objset_hold(zc->zc_name, FTAG, &os);
-	if (error != 0) {
+	if (error != 0)
+	{
 		return (error == ENOENT ? SET_ERROR(ESRCH) : error);
 	}
 
@@ -2297,36 +2397,43 @@ zfs_ioc_snapshot_list_next(zfs_cmd_t *zc)
 	 * A dataset name of maximum length cannot have any snapshots,
 	 * so exit immediately.
 	 */
-	if (strlcat(zc->zc_name, "@", sizeof (zc->zc_name)) >=
-	    ZFS_MAX_DATASET_NAME_LEN) {
+	if (strlcat(zc->zc_name, "@", sizeof(zc->zc_name)) >=
+		ZFS_MAX_DATASET_NAME_LEN)
+	{
 		dmu_objset_rele(os, FTAG);
 		return (SET_ERROR(ESRCH));
 	}
 
-	while (error == 0) {
-		if (issig(JUSTLOOKING) && issig(FORREAL)) {
+	while (error == 0)
+	{
+		if (issig(JUSTLOOKING) && issig(FORREAL))
+		{
 			error = SET_ERROR(EINTR);
 			break;
 		}
 
 		error = dmu_snapshot_list_next(os,
-		    sizeof (zc->zc_name) - strlen(zc->zc_name),
-		    zc->zc_name + strlen(zc->zc_name), &zc->zc_obj,
-		    &zc->zc_cookie, NULL);
-		if (error == ENOENT) {
+									   sizeof(zc->zc_name) - strlen(zc->zc_name),
+									   zc->zc_name + strlen(zc->zc_name), &zc->zc_obj,
+									   &zc->zc_cookie, NULL);
+		if (error == ENOENT)
+		{
 			error = SET_ERROR(ESRCH);
 			break;
-		} else if (error != 0) {
+		}
+		else if (error != 0)
+		{
 			break;
 		}
 
 		error = dsl_dataset_hold_obj(dmu_objset_pool(os), zc->zc_obj,
-		    FTAG, &ds);
+									 FTAG, &ds);
 		if (error != 0)
 			break;
 
 		if ((min_txg != 0 && dsl_get_creationtxg(ds) < min_txg) ||
-		    (max_txg != 0 && dsl_get_creationtxg(ds) > max_txg)) {
+			(max_txg != 0 && dsl_get_creationtxg(ds) > max_txg))
+		{
 			dsl_dataset_rele(ds, FTAG);
 			/* undo snapshot name append */
 			*(strchr(zc->zc_name, '@') + 1) = '\0';
@@ -2334,16 +2441,19 @@ zfs_ioc_snapshot_list_next(zfs_cmd_t *zc)
 			continue;
 		}
 
-		if (zc->zc_simple) {
+		if (zc->zc_simple)
+		{
 			dsl_dataset_rele(ds, FTAG);
 			break;
 		}
 
-		if ((error = dmu_objset_from_ds(ds, &ossnap)) != 0) {
+		if ((error = dmu_objset_from_ds(ds, &ossnap)) != 0)
+		{
 			dsl_dataset_rele(ds, FTAG);
 			break;
 		}
-		if ((error = zfs_ioc_objset_stats_impl(zc, ossnap)) != 0) {
+		if ((error = zfs_ioc_objset_stats_impl(zc, ossnap)) != 0)
+		{
 			dsl_dataset_rele(ds, FTAG);
 			break;
 		}
@@ -2371,11 +2481,12 @@ zfs_prop_set_userquota(const char *dsname, nvpair_t *pair)
 	zfsvfs_t *zfsvfs;
 	int err;
 
-	if (nvpair_type(pair) == DATA_TYPE_NVLIST) {
+	if (nvpair_type(pair) == DATA_TYPE_NVLIST)
+	{
 		nvlist_t *attrs;
 		VERIFY(nvpair_value_nvlist(pair, &attrs) == 0);
 		if (nvlist_lookup_nvpair(attrs, ZPROP_VALUE,
-		    &pair) != 0)
+								 &pair) != 0)
 			return (SET_ERROR(EINVAL));
 	}
 
@@ -2384,8 +2495,8 @@ zfs_prop_set_userquota(const char *dsname, nvpair_t *pair)
 	 * userquota@<rid>-<domain>.
 	 */
 	if ((dash = strchr(propname, '-')) == NULL ||
-	    nvpair_value_uint64_array(pair, &valary, &vallen) != 0 ||
-	    vallen != 3)
+		nvpair_value_uint64_array(pair, &valary, &vallen) != 0 ||
+		vallen != 3)
 		return (SET_ERROR(EINVAL));
 
 	domain = dash + 1;
@@ -2394,7 +2505,8 @@ zfs_prop_set_userquota(const char *dsname, nvpair_t *pair)
 	quota = valary[2];
 
 	err = zfsvfs_hold(dsname, FTAG, &zfsvfs, B_FALSE);
-	if (err == 0) {
+	if (err == 0)
+	{
 		err = zfs_set_userquota(zfsvfs, type, domain, rid, quota);
 		zfsvfs_rele(zfsvfs, FTAG);
 	}
@@ -2412,7 +2524,7 @@ zfs_prop_set_userquota(const char *dsname, nvpair_t *pair)
  */
 static int
 zfs_prop_set_special(const char *dsname, zprop_source_t source,
-    nvpair_t *pair)
+					 nvpair_t *pair)
 {
 	const char *propname = nvpair_name(pair);
 	zfs_prop_t prop = zfs_name_to_prop(propname);
@@ -2420,27 +2532,33 @@ zfs_prop_set_special(const char *dsname, zprop_source_t source,
 	const char *strval = NULL;
 	int err = -1;
 
-	if (prop == ZPROP_INVAL) {
+	if (prop == ZPROP_INVAL)
+	{
 		if (zfs_prop_userquota(propname))
 			return (zfs_prop_set_userquota(dsname, pair));
 		return (-1);
 	}
 
-	if (nvpair_type(pair) == DATA_TYPE_NVLIST) {
+	if (nvpair_type(pair) == DATA_TYPE_NVLIST)
+	{
 		nvlist_t *attrs;
 		VERIFY(nvpair_value_nvlist(pair, &attrs) == 0);
 		VERIFY(nvlist_lookup_nvpair(attrs, ZPROP_VALUE,
-		    &pair) == 0);
+									&pair) == 0);
 	}
 
 	/* all special properties are numeric except for keylocation */
-	if (zfs_prop_get_type(prop) == PROP_TYPE_STRING) {
+	if (zfs_prop_get_type(prop) == PROP_TYPE_STRING)
+	{
 		strval = fnvpair_value_string(pair);
-	} else {
+	}
+	else
+	{
 		intval = fnvpair_value_uint64(pair);
 	}
 
-	switch (prop) {
+	switch (prop)
+	{
 	case ZFS_PROP_QUOTA:
 		err = dsl_dir_set_quota(dsname, source, intval);
 		break;
@@ -2449,10 +2567,13 @@ zfs_prop_set_special(const char *dsname, zprop_source_t source,
 		break;
 	case ZFS_PROP_FILESYSTEM_LIMIT:
 	case ZFS_PROP_SNAPSHOT_LIMIT:
-		if (intval == UINT64_MAX) {
+		if (intval == UINT64_MAX)
+		{
 			/* clearing the limit, just do it */
 			err = 0;
-		} else {
+		}
+		else
+		{
 			err = dsl_dir_activate_fs_ss_limit(dsname);
 		}
 		/*
@@ -2506,15 +2627,16 @@ zfs_prop_set_special(const char *dsname, zprop_source_t source,
 		err = zfs_set_version(zfsvfs, intval);
 		zfsvfs_rele(zfsvfs, FTAG);
 
-		if (err == 0 && intval >= ZPL_VERSION_USERSPACE) {
+		if (err == 0 && intval >= ZPL_VERSION_USERSPACE)
+		{
 			zfs_cmd_t *zc;
 
-			zc = kmem_zalloc(sizeof (zfs_cmd_t), KM_SLEEP);
-			(void) strlcpy(zc->zc_name, dsname,
-			    sizeof (zc->zc_name));
-			(void) zfs_ioc_userspace_upgrade(zc);
-			(void) zfs_ioc_id_quota_upgrade(zc);
-			kmem_free(zc, sizeof (zfs_cmd_t));
+			zc = kmem_zalloc(sizeof(zfs_cmd_t), KM_SLEEP);
+			(void)strlcpy(zc->zc_name, dsname,
+						  sizeof(zc->zc_name));
+			(void)zfs_ioc_userspace_upgrade(zc);
+			(void)zfs_ioc_id_quota_upgrade(zc);
+			kmem_free(zc, sizeof(zfs_cmd_t));
 		}
 		break;
 	}
@@ -2528,7 +2650,8 @@ zfs_prop_set_special(const char *dsname, zprop_source_t source,
 static boolean_t
 zfs_is_namespace_prop(zfs_prop_t prop)
 {
-	switch (prop) {
+	switch (prop)
+	{
 
 	case ZFS_PROP_ATIME:
 	case ZFS_PROP_RELATIME:
@@ -2555,9 +2678,8 @@ zfs_is_namespace_prop(zfs_prop_t prop)
  * If every property is set successfully, zero is returned and errlist is not
  * modified.
  */
-int
-zfs_set_prop_nvlist(const char *dsname, zprop_source_t source, nvlist_t *nvl,
-    nvlist_t *errlist)
+int zfs_set_prop_nvlist(const char *dsname, zprop_source_t source, nvlist_t *nvl,
+						nvlist_t *errlist)
 {
 	nvpair_t *pair;
 	nvpair_t *propval;
@@ -2570,47 +2692,63 @@ zfs_set_prop_nvlist(const char *dsname, zprop_source_t source, nvlist_t *nvl,
 	nvlist_t *retrynvl = fnvlist_alloc();
 retry:
 	pair = NULL;
-	while ((pair = nvlist_next_nvpair(nvl, pair)) != NULL) {
+	while ((pair = nvlist_next_nvpair(nvl, pair)) != NULL)
+	{
 		const char *propname = nvpair_name(pair);
 		zfs_prop_t prop = zfs_name_to_prop(propname);
 		int err = 0;
 
 		/* decode the property value */
 		propval = pair;
-		if (nvpair_type(pair) == DATA_TYPE_NVLIST) {
+		if (nvpair_type(pair) == DATA_TYPE_NVLIST)
+		{
 			nvlist_t *attrs;
 			attrs = fnvpair_value_nvlist(pair);
 			if (nvlist_lookup_nvpair(attrs, ZPROP_VALUE,
-			    &propval) != 0)
+									 &propval) != 0)
 				err = SET_ERROR(EINVAL);
 		}
 
 		/* Validate value type */
-		if (err == 0 && source == ZPROP_SRC_INHERITED) {
+		if (err == 0 && source == ZPROP_SRC_INHERITED)
+		{
 			/* inherited properties are expected to be booleans */
 			if (nvpair_type(propval) != DATA_TYPE_BOOLEAN)
 				err = SET_ERROR(EINVAL);
-		} else if (err == 0 && prop == ZPROP_INVAL) {
-			if (zfs_prop_user(propname)) {
+		}
+		else if (err == 0 && prop == ZPROP_INVAL)
+		{
+			if (zfs_prop_user(propname))
+			{
 				if (nvpair_type(propval) != DATA_TYPE_STRING)
 					err = SET_ERROR(EINVAL);
-			} else if (zfs_prop_userquota(propname)) {
+			}
+			else if (zfs_prop_userquota(propname))
+			{
 				if (nvpair_type(propval) !=
-				    DATA_TYPE_UINT64_ARRAY)
+					DATA_TYPE_UINT64_ARRAY)
 					err = SET_ERROR(EINVAL);
-			} else {
+			}
+			else
+			{
 				err = SET_ERROR(EINVAL);
 			}
-		} else if (err == 0) {
-			if (nvpair_type(propval) == DATA_TYPE_STRING) {
+		}
+		else if (err == 0)
+		{
+			if (nvpair_type(propval) == DATA_TYPE_STRING)
+			{
 				if (zfs_prop_get_type(prop) != PROP_TYPE_STRING)
 					err = SET_ERROR(EINVAL);
-			} else if (nvpair_type(propval) == DATA_TYPE_UINT64) {
+			}
+			else if (nvpair_type(propval) == DATA_TYPE_UINT64)
+			{
 				const char *unused;
 
 				intval = fnvpair_value_uint64(propval);
 
-				switch (zfs_prop_get_type(prop)) {
+				switch (zfs_prop_get_type(prop))
+				{
 				case PROP_TYPE_NUMBER:
 					break;
 				case PROP_TYPE_STRING:
@@ -2618,15 +2756,17 @@ retry:
 					break;
 				case PROP_TYPE_INDEX:
 					if (zfs_prop_index_to_string(prop,
-					    intval, &unused) != 0)
+												 intval, &unused) != 0)
 						err =
-						    SET_ERROR(ZFS_ERR_BADPROP);
+							SET_ERROR(ZFS_ERR_BADPROP);
 					break;
 				default:
 					cmn_err(CE_PANIC,
-					    "unknown property type");
+							"unknown property type");
 				}
-			} else {
+			}
+			else
+			{
 				err = SET_ERROR(EINVAL);
 			}
 		}
@@ -2635,19 +2775,23 @@ retry:
 		if (err == 0)
 			err = zfs_check_settable(dsname, pair, CRED());
 
-		if (err == 0) {
+		if (err == 0)
+		{
 			if (source == ZPROP_SRC_INHERITED)
 				err = -1; /* does not need special handling */
 			else
 				err = zfs_prop_set_special(dsname, source,
-				    pair);
-			if (err == -1) {
+										   pair);
+			if (err == -1)
+			{
 				/*
 				 * For better performance we build up a list of
 				 * properties to set in a single transaction.
 				 */
 				err = nvlist_add_nvpair(genericnvl, pair);
-			} else if (err != 0 && nvl != retrynvl) {
+			}
+			else if (err != 0 && nvl != retrynvl)
+			{
 				/*
 				 * This may be a spurious error caused by
 				 * receiving quota and reservation out of order.
@@ -2657,7 +2801,8 @@ retry:
 			}
 		}
 
-		if (err != 0) {
+		if (err != 0)
+		{
 			if (errlist != NULL)
 				fnvlist_add_int32(errlist, propname, err);
 			rv = err;
@@ -2667,47 +2812,58 @@ retry:
 			should_update_mount_cache = B_TRUE;
 	}
 
-	if (nvl != retrynvl && !nvlist_empty(retrynvl)) {
+	if (nvl != retrynvl && !nvlist_empty(retrynvl))
+	{
 		nvl = retrynvl;
 		goto retry;
 	}
 
 	if (!nvlist_empty(genericnvl) &&
-	    dsl_props_set(dsname, source, genericnvl) != 0) {
+		dsl_props_set(dsname, source, genericnvl) != 0)
+	{
 		/*
 		 * If this fails, we still want to set as many properties as we
 		 * can, so try setting them individually.
 		 */
 		pair = NULL;
-		while ((pair = nvlist_next_nvpair(genericnvl, pair)) != NULL) {
+		while ((pair = nvlist_next_nvpair(genericnvl, pair)) != NULL)
+		{
 			const char *propname = nvpair_name(pair);
 			int err = 0;
 
 			propval = pair;
-			if (nvpair_type(pair) == DATA_TYPE_NVLIST) {
+			if (nvpair_type(pair) == DATA_TYPE_NVLIST)
+			{
 				nvlist_t *attrs;
 				attrs = fnvpair_value_nvlist(pair);
 				propval = fnvlist_lookup_nvpair(attrs,
-				    ZPROP_VALUE);
+												ZPROP_VALUE);
 			}
 
-			if (nvpair_type(propval) == DATA_TYPE_STRING) {
+			if (nvpair_type(propval) == DATA_TYPE_STRING)
+			{
 				strval = fnvpair_value_string(propval);
 				err = dsl_prop_set_string(dsname, propname,
-				    source, strval);
-			} else if (nvpair_type(propval) == DATA_TYPE_BOOLEAN) {
+										  source, strval);
+			}
+			else if (nvpair_type(propval) == DATA_TYPE_BOOLEAN)
+			{
 				err = dsl_prop_inherit(dsname, propname,
-				    source);
-			} else {
+									   source);
+			}
+			else
+			{
 				intval = fnvpair_value_uint64(propval);
 				err = dsl_prop_set_int(dsname, propname, source,
-				    intval);
+									   intval);
 			}
 
-			if (err != 0) {
-				if (errlist != NULL) {
+			if (err != 0)
+			{
+				if (errlist != NULL)
+				{
 					fnvlist_add_int32(errlist, propname,
-					    err);
+									  err);
 				}
 				rv = err;
 			}
@@ -2730,11 +2886,12 @@ zfs_check_userprops(nvlist_t *nvl)
 {
 	nvpair_t *pair = NULL;
 
-	while ((pair = nvlist_next_nvpair(nvl, pair)) != NULL) {
+	while ((pair = nvlist_next_nvpair(nvl, pair)) != NULL)
+	{
 		const char *propname = nvpair_name(pair);
 
 		if (!zfs_prop_user(propname) ||
-		    nvpair_type(pair) != DATA_TYPE_STRING)
+			nvpair_type(pair) != DATA_TYPE_STRING)
 			return (SET_ERROR(EINVAL));
 
 		if (strlen(propname) >= ZAP_MAXNAMELEN)
@@ -2754,7 +2911,8 @@ props_skip(nvlist_t *props, nvlist_t *skipped, nvlist_t **newprops)
 	VERIFY(nvlist_alloc(newprops, NV_UNIQUE_NAME, KM_SLEEP) == 0);
 
 	pair = NULL;
-	while ((pair = nvlist_next_nvpair(props, pair)) != NULL) {
+	while ((pair = nvlist_next_nvpair(props, pair)) != NULL)
+	{
 		if (nvlist_exists(skipped, nvpair_name(pair)))
 			continue;
 
@@ -2764,18 +2922,19 @@ props_skip(nvlist_t *props, nvlist_t *skipped, nvlist_t **newprops)
 
 static int
 clear_received_props(const char *dsname, nvlist_t *props,
-    nvlist_t *skipped)
+					 nvlist_t *skipped)
 {
 	int err = 0;
 	nvlist_t *cleared_props = NULL;
 	props_skip(props, skipped, &cleared_props);
-	if (!nvlist_empty(cleared_props)) {
+	if (!nvlist_empty(cleared_props))
+	{
 		/*
 		 * Acts on local properties until the dataset has received
 		 * properties at least once on or after SPA_VERSION_RECVD_PROPS.
 		 */
 		zprop_source_t flags = (ZPROP_SRC_NONE |
-		    (dsl_prop_get_hasrecvd(dsname) ? ZPROP_SRC_RECEIVED : 0));
+								(dsl_prop_get_hasrecvd(dsname) ? ZPROP_SRC_RECEIVED : 0));
 		err = zfs_set_prop_nvlist(dsname, flags, cleared_props, NULL);
 	}
 	nvlist_free(cleared_props);
@@ -2797,21 +2956,22 @@ zfs_ioc_set_prop(zfs_cmd_t *zc)
 {
 	nvlist_t *nvl;
 	boolean_t received = zc->zc_cookie;
-	zprop_source_t source = (received ? ZPROP_SRC_RECEIVED :
-	    ZPROP_SRC_LOCAL);
+	zprop_source_t source = (received ? ZPROP_SRC_RECEIVED : ZPROP_SRC_LOCAL);
 	nvlist_t *errors;
 	int error;
 
 	if ((error = get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-	    zc->zc_iflags, &nvl)) != 0)
+							zc->zc_iflags, &nvl)) != 0)
 		return (error);
 
-	if (received) {
+	if (received)
+	{
 		nvlist_t *origprops;
 
-		if (dsl_prop_get_received(zc->zc_name, &origprops) == 0) {
-			(void) clear_received_props(zc->zc_name,
-			    origprops, nvl);
+		if (dsl_prop_get_received(zc->zc_name, &origprops) == 0)
+		{
+			(void)clear_received_props(zc->zc_name,
+									   origprops, nvl);
 			nvlist_free(origprops);
 		}
 
@@ -2822,8 +2982,9 @@ zfs_ioc_set_prop(zfs_cmd_t *zc)
 	if (error == 0)
 		error = zfs_set_prop_nvlist(zc->zc_name, source, nvl, errors);
 
-	if (zc->zc_nvlist_dst != 0 && errors != NULL) {
-		(void) put_nvlist(zc, errors);
+	if (zc->zc_nvlist_dst != 0 && errors != NULL)
+	{
+		(void)put_nvlist(zc, errors);
 	}
 
 	nvlist_free(errors);
@@ -2846,14 +3007,15 @@ zfs_ioc_inherit_prop(zfs_cmd_t *zc)
 	zfs_prop_t prop = zfs_name_to_prop(propname);
 	boolean_t received = zc->zc_cookie;
 	zprop_source_t source = (received
-	    ? ZPROP_SRC_NONE		/* revert to received value, if any */
-	    : ZPROP_SRC_INHERITED);	/* explicitly inherit */
+								 ? ZPROP_SRC_NONE		 /* revert to received value, if any */
+								 : ZPROP_SRC_INHERITED); /* explicitly inherit */
 	nvlist_t *dummy;
 	nvpair_t *pair;
 	zprop_type_t type;
 	int err;
 
-	if (!received) {
+	if (!received)
+	{
 		/*
 		 * Only check this in the non-received case. We want to allow
 		 * 'inherit -S' to revert non-inheritable properties like quota
@@ -2864,14 +3026,19 @@ zfs_ioc_inherit_prop(zfs_cmd_t *zc)
 			return (SET_ERROR(EINVAL));
 	}
 
-	if (prop == ZPROP_INVAL) {
+	if (prop == ZPROP_INVAL)
+	{
 		if (!zfs_prop_user(propname))
 			return (SET_ERROR(EINVAL));
 
 		type = PROP_TYPE_STRING;
-	} else if (prop == ZFS_PROP_VOLSIZE || prop == ZFS_PROP_VERSION) {
+	}
+	else if (prop == ZFS_PROP_VOLSIZE || prop == ZFS_PROP_VERSION)
+	{
 		return (SET_ERROR(EINVAL));
-	} else {
+	}
+	else
+	{
 		type = zfs_prop_get_type(prop);
 	}
 
@@ -2881,7 +3048,8 @@ zfs_ioc_inherit_prop(zfs_cmd_t *zc)
 	 */
 	dummy = fnvlist_alloc();
 
-	switch (type) {
+	switch (type)
+	{
 	case PROP_TYPE_STRING:
 		VERIFY(0 == nvlist_add_string(dummy, propname, ""));
 		break;
@@ -2895,13 +3063,16 @@ zfs_ioc_inherit_prop(zfs_cmd_t *zc)
 	}
 
 	pair = nvlist_next_nvpair(dummy, NULL);
-	if (pair == NULL) {
+	if (pair == NULL)
+	{
 		err = SET_ERROR(EINVAL);
-	} else {
+	}
+	else
+	{
 		err = zfs_prop_set_special(zc->zc_name, source, pair);
 		if (err == -1) /* property is not "special", needs handling */
 			err = dsl_prop_inherit(zc->zc_name, zc->zc_value,
-			    source);
+								   source);
 	}
 
 errout:
@@ -2918,7 +3089,7 @@ zfs_ioc_pool_set_props(zfs_cmd_t *zc)
 	nvpair_t *pair;
 
 	if ((error = get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-	    zc->zc_iflags, &props)))
+							zc->zc_iflags, &props)))
 		return (error);
 
 	/*
@@ -2926,22 +3097,25 @@ zfs_ioc_pool_set_props(zfs_cmd_t *zc)
 	 * to handle the faulted case.
 	 */
 	pair = nvlist_next_nvpair(props, NULL);
-	if (pair != NULL && strcmp(nvpair_name(pair),
-	    zpool_prop_to_name(ZPOOL_PROP_CACHEFILE)) == 0 &&
-	    nvlist_next_nvpair(props, pair) == NULL) {
+	if (pair != NULL && strcmp(nvpair_name(pair), zpool_prop_to_name(ZPOOL_PROP_CACHEFILE)) == 0 &&
+		nvlist_next_nvpair(props, pair) == NULL)
+	{
 		mutex_enter(&spa_namespace_lock);
-		if ((spa = spa_lookup(zc->zc_name)) != NULL) {
+		if ((spa = spa_lookup(zc->zc_name)) != NULL)
+		{
 			spa_configfile_set(spa, props, B_FALSE);
 			spa_write_cachefile(spa, B_FALSE, B_TRUE, B_FALSE);
 		}
 		mutex_exit(&spa_namespace_lock);
-		if (spa != NULL) {
+		if (spa != NULL)
+		{
 			nvlist_free(props);
 			return (0);
 		}
 	}
 
-	if ((error = spa_open(zc->zc_name, &spa, FTAG)) != 0) {
+	if ((error = spa_open(zc->zc_name, &spa, FTAG)) != 0)
+	{
 		nvlist_free(props);
 		return (error);
 	}
@@ -2961,7 +3135,8 @@ zfs_ioc_pool_get_props(zfs_cmd_t *zc)
 	int error;
 	nvlist_t *nvp = NULL;
 
-	if ((error = spa_open(zc->zc_name, &spa, FTAG)) != 0) {
+	if ((error = spa_open(zc->zc_name, &spa, FTAG)) != 0)
+	{
 		/*
 		 * If the pool is faulted, there may be properties we can still
 		 * get (such as altroot and cachefile), so attempt to get them
@@ -2971,7 +3146,9 @@ zfs_ioc_pool_get_props(zfs_cmd_t *zc)
 		if ((spa = spa_lookup(zc->zc_name)) != NULL)
 			error = spa_prop_get(spa, &nvp);
 		mutex_exit(&spa_namespace_lock);
-	} else {
+	}
+	else
+	{
 		error = spa_prop_get(spa, &nvp);
 		spa_close(spa, FTAG);
 	}
@@ -3000,13 +3177,14 @@ zfs_ioc_set_fsacl(zfs_cmd_t *zc)
 	nvlist_t *fsaclnv = NULL;
 
 	if ((error = get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-	    zc->zc_iflags, &fsaclnv)) != 0)
+							zc->zc_iflags, &fsaclnv)) != 0)
 		return (error);
 
 	/*
 	 * Verify nvlist is constructed correctly
 	 */
-	if ((error = zfs_deleg_verify_nvlist(fsaclnv)) != 0) {
+	if ((error = zfs_deleg_verify_nvlist(fsaclnv)) != 0)
+	{
 		nvlist_free(fsaclnv);
 		return (SET_ERROR(EINVAL));
 	}
@@ -3018,13 +3196,17 @@ zfs_ioc_set_fsacl(zfs_cmd_t *zc)
 	 */
 
 	error = secpolicy_zfs(CRED());
-	if (error != 0) {
-		if (zc->zc_perm_action == B_FALSE) {
+	if (error != 0)
+	{
+		if (zc->zc_perm_action == B_FALSE)
+		{
 			error = dsl_deleg_can_allow(zc->zc_name,
-			    fsaclnv, CRED());
-		} else {
+										fsaclnv, CRED());
+		}
+		else
+		{
 			error = dsl_deleg_can_unallow(zc->zc_name,
-			    fsaclnv, CRED());
+										  fsaclnv, CRED());
 		}
 	}
 
@@ -3048,7 +3230,8 @@ zfs_ioc_get_fsacl(zfs_cmd_t *zc)
 	nvlist_t *nvp;
 	int error;
 
-	if ((error = dsl_deleg_get(zc->zc_name, &nvp)) == 0) {
+	if ((error = dsl_deleg_get(zc->zc_name, &nvp)) == 0)
+	{
 		error = put_nvlist(zc, nvp);
 		nvlist_free(nvp);
 	}
@@ -3065,7 +3248,7 @@ zfs_create_cb(objset_t *os, void *arg, cred_t *cr, dmu_tx_t *tx)
 	zfs_create_fs(os, cr, zct->zct_zplprops, tx);
 }
 
-#define	ZFS_PROP_UNDEFINED	((uint64_t)-1)
+#define ZFS_PROP_UNDEFINED ((uint64_t) - 1)
 
 /*
  * inputs:
@@ -3089,8 +3272,8 @@ zfs_create_cb(objset_t *os, void *arg, cred_t *cr, dmu_tx_t *tx)
  */
 static int
 zfs_fill_zplprops_impl(objset_t *os, uint64_t zplver,
-    boolean_t fuids_ok, boolean_t sa_ok, nvlist_t *createprops,
-    nvlist_t *zplprops, boolean_t *is_ci)
+					   boolean_t fuids_ok, boolean_t sa_ok, nvlist_t *createprops,
+					   nvlist_t *zplprops, boolean_t *is_ci)
 {
 	uint64_t sense = ZFS_PROP_UNDEFINED;
 	uint64_t norm = ZFS_PROP_UNDEFINED;
@@ -3106,21 +3289,22 @@ zfs_fill_zplprops_impl(objset_t *os, uint64_t zplver,
 	/*
 	 * Pull out creator prop choices, if any.
 	 */
-	if (createprops) {
-		(void) nvlist_lookup_uint64(createprops,
-		    zfs_prop_to_name(ZFS_PROP_VERSION), &zplver);
-		(void) nvlist_lookup_uint64(createprops,
-		    zfs_prop_to_name(ZFS_PROP_NORMALIZE), &norm);
-		(void) nvlist_remove_all(createprops,
-		    zfs_prop_to_name(ZFS_PROP_NORMALIZE));
-		(void) nvlist_lookup_uint64(createprops,
-		    zfs_prop_to_name(ZFS_PROP_UTF8ONLY), &u8);
-		(void) nvlist_remove_all(createprops,
-		    zfs_prop_to_name(ZFS_PROP_UTF8ONLY));
-		(void) nvlist_lookup_uint64(createprops,
-		    zfs_prop_to_name(ZFS_PROP_CASE), &sense);
-		(void) nvlist_remove_all(createprops,
-		    zfs_prop_to_name(ZFS_PROP_CASE));
+	if (createprops)
+	{
+		(void)nvlist_lookup_uint64(createprops,
+								   zfs_prop_to_name(ZFS_PROP_VERSION), &zplver);
+		(void)nvlist_lookup_uint64(createprops,
+								   zfs_prop_to_name(ZFS_PROP_NORMALIZE), &norm);
+		(void)nvlist_remove_all(createprops,
+								zfs_prop_to_name(ZFS_PROP_NORMALIZE));
+		(void)nvlist_lookup_uint64(createprops,
+								   zfs_prop_to_name(ZFS_PROP_UTF8ONLY), &u8);
+		(void)nvlist_remove_all(createprops,
+								zfs_prop_to_name(ZFS_PROP_UTF8ONLY));
+		(void)nvlist_lookup_uint64(createprops,
+								   zfs_prop_to_name(ZFS_PROP_CASE), &sense);
+		(void)nvlist_remove_all(createprops,
+								zfs_prop_to_name(ZFS_PROP_CASE));
 	}
 
 	/*
@@ -3130,24 +3314,24 @@ zfs_fill_zplprops_impl(objset_t *os, uint64_t zplver,
 	 * error out.
 	 */
 	if ((zplver < ZPL_VERSION_INITIAL || zplver > ZPL_VERSION) ||
-	    (zplver >= ZPL_VERSION_FUID && !fuids_ok) ||
-	    (zplver >= ZPL_VERSION_SA && !sa_ok) ||
-	    (zplver < ZPL_VERSION_NORMALIZATION &&
-	    (norm != ZFS_PROP_UNDEFINED || u8 != ZFS_PROP_UNDEFINED ||
-	    sense != ZFS_PROP_UNDEFINED)))
+		(zplver >= ZPL_VERSION_FUID && !fuids_ok) ||
+		(zplver >= ZPL_VERSION_SA && !sa_ok) ||
+		(zplver < ZPL_VERSION_NORMALIZATION &&
+		 (norm != ZFS_PROP_UNDEFINED || u8 != ZFS_PROP_UNDEFINED ||
+		  sense != ZFS_PROP_UNDEFINED)))
 		return (SET_ERROR(ENOTSUP));
 
 	/*
 	 * Put the version in the zplprops
 	 */
 	VERIFY(nvlist_add_uint64(zplprops,
-	    zfs_prop_to_name(ZFS_PROP_VERSION), zplver) == 0);
+							 zfs_prop_to_name(ZFS_PROP_VERSION), zplver) == 0);
 
 	if (norm == ZFS_PROP_UNDEFINED &&
-	    (error = zfs_get_zplprop(os, ZFS_PROP_NORMALIZE, &norm)) != 0)
+		(error = zfs_get_zplprop(os, ZFS_PROP_NORMALIZE, &norm)) != 0)
 		return (error);
 	VERIFY(nvlist_add_uint64(zplprops,
-	    zfs_prop_to_name(ZFS_PROP_NORMALIZE), norm) == 0);
+							 zfs_prop_to_name(ZFS_PROP_NORMALIZE), norm) == 0);
 
 	/*
 	 * If we're normalizing, names must always be valid UTF-8 strings.
@@ -3155,16 +3339,16 @@ zfs_fill_zplprops_impl(objset_t *os, uint64_t zplver,
 	if (norm)
 		u8 = 1;
 	if (u8 == ZFS_PROP_UNDEFINED &&
-	    (error = zfs_get_zplprop(os, ZFS_PROP_UTF8ONLY, &u8)) != 0)
+		(error = zfs_get_zplprop(os, ZFS_PROP_UTF8ONLY, &u8)) != 0)
 		return (error);
 	VERIFY(nvlist_add_uint64(zplprops,
-	    zfs_prop_to_name(ZFS_PROP_UTF8ONLY), u8) == 0);
+							 zfs_prop_to_name(ZFS_PROP_UTF8ONLY), u8) == 0);
 
 	if (sense == ZFS_PROP_UNDEFINED &&
-	    (error = zfs_get_zplprop(os, ZFS_PROP_CASE, &sense)) != 0)
+		(error = zfs_get_zplprop(os, ZFS_PROP_CASE, &sense)) != 0)
 		return (error);
 	VERIFY(nvlist_add_uint64(zplprops,
-	    zfs_prop_to_name(ZFS_PROP_CASE), sense) == 0);
+							 zfs_prop_to_name(ZFS_PROP_CASE), sense) == 0);
 
 	if (is_ci)
 		*is_ci = (sense == ZFS_CASE_INSENSITIVE);
@@ -3174,7 +3358,7 @@ zfs_fill_zplprops_impl(objset_t *os, uint64_t zplver,
 
 static int
 zfs_fill_zplprops(const char *dataset, nvlist_t *createprops,
-    nvlist_t *zplprops, boolean_t *is_ci)
+				  nvlist_t *zplprops, boolean_t *is_ci)
 {
 	boolean_t fuids_ok, sa_ok;
 	uint64_t zplver = ZPL_VERSION;
@@ -3184,7 +3368,7 @@ zfs_fill_zplprops(const char *dataset, nvlist_t *createprops,
 	uint64_t spa_vers;
 	int error;
 
-	zfs_get_parent(dataset, parentname, sizeof (parentname));
+	zfs_get_parent(dataset, parentname, sizeof(parentname));
 
 	if ((error = spa_open(dataset, &spa, FTAG)) != 0)
 		return (error);
@@ -3203,14 +3387,14 @@ zfs_fill_zplprops(const char *dataset, nvlist_t *createprops,
 		return (error);
 
 	error = zfs_fill_zplprops_impl(os, zplver, fuids_ok, sa_ok, createprops,
-	    zplprops, is_ci);
+								   zplprops, is_ci);
 	dmu_objset_rele(os, FTAG);
 	return (error);
 }
 
 static int
 zfs_fill_zplprops_root(uint64_t spa_vers, nvlist_t *createprops,
-    nvlist_t *zplprops, boolean_t *is_ci)
+					   nvlist_t *zplprops, boolean_t *is_ci)
 {
 	boolean_t fuids_ok;
 	boolean_t sa_ok;
@@ -3222,7 +3406,7 @@ zfs_fill_zplprops_root(uint64_t spa_vers, nvlist_t *createprops,
 	sa_ok = (zplver >= ZPL_VERSION_SA);
 
 	error = zfs_fill_zplprops_impl(NULL, zplver, fuids_ok, sa_ok,
-	    createprops, zplprops, is_ci);
+								   createprops, zplprops, is_ci);
 	return (error);
 }
 
@@ -3238,16 +3422,16 @@ zfs_fill_zplprops_root(uint64_t spa_vers, nvlist_t *createprops,
  */
 
 static const zfs_ioc_key_t zfs_keys_create[] = {
-	{"type",	DATA_TYPE_INT32,	0},
-	{"props",	DATA_TYPE_NVLIST,	ZK_OPTIONAL},
-	{"hidden_args",	DATA_TYPE_NVLIST,	ZK_OPTIONAL},
+	{"type", DATA_TYPE_INT32, 0},
+	{"props", DATA_TYPE_NVLIST, ZK_OPTIONAL},
+	{"hidden_args", DATA_TYPE_NVLIST, ZK_OPTIONAL},
 };
 
 static int
 zfs_ioc_create(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 {
 	int error = 0;
-	zfs_creat_t zct = { 0 };
+	zfs_creat_t zct = {0};
 	nvlist_t *nvprops = NULL;
 	nvlist_t *hidden_args = NULL;
 	void (*cbfunc)(objset_t *os, void *arg, cred_t *cr, dmu_tx_t *tx);
@@ -3256,10 +3440,11 @@ zfs_ioc_create(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 	dsl_crypto_params_t *dcp = NULL;
 
 	type = (dmu_objset_type_t)fnvlist_lookup_int32(innvl, "type");
-	(void) nvlist_lookup_nvlist(innvl, "props", &nvprops);
-	(void) nvlist_lookup_nvlist(innvl, ZPOOL_HIDDEN_ARGS, &hidden_args);
+	(void)nvlist_lookup_nvlist(innvl, "props", &nvprops);
+	(void)nvlist_lookup_nvlist(innvl, ZPOOL_HIDDEN_ARGS, &hidden_args);
 
-	switch (type) {
+	switch (type)
+	{
 	case DMU_OST_ZFS:
 		cbfunc = zfs_create_cb;
 		break;
@@ -3273,7 +3458,7 @@ zfs_ioc_create(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 		break;
 	}
 	if (strchr(fsname, '@') ||
-	    strchr(fsname, '%'))
+		strchr(fsname, '%'))
 		return (SET_ERROR(EINVAL));
 
 	zct.zct_props = nvprops;
@@ -3281,30 +3466,34 @@ zfs_ioc_create(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 	if (cbfunc == NULL)
 		return (SET_ERROR(EINVAL));
 
-	if (type == DMU_OST_ZVOL) {
+	if (type == DMU_OST_ZVOL)
+	{
 		uint64_t volsize, volblocksize;
 
 		if (nvprops == NULL)
 			return (SET_ERROR(EINVAL));
 		if (nvlist_lookup_uint64(nvprops,
-		    zfs_prop_to_name(ZFS_PROP_VOLSIZE), &volsize) != 0)
+								 zfs_prop_to_name(ZFS_PROP_VOLSIZE), &volsize) != 0)
 			return (SET_ERROR(EINVAL));
 
 		if ((error = nvlist_lookup_uint64(nvprops,
-		    zfs_prop_to_name(ZFS_PROP_VOLBLOCKSIZE),
-		    &volblocksize)) != 0 && error != ENOENT)
+										  zfs_prop_to_name(ZFS_PROP_VOLBLOCKSIZE),
+										  &volblocksize)) != 0 &&
+			error != ENOENT)
 			return (SET_ERROR(EINVAL));
 
 		if (error != 0)
 			volblocksize = zfs_prop_default_numeric(
-			    ZFS_PROP_VOLBLOCKSIZE);
+				ZFS_PROP_VOLBLOCKSIZE);
 
 		if ((error = zvol_check_volblocksize(fsname,
-		    volblocksize)) != 0 ||
-		    (error = zvol_check_volsize(volsize,
-		    volblocksize)) != 0)
+											 volblocksize)) != 0 ||
+			(error = zvol_check_volsize(volsize,
+										volblocksize)) != 0)
 			return (error);
-	} else if (type == DMU_OST_ZFS) {
+	}
+	else if (type == DMU_OST_ZFS)
+	{
 		int error;
 
 		/*
@@ -3314,24 +3503,26 @@ zfs_ioc_create(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 		 * now.
 		 */
 		VERIFY(nvlist_alloc(&zct.zct_zplprops,
-		    NV_UNIQUE_NAME, KM_SLEEP) == 0);
+							NV_UNIQUE_NAME, KM_SLEEP) == 0);
 		error = zfs_fill_zplprops(fsname, nvprops,
-		    zct.zct_zplprops, &is_insensitive);
-		if (error != 0) {
+								  zct.zct_zplprops, &is_insensitive);
+		if (error != 0)
+		{
 			nvlist_free(zct.zct_zplprops);
 			return (error);
 		}
 	}
 
 	error = dsl_crypto_params_create_nvlist(DCP_CMD_NONE, nvprops,
-	    hidden_args, &dcp);
-	if (error != 0) {
+											hidden_args, &dcp);
+	if (error != 0)
+	{
 		nvlist_free(zct.zct_zplprops);
 		return (error);
 	}
 
 	error = dmu_objset_create(fsname, type,
-	    is_insensitive ? DS_FLAG_CI_DATASET : 0, dcp, cbfunc, &zct);
+							  is_insensitive ? DS_FLAG_CI_DATASET : 0, dcp, cbfunc, &zct);
 
 	nvlist_free(zct.zct_zplprops);
 	dsl_crypto_params_free(dcp, !!error);
@@ -3339,10 +3530,12 @@ zfs_ioc_create(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 	/*
 	 * It would be nice to do this atomically.
 	 */
-	if (error == 0) {
+	if (error == 0)
+	{
 		error = zfs_set_prop_nvlist(fsname, ZPROP_SRC_LOCAL,
-		    nvprops, outnvl);
-		if (error != 0) {
+									nvprops, outnvl);
+		if (error != 0)
+		{
 			spa_t *spa;
 			int error2;
 
@@ -3353,9 +3546,11 @@ zfs_ioc_create(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 			 * the spa_zvol_taskq to drain then retry.
 			 */
 			error2 = dsl_destroy_head(fsname);
-			while ((error2 == EBUSY) && (type == DMU_OST_ZVOL)) {
+			while ((error2 == EBUSY) && (type == DMU_OST_ZVOL))
+			{
 				error2 = spa_open(fsname, &spa, FTAG);
-				if (error2 == 0) {
+				if (error2 == 0)
+				{
 					taskq_wait(spa->spa_zvol_taskq);
 					spa_close(spa, FTAG);
 				}
@@ -3378,9 +3573,9 @@ zfs_ioc_create(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
  * outnvl: propname -> error code (int32)
  */
 static const zfs_ioc_key_t zfs_keys_clone[] = {
-	{"origin",	DATA_TYPE_STRING,	0},
-	{"props",	DATA_TYPE_NVLIST,	ZK_OPTIONAL},
-	{"hidden_args",	DATA_TYPE_NVLIST,	ZK_OPTIONAL},
+	{"origin", DATA_TYPE_STRING, 0},
+	{"props", DATA_TYPE_NVLIST, ZK_OPTIONAL},
+	{"hidden_args", DATA_TYPE_NVLIST, ZK_OPTIONAL},
 };
 
 static int
@@ -3391,10 +3586,10 @@ zfs_ioc_clone(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 	const char *origin_name;
 
 	origin_name = fnvlist_lookup_string(innvl, "origin");
-	(void) nvlist_lookup_nvlist(innvl, "props", &nvprops);
+	(void)nvlist_lookup_nvlist(innvl, "props", &nvprops);
 
 	if (strchr(fsname, '@') ||
-	    strchr(fsname, '%'))
+		strchr(fsname, '%'))
 		return (SET_ERROR(EINVAL));
 
 	if (dataset_namecheck(origin_name, NULL, NULL) != 0)
@@ -3405,11 +3600,12 @@ zfs_ioc_clone(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 	/*
 	 * It would be nice to do this atomically.
 	 */
-	if (error == 0) {
+	if (error == 0)
+	{
 		error = zfs_set_prop_nvlist(fsname, ZPROP_SRC_LOCAL,
-		    nvprops, outnvl);
+									nvprops, outnvl);
 		if (error != 0)
-			(void) dsl_destroy_head(fsname);
+			(void)dsl_destroy_head(fsname);
 	}
 	return (error);
 }
@@ -3435,8 +3631,8 @@ zfs_ioc_remap(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
  * outnvl: snapshot -> error code (int32)
  */
 static const zfs_ioc_key_t zfs_keys_snapshot[] = {
-	{"snaps",	DATA_TYPE_NVLIST,	0},
-	{"props",	DATA_TYPE_NVLIST,	ZK_OPTIONAL},
+	{"snaps", DATA_TYPE_NVLIST, 0},
+	{"props", DATA_TYPE_NVLIST, ZK_OPTIONAL},
 };
 
 static int
@@ -3447,9 +3643,9 @@ zfs_ioc_snapshot(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 	int error, poollen;
 	nvpair_t *pair;
 
-	(void) nvlist_lookup_nvlist(innvl, "props", &props);
+	(void)nvlist_lookup_nvlist(innvl, "props", &props);
 	if (!nvlist_empty(props) &&
-	    zfs_earlier_version(poolname, SPA_VERSION_SNAP_PROPS))
+		zfs_earlier_version(poolname, SPA_VERSION_SNAP_PROPS))
 		return (SET_ERROR(ENOTSUP));
 	if ((error = zfs_check_userprops(props)) != 0)
 		return (error);
@@ -3457,7 +3653,8 @@ zfs_ioc_snapshot(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 	snaps = fnvlist_lookup_nvlist(innvl, "snaps");
 	poollen = strlen(poolname);
 	for (pair = nvlist_next_nvpair(snaps, NULL); pair != NULL;
-	    pair = nvlist_next_nvpair(snaps, pair)) {
+		 pair = nvlist_next_nvpair(snaps, pair))
+	{
 		const char *name = nvpair_name(pair);
 		char *cp = strchr(name, '@');
 
@@ -3466,23 +3663,24 @@ zfs_ioc_snapshot(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 		 * contain only valid characters.
 		 */
 		if (cp == NULL ||
-		    zfs_component_namecheck(cp + 1, NULL, NULL) != 0)
+			zfs_component_namecheck(cp + 1, NULL, NULL) != 0)
 			return (SET_ERROR(EINVAL));
 
 		/*
 		 * The snap must be in the specified pool.
 		 */
 		if (strncmp(name, poolname, poollen) != 0 ||
-		    (name[poollen] != '/' && name[poollen] != '@'))
+			(name[poollen] != '/' && name[poollen] != '@'))
 			return (SET_ERROR(EXDEV));
 
 		/*
 		 * Check for permission to set the properties on the fs.
 		 */
-		if (!nvlist_empty(props)) {
+		if (!nvlist_empty(props))
+		{
 			*cp = '\0';
 			error = zfs_secpolicy_write_perms(name,
-			    ZFS_DELEG_PERM_USERPROP, CRED());
+											  ZFS_DELEG_PERM_USERPROP, CRED());
 			*cp = '@';
 			if (error != 0)
 				return (error);
@@ -3490,9 +3688,10 @@ zfs_ioc_snapshot(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 
 		/* This must be the only snap of this fs. */
 		for (nvpair_t *pair2 = nvlist_next_nvpair(snaps, pair);
-		    pair2 != NULL; pair2 = nvlist_next_nvpair(snaps, pair2)) {
-			if (strncmp(name, nvpair_name(pair2), cp - name + 1)
-			    == 0) {
+			 pair2 != NULL; pair2 = nvlist_next_nvpair(snaps, pair2))
+		{
+			if (strncmp(name, nvpair_name(pair2), cp - name + 1) == 0)
+			{
 				return (SET_ERROR(EXDEV));
 			}
 		}
@@ -3507,7 +3706,7 @@ zfs_ioc_snapshot(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
  * innvl: "message" -> string
  */
 static const zfs_ioc_key_t zfs_keys_log_history[] = {
-	{"message",	DATA_TYPE_STRING,	0},
+	{"message", DATA_TYPE_STRING, 0},
 };
 
 /* ARGSUSED */
@@ -3529,7 +3728,7 @@ zfs_ioc_log_history(const char *unused, nvlist_t *innvl, nvlist_t *outnvl)
 	poolname = tsd_get(zfs_allow_log_key);
 	if (poolname == NULL)
 		return (SET_ERROR(EINVAL));
-	(void) tsd_set(zfs_allow_log_key, NULL);
+	(void)tsd_set(zfs_allow_log_key, NULL);
 	error = spa_open(poolname, &spa, FTAG);
 	kmem_strfree(poolname);
 	if (error != 0)
@@ -3537,7 +3736,8 @@ zfs_ioc_log_history(const char *unused, nvlist_t *innvl, nvlist_t *outnvl)
 
 	message = fnvlist_lookup_string(innvl, "message");
 
-	if (spa_version(spa) < SPA_VERSION_ZPOOL_HISTORY) {
+	if (spa_version(spa) < SPA_VERSION_ZPOOL_HISTORY)
+	{
 		spa_close(spa, FTAG);
 		return (SET_ERROR(ENOTSUP));
 	}
@@ -3559,8 +3759,8 @@ zfs_ioc_log_history(const char *unused, nvlist_t *innvl, nvlist_t *outnvl)
  * VB_NVLIST: nvlist with arbitrary <key, value> pairs.
  */
 static const zfs_ioc_key_t zfs_keys_set_bootenv[] = {
-	{"version",	DATA_TYPE_UINT64,	0},
-	{"<keys>",	DATA_TYPE_ANY, ZK_OPTIONAL | ZK_WILDCARDLIST},
+	{"version", DATA_TYPE_UINT64, 0},
+	{"<keys>", DATA_TYPE_ANY, ZK_OPTIONAL | ZK_WILDCARDLIST},
 };
 
 static int
@@ -3573,7 +3773,7 @@ zfs_ioc_set_bootenv(const char *name, nvlist_t *innvl, nvlist_t *outnvl)
 		return (error);
 	spa_vdev_state_enter(spa, SCL_ALL);
 	error = vdev_label_write_bootenv(spa->spa_root_vdev, innvl);
-	(void) spa_vdev_state_exit(spa, NULL, 0);
+	(void)spa_vdev_state_exit(spa, NULL, 0);
 	spa_close(spa, FTAG);
 	return (error);
 }
@@ -3592,7 +3792,7 @@ zfs_ioc_get_bootenv(const char *name, nvlist_t *innvl, nvlist_t *outnvl)
 		return (error);
 	spa_vdev_state_enter(spa, SCL_ALL);
 	error = vdev_label_read_bootenv(spa->spa_root_vdev, outnvl);
-	(void) spa_vdev_state_exit(spa, NULL, 0);
+	(void)spa_vdev_state_exit(spa, NULL, 0);
 	spa_close(spa, FTAG);
 	return (error);
 }
@@ -3607,13 +3807,12 @@ zfs_ioc_get_bootenv(const char *name, nvlist_t *innvl, nvlist_t *outnvl)
  * Returns 0 if the argument is not a snapshot, or it is not currently a
  * filesystem, or we were able to unmount it.  Returns error code otherwise.
  */
-void
-zfs_unmount_snap(const char *snapname)
+void zfs_unmount_snap(const char *snapname)
 {
 	if (strchr(snapname, '@') == NULL)
 		return;
 
-	(void) zfsctl_snapshot_unmount(snapname, MNT_FORCE);
+	(void)zfsctl_snapshot_unmount(snapname, MNT_FORCE);
 }
 
 /* ARGSUSED */
@@ -3629,8 +3828,7 @@ zfs_unmount_snap_cb(const char *snapname, void *arg)
  * in which case it must be unmounted.  This routine will do that unmount
  * if necessary.
  */
-void
-zfs_destroy_unmount_origin(const char *fsname)
+void zfs_destroy_unmount_origin(const char *fsname)
 {
 	int error;
 	objset_t *os;
@@ -3640,12 +3838,15 @@ zfs_destroy_unmount_origin(const char *fsname)
 	if (error != 0)
 		return;
 	ds = dmu_objset_ds(os);
-	if (dsl_dir_is_clone(ds->ds_dir) && DS_IS_DEFER_DESTROY(ds->ds_prev)) {
+	if (dsl_dir_is_clone(ds->ds_dir) && DS_IS_DEFER_DESTROY(ds->ds_prev))
+	{
 		char originname[ZFS_MAX_DATASET_NAME_LEN];
 		dsl_dataset_name(ds->ds_prev, originname);
 		dmu_objset_rele(os, FTAG);
 		zfs_unmount_snap(originname);
-	} else {
+	}
+	else
+	{
 		dmu_objset_rele(os, FTAG);
 	}
 }
@@ -3659,8 +3860,8 @@ zfs_destroy_unmount_origin(const char *fsname)
  * outnvl: snapshot -> error code (int32)
  */
 static const zfs_ioc_key_t zfs_keys_destroy_snaps[] = {
-	{"snaps",	DATA_TYPE_NVLIST,	0},
-	{"defer",	DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
+	{"snaps", DATA_TYPE_NVLIST, 0},
+	{"defer", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
 };
 
 /* ARGSUSED */
@@ -3678,7 +3879,8 @@ zfs_ioc_destroy_snaps(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 
 	poollen = strlen(poolname);
 	for (pair = nvlist_next_nvpair(snaps, NULL); pair != NULL;
-	    pair = nvlist_next_nvpair(snaps, pair)) {
+		 pair = nvlist_next_nvpair(snaps, pair))
+	{
 		const char *name = nvpair_name(pair);
 
 		/*
@@ -3686,11 +3888,12 @@ zfs_ioc_destroy_snaps(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 		 * invalid removal of zvol minors below.
 		 */
 		if (strncmp(name, poolname, poollen) != 0 ||
-		    (name[poollen] != '/' && name[poollen] != '@'))
+			(name[poollen] != '/' && name[poollen] != '@'))
 			return (SET_ERROR(EXDEV));
 
 		zfs_unmount_snap(nvpair_name(pair));
-		if (spa_open(name, &spa, FTAG) == 0) {
+		if (spa_open(name, &spa, FTAG) == 0)
+		{
 			zvol_remove_minors(spa, name, B_TRUE);
 			spa_close(spa, FTAG);
 		}
@@ -3713,7 +3916,7 @@ zfs_ioc_destroy_snaps(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
  *
  */
 static const zfs_ioc_key_t zfs_keys_bookmark[] = {
-	{"<bookmark>...",	DATA_TYPE_STRING,	ZK_WILDCARDLIST},
+	{"<bookmark>...", DATA_TYPE_STRING, ZK_WILDCARDLIST},
 };
 
 /* ARGSUSED */
@@ -3759,7 +3962,7 @@ static const zfs_ioc_key_t zfs_keys_get_bookmark_props[] = {
 /* ARGSUSED */
 static int
 zfs_ioc_get_bookmark_props(const char *bookmark, nvlist_t *innvl,
-    nvlist_t *outnvl)
+						   nvlist_t *outnvl)
 {
 	char fsname[ZFS_MAX_DATASET_NAME_LEN];
 	char *bmname;
@@ -3769,7 +3972,7 @@ zfs_ioc_get_bookmark_props(const char *bookmark, nvlist_t *innvl,
 		return (SET_ERROR(EINVAL));
 	bmname++;
 
-	(void) strlcpy(fsname, bookmark, sizeof (fsname));
+	(void)strlcpy(fsname, bookmark, sizeof(fsname));
 	*(strchr(fsname, '#')) = '\0';
 
 	return (dsl_get_bookmark_props(fsname, bmname, outnvl));
@@ -3784,18 +3987,19 @@ zfs_ioc_get_bookmark_props(const char *bookmark, nvlist_t *innvl,
  *
  */
 static const zfs_ioc_key_t zfs_keys_destroy_bookmarks[] = {
-	{"<bookmark>...",	DATA_TYPE_BOOLEAN,	ZK_WILDCARDLIST},
+	{"<bookmark>...", DATA_TYPE_BOOLEAN, ZK_WILDCARDLIST},
 };
 
 static int
 zfs_ioc_destroy_bookmarks(const char *poolname, nvlist_t *innvl,
-    nvlist_t *outnvl)
+						  nvlist_t *outnvl)
 {
 	int error, poollen;
 
 	poollen = strlen(poolname);
 	for (nvpair_t *pair = nvlist_next_nvpair(innvl, NULL);
-	    pair != NULL; pair = nvlist_next_nvpair(innvl, pair)) {
+		 pair != NULL; pair = nvlist_next_nvpair(innvl, pair))
+	{
 		const char *name = nvpair_name(pair);
 		const char *cp = strchr(name, '#');
 
@@ -3804,14 +4008,14 @@ zfs_ioc_destroy_bookmarks(const char *poolname, nvlist_t *innvl,
 		 * must contain only valid characters.
 		 */
 		if (cp == NULL ||
-		    zfs_component_namecheck(cp + 1, NULL, NULL) != 0)
+			zfs_component_namecheck(cp + 1, NULL, NULL) != 0)
 			return (SET_ERROR(EINVAL));
 
 		/*
 		 * The bookmark must be in the specified pool.
 		 */
 		if (strncmp(name, poolname, poollen) != 0 ||
-		    (name[poollen] != '/' && name[poollen] != '#'))
+			(name[poollen] != '/' && name[poollen] != '#'))
 			return (SET_ERROR(EXDEV));
 	}
 
@@ -3820,16 +4024,16 @@ zfs_ioc_destroy_bookmarks(const char *poolname, nvlist_t *innvl,
 }
 
 static const zfs_ioc_key_t zfs_keys_channel_program[] = {
-	{"program",	DATA_TYPE_STRING,		0},
-	{"arg",		DATA_TYPE_ANY,			0},
-	{"sync",	DATA_TYPE_BOOLEAN_VALUE,	ZK_OPTIONAL},
-	{"instrlimit",	DATA_TYPE_UINT64,		ZK_OPTIONAL},
-	{"memlimit",	DATA_TYPE_UINT64,		ZK_OPTIONAL},
+	{"program", DATA_TYPE_STRING, 0},
+	{"arg", DATA_TYPE_ANY, 0},
+	{"sync", DATA_TYPE_BOOLEAN_VALUE, ZK_OPTIONAL},
+	{"instrlimit", DATA_TYPE_UINT64, ZK_OPTIONAL},
+	{"memlimit", DATA_TYPE_UINT64, ZK_OPTIONAL},
 };
 
 static int
 zfs_ioc_channel_program(const char *poolname, nvlist_t *innvl,
-    nvlist_t *outnvl)
+						nvlist_t *outnvl)
 {
 	char *program;
 	uint64_t instrlimit, memlimit;
@@ -3837,13 +4041,16 @@ zfs_ioc_channel_program(const char *poolname, nvlist_t *innvl,
 	nvpair_t *nvarg = NULL;
 
 	program = fnvlist_lookup_string(innvl, ZCP_ARG_PROGRAM);
-	if (0 != nvlist_lookup_boolean_value(innvl, ZCP_ARG_SYNC, &sync_flag)) {
+	if (0 != nvlist_lookup_boolean_value(innvl, ZCP_ARG_SYNC, &sync_flag))
+	{
 		sync_flag = B_TRUE;
 	}
-	if (0 != nvlist_lookup_uint64(innvl, ZCP_ARG_INSTRLIMIT, &instrlimit)) {
+	if (0 != nvlist_lookup_uint64(innvl, ZCP_ARG_INSTRLIMIT, &instrlimit))
+	{
 		instrlimit = ZCP_DEFAULT_INSTRLIMIT;
 	}
-	if (0 != nvlist_lookup_uint64(innvl, ZCP_ARG_MEMLIMIT, &memlimit)) {
+	if (0 != nvlist_lookup_uint64(innvl, ZCP_ARG_MEMLIMIT, &memlimit))
+	{
 		memlimit = ZCP_DEFAULT_MEMLIMIT;
 	}
 	nvarg = fnvlist_lookup_nvpair(innvl, ZCP_ARG_ARGLIST);
@@ -3854,7 +4061,7 @@ zfs_ioc_channel_program(const char *poolname, nvlist_t *innvl,
 		return (SET_ERROR(EINVAL));
 
 	return (zcp_eval(poolname, program, sync_flag, instrlimit, memlimit,
-	    nvarg, outnvl));
+					 nvarg, outnvl));
 }
 
 /*
@@ -3883,7 +4090,7 @@ static const zfs_ioc_key_t zfs_keys_pool_discard_checkpoint[] = {
 /* ARGSUSED */
 static int
 zfs_ioc_pool_discard_checkpoint(const char *poolname, nvlist_t *innvl,
-    nvlist_t *outnvl)
+								nvlist_t *outnvl)
 {
 	return (spa_checkpoint_discard(poolname));
 }
@@ -3911,11 +4118,15 @@ zfs_ioc_destroy(zfs_cmd_t *zc)
 	if (ost == DMU_OST_ZFS)
 		zfs_unmount_snap(zc->zc_name);
 
-	if (strchr(zc->zc_name, '@')) {
+	if (strchr(zc->zc_name, '@'))
+	{
 		err = dsl_destroy_snapshot(zc->zc_name, zc->zc_defer_destroy);
-	} else {
+	}
+	else
+	{
 		err = dsl_destroy_head(zc->zc_name);
-		if (err == EEXIST) {
+		if (err == EEXIST)
+		{
 			/*
 			 * It is possible that the given DS may have
 			 * hidden child (%recv) datasets - "leftovers"
@@ -3926,9 +4137,9 @@ zfs_ioc_destroy(zfs_cmd_t *zc)
 			 */
 			char namebuf[ZFS_MAX_DATASET_NAME_LEN + 6];
 
-			if (snprintf(namebuf, sizeof (namebuf), "%s/%s",
-			    zc->zc_name, recv_clone_name) >=
-			    sizeof (namebuf))
+			if (snprintf(namebuf, sizeof(namebuf), "%s/%s",
+						 zc->zc_name, recv_clone_name) >=
+				sizeof(namebuf))
 				return (SET_ERROR(EINVAL));
 
 			/*
@@ -3970,36 +4181,40 @@ zfs_ioc_destroy(zfs_cmd_t *zc)
  * guids have be specified with a type other than uint64.
  */
 static const zfs_ioc_key_t zfs_keys_pool_initialize[] = {
-	{ZPOOL_INITIALIZE_COMMAND,	DATA_TYPE_UINT64,	0},
-	{ZPOOL_INITIALIZE_VDEVS,	DATA_TYPE_NVLIST,	0}
-};
+	{ZPOOL_INITIALIZE_COMMAND, DATA_TYPE_UINT64, 0},
+	{ZPOOL_INITIALIZE_VDEVS, DATA_TYPE_NVLIST, 0}};
 
 static int
 zfs_ioc_pool_initialize(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 {
 	uint64_t cmd_type;
 	if (nvlist_lookup_uint64(innvl, ZPOOL_INITIALIZE_COMMAND,
-	    &cmd_type) != 0) {
+							 &cmd_type) != 0)
+	{
 		return (SET_ERROR(EINVAL));
 	}
 
 	if (!(cmd_type == POOL_INITIALIZE_CANCEL ||
-	    cmd_type == POOL_INITIALIZE_START ||
-	    cmd_type == POOL_INITIALIZE_SUSPEND ||
-	    cmd_type == POOL_INITIALIZE_UNINIT)) {
+		  cmd_type == POOL_INITIALIZE_START ||
+		  cmd_type == POOL_INITIALIZE_SUSPEND ||
+		  cmd_type == POOL_INITIALIZE_UNINIT))
+	{
 		return (SET_ERROR(EINVAL));
 	}
 
 	nvlist_t *vdev_guids;
 	if (nvlist_lookup_nvlist(innvl, ZPOOL_INITIALIZE_VDEVS,
-	    &vdev_guids) != 0) {
+							 &vdev_guids) != 0)
+	{
 		return (SET_ERROR(EINVAL));
 	}
 
 	for (nvpair_t *pair = nvlist_next_nvpair(vdev_guids, NULL);
-	    pair != NULL; pair = nvlist_next_nvpair(vdev_guids, pair)) {
+		 pair != NULL; pair = nvlist_next_nvpair(vdev_guids, pair))
+	{
 		uint64_t vdev_guid;
-		if (nvpair_value_uint64(pair, &vdev_guid) != 0) {
+		if (nvpair_value_uint64(pair, &vdev_guid) != 0)
+		{
 			return (SET_ERROR(EINVAL));
 		}
 	}
@@ -4011,11 +4226,12 @@ zfs_ioc_pool_initialize(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 
 	nvlist_t *vdev_errlist = fnvlist_alloc();
 	int total_errors = spa_vdev_initialize(spa, vdev_guids, cmd_type,
-	    vdev_errlist);
+										   vdev_errlist);
 
-	if (fnvlist_size(vdev_errlist) > 0) {
+	if (fnvlist_size(vdev_errlist) > 0)
+	{
 		fnvlist_add_nvlist(outnvl, ZPOOL_INITIALIZE_VDEVS,
-		    vdev_errlist);
+						   vdev_errlist);
 	}
 	fnvlist_free(vdev_errlist);
 
@@ -4047,10 +4263,10 @@ zfs_ioc_pool_initialize(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
  * guids have be specified with a type other than uint64.
  */
 static const zfs_ioc_key_t zfs_keys_pool_trim[] = {
-	{ZPOOL_TRIM_COMMAND,	DATA_TYPE_UINT64,		0},
-	{ZPOOL_TRIM_VDEVS,	DATA_TYPE_NVLIST,		0},
-	{ZPOOL_TRIM_RATE,	DATA_TYPE_UINT64,		ZK_OPTIONAL},
-	{ZPOOL_TRIM_SECURE,	DATA_TYPE_BOOLEAN_VALUE,	ZK_OPTIONAL},
+	{ZPOOL_TRIM_COMMAND, DATA_TYPE_UINT64, 0},
+	{ZPOOL_TRIM_VDEVS, DATA_TYPE_NVLIST, 0},
+	{ZPOOL_TRIM_RATE, DATA_TYPE_UINT64, ZK_OPTIONAL},
+	{ZPOOL_TRIM_SECURE, DATA_TYPE_BOOLEAN_VALUE, ZK_OPTIONAL},
 };
 
 static int
@@ -4061,8 +4277,9 @@ zfs_ioc_pool_trim(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 		return (SET_ERROR(EINVAL));
 
 	if (!(cmd_type == POOL_TRIM_CANCEL ||
-	    cmd_type == POOL_TRIM_START ||
-	    cmd_type == POOL_TRIM_SUSPEND)) {
+		  cmd_type == POOL_TRIM_START ||
+		  cmd_type == POOL_TRIM_SUSPEND))
+	{
 		return (SET_ERROR(EINVAL));
 	}
 
@@ -4071,9 +4288,11 @@ zfs_ioc_pool_trim(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 		return (SET_ERROR(EINVAL));
 
 	for (nvpair_t *pair = nvlist_next_nvpair(vdev_guids, NULL);
-	    pair != NULL; pair = nvlist_next_nvpair(vdev_guids, pair)) {
+		 pair != NULL; pair = nvlist_next_nvpair(vdev_guids, pair))
+	{
 		uint64_t vdev_guid;
-		if (nvpair_value_uint64(pair, &vdev_guid) != 0) {
+		if (nvpair_value_uint64(pair, &vdev_guid) != 0)
+		{
 			return (SET_ERROR(EINVAL));
 		}
 	}
@@ -4086,7 +4305,8 @@ zfs_ioc_pool_trim(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 	/* Optional, defaults to standard TRIM when not provided */
 	boolean_t secure;
 	if (nvlist_lookup_boolean_value(innvl, ZPOOL_TRIM_SECURE,
-	    &secure) != 0) {
+									&secure) != 0)
+	{
 		secure = B_FALSE;
 	}
 
@@ -4097,7 +4317,7 @@ zfs_ioc_pool_trim(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
 
 	nvlist_t *vdev_errlist = fnvlist_alloc();
 	int total_errors = spa_vdev_trim(spa, vdev_guids, cmd_type,
-	    rate, !!zfs_trim_metaslab_skip, secure, vdev_errlist);
+									 rate, !!zfs_trim_metaslab_skip, secure, vdev_errlist);
 
 	if (fnvlist_size(vdev_errlist) > 0)
 		fnvlist_add_nvlist(outnvl, ZPOOL_TRIM_VDEVS, vdev_errlist);
@@ -4132,8 +4352,8 @@ zfs_ioc_pool_trim(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
  * outnvl: "waited" -> boolean_t
  */
 static const zfs_ioc_key_t zfs_keys_pool_wait[] = {
-	{ZPOOL_WAIT_ACTIVITY,	DATA_TYPE_INT32,		0},
-	{ZPOOL_WAIT_TAG,	DATA_TYPE_UINT64,		ZK_OPTIONAL},
+	{ZPOOL_WAIT_ACTIVITY, DATA_TYPE_INT32, 0},
+	{ZPOOL_WAIT_TAG, DATA_TYPE_UINT64, ZK_OPTIONAL},
 };
 
 static int
@@ -4175,7 +4395,7 @@ zfs_ioc_wait(const char *name, nvlist_t *innvl, nvlist_t *outnvl)
  * outnvl: "waited" -> boolean_t
  */
 static const zfs_ioc_key_t zfs_keys_fs_wait[] = {
-	{ZFS_WAIT_ACTIVITY,	DATA_TYPE_INT32,		0},
+	{ZFS_WAIT_ACTIVITY, DATA_TYPE_INT32, 0},
 };
 
 static int
@@ -4197,7 +4417,8 @@ zfs_ioc_wait_fs(const char *name, nvlist_t *innvl, nvlist_t *outnvl)
 	if ((error = dsl_pool_hold(name, FTAG, &dp)) != 0)
 		return (error);
 
-	if ((error = dsl_dataset_hold(dp, name, FTAG, &ds)) != 0) {
+	if ((error = dsl_dataset_hold(dp, name, FTAG, &ds)) != 0)
+	{
 		dsl_pool_rele(dp, FTAG);
 		return (error);
 	}
@@ -4244,7 +4465,7 @@ zfs_ioc_wait_fs(const char *name, nvlist_t *innvl, nvlist_t *outnvl)
  * }
  */
 static const zfs_ioc_key_t zfs_keys_rollback[] = {
-	{"target",	DATA_TYPE_STRING,	ZK_OPTIONAL},
+	{"target", DATA_TYPE_STRING, ZK_OPTIONAL},
 };
 
 /* ARGSUSED */
@@ -4256,8 +4477,9 @@ zfs_ioc_rollback(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 	char *target = NULL;
 	int error;
 
-	(void) nvlist_lookup_string(innvl, "target", &target);
-	if (target != NULL) {
+	(void)nvlist_lookup_string(innvl, "target", &target);
+	if (target != NULL)
+	{
 		const char *cp = strchr(target, '@');
 
 		/*
@@ -4265,29 +4487,35 @@ zfs_ioc_rollback(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 		 * contain only valid characters.
 		 */
 		if (cp == NULL ||
-		    zfs_component_namecheck(cp + 1, NULL, NULL) != 0)
+			zfs_component_namecheck(cp + 1, NULL, NULL) != 0)
 			return (SET_ERROR(EINVAL));
 	}
 
-	if (getzfsvfs(fsname, &zfsvfs) == 0) {
+	if (getzfsvfs(fsname, &zfsvfs) == 0)
+	{
 		dsl_dataset_t *ds;
 
 		ds = dmu_objset_ds(zfsvfs->z_os);
 		error = zfs_suspend_fs(zfsvfs);
-		if (error == 0) {
+		if (error == 0)
+		{
 			int resume_err;
 
 			error = dsl_dataset_rollback(fsname, target, zfsvfs,
-			    outnvl);
+										 outnvl);
 			resume_err = zfs_resume_fs(zfsvfs, ds);
 			error = error ? error : resume_err;
 		}
 		zfs_vfs_rele(zfsvfs);
-	} else if ((zv = zvol_suspend(fsname)) != NULL) {
+	}
+	else if ((zv = zvol_suspend(fsname)) != NULL)
+	{
 		error = dsl_dataset_rollback(fsname, target, zvol_tag(zv),
-		    outnvl);
+									 outnvl);
 		zvol_resume(zv);
-	} else {
+	}
+	else
+	{
 		error = dsl_dataset_rollback(fsname, target, NULL, outnvl);
 	}
 	return (error);
@@ -4321,8 +4549,8 @@ recursive_unmount(const char *fsname, void *arg)
 
 /* ARGSUSED */
 static const zfs_ioc_key_t zfs_keys_redact[] = {
-	{"bookname",		DATA_TYPE_STRING,	0},
-	{"snapnv",		DATA_TYPE_NVLIST,	0},
+	{"bookname", DATA_TYPE_STRING, 0},
+	{"snapnv", DATA_TYPE_NVLIST, 0},
 };
 static int
 zfs_ioc_redact(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
@@ -4359,11 +4587,11 @@ zfs_ioc_rename(zfs_cmd_t *zc)
 	int err;
 
 	/* "zfs rename" from and to ...%recv datasets should both fail */
-	zc->zc_name[sizeof (zc->zc_name) - 1] = '\0';
-	zc->zc_value[sizeof (zc->zc_value) - 1] = '\0';
+	zc->zc_name[sizeof(zc->zc_name) - 1] = '\0';
+	zc->zc_value[sizeof(zc->zc_value) - 1] = '\0';
 	if (dataset_namecheck(zc->zc_name, NULL, NULL) != 0 ||
-	    dataset_namecheck(zc->zc_value, NULL, NULL) != 0 ||
-	    strchr(zc->zc_name, '%') || strchr(zc->zc_value, '%'))
+		dataset_namecheck(zc->zc_value, NULL, NULL) != 0 ||
+		strchr(zc->zc_name, '%') || strchr(zc->zc_value, '%'))
 		return (SET_ERROR(EINVAL));
 
 	err = dmu_objset_hold(zc->zc_name, FTAG, &os);
@@ -4373,28 +4601,33 @@ zfs_ioc_rename(zfs_cmd_t *zc)
 	dmu_objset_rele(os, FTAG);
 
 	at = strchr(zc->zc_name, '@');
-	if (at != NULL) {
+	if (at != NULL)
+	{
 		/* snaps must be in same fs */
 		int error;
 
 		if (strncmp(zc->zc_name, zc->zc_value, at - zc->zc_name + 1))
 			return (SET_ERROR(EXDEV));
 		*at = '\0';
-		if (ost == DMU_OST_ZFS && !nounmount) {
+		if (ost == DMU_OST_ZFS && !nounmount)
+		{
 			error = dmu_objset_find(zc->zc_name,
-			    recursive_unmount, at + 1,
-			    recursive ? DS_FIND_CHILDREN : 0);
-			if (error != 0) {
+									recursive_unmount, at + 1,
+									recursive ? DS_FIND_CHILDREN : 0);
+			if (error != 0)
+			{
 				*at = '@';
 				return (error);
 			}
 		}
 		error = dsl_dataset_rename_snapshot(zc->zc_name,
-		    at + 1, strchr(zc->zc_value, '@') + 1, recursive);
+											at + 1, strchr(zc->zc_value, '@') + 1, recursive);
 		*at = '@';
 
 		return (error);
-	} else {
+	}
+	else
+	{
 		return (dsl_dir_rename(zc->zc_name, zc->zc_value));
 	}
 }
@@ -4408,48 +4641,63 @@ zfs_check_settable(const char *dsname, nvpair_t *pair, cred_t *cr)
 	uint64_t intval, compval;
 	int err;
 
-	if (prop == ZPROP_INVAL) {
-		if (zfs_prop_user(propname)) {
+	if (prop == ZPROP_INVAL)
+	{
+		if (zfs_prop_user(propname))
+		{
 			if ((err = zfs_secpolicy_write_perms(dsname,
-			    ZFS_DELEG_PERM_USERPROP, cr)))
+												 ZFS_DELEG_PERM_USERPROP, cr)))
 				return (err);
 			return (0);
 		}
 
-		if (!issnap && zfs_prop_userquota(propname)) {
+		if (!issnap && zfs_prop_userquota(propname))
+		{
 			const char *perm = NULL;
 			const char *uq_prefix =
-			    zfs_userquota_prop_prefixes[ZFS_PROP_USERQUOTA];
+				zfs_userquota_prop_prefixes[ZFS_PROP_USERQUOTA];
 			const char *gq_prefix =
-			    zfs_userquota_prop_prefixes[ZFS_PROP_GROUPQUOTA];
+				zfs_userquota_prop_prefixes[ZFS_PROP_GROUPQUOTA];
 			const char *uiq_prefix =
-			    zfs_userquota_prop_prefixes[ZFS_PROP_USEROBJQUOTA];
+				zfs_userquota_prop_prefixes[ZFS_PROP_USEROBJQUOTA];
 			const char *giq_prefix =
-			    zfs_userquota_prop_prefixes[ZFS_PROP_GROUPOBJQUOTA];
+				zfs_userquota_prop_prefixes[ZFS_PROP_GROUPOBJQUOTA];
 			const char *pq_prefix =
-			    zfs_userquota_prop_prefixes[ZFS_PROP_PROJECTQUOTA];
-			const char *piq_prefix = zfs_userquota_prop_prefixes[\
-			    ZFS_PROP_PROJECTOBJQUOTA];
+				zfs_userquota_prop_prefixes[ZFS_PROP_PROJECTQUOTA];
+			const char *piq_prefix = zfs_userquota_prop_prefixes[ZFS_PROP_PROJECTOBJQUOTA];
 
 			if (strncmp(propname, uq_prefix,
-			    strlen(uq_prefix)) == 0) {
+						strlen(uq_prefix)) == 0)
+			{
 				perm = ZFS_DELEG_PERM_USERQUOTA;
-			} else if (strncmp(propname, uiq_prefix,
-			    strlen(uiq_prefix)) == 0) {
+			}
+			else if (strncmp(propname, uiq_prefix,
+							 strlen(uiq_prefix)) == 0)
+			{
 				perm = ZFS_DELEG_PERM_USEROBJQUOTA;
-			} else if (strncmp(propname, gq_prefix,
-			    strlen(gq_prefix)) == 0) {
+			}
+			else if (strncmp(propname, gq_prefix,
+							 strlen(gq_prefix)) == 0)
+			{
 				perm = ZFS_DELEG_PERM_GROUPQUOTA;
-			} else if (strncmp(propname, giq_prefix,
-			    strlen(giq_prefix)) == 0) {
+			}
+			else if (strncmp(propname, giq_prefix,
+							 strlen(giq_prefix)) == 0)
+			{
 				perm = ZFS_DELEG_PERM_GROUPOBJQUOTA;
-			} else if (strncmp(propname, pq_prefix,
-			    strlen(pq_prefix)) == 0) {
+			}
+			else if (strncmp(propname, pq_prefix,
+							 strlen(pq_prefix)) == 0)
+			{
 				perm = ZFS_DELEG_PERM_PROJECTQUOTA;
-			} else if (strncmp(propname, piq_prefix,
-			    strlen(piq_prefix)) == 0) {
+			}
+			else if (strncmp(propname, piq_prefix,
+							 strlen(piq_prefix)) == 0)
+			{
 				perm = ZFS_DELEG_PERM_PROJECTOBJQUOTA;
-			} else {
+			}
+			else
+			{
 				/* {USER|GROUP|PROJECT}USED are read-only */
 				return (SET_ERROR(EINVAL));
 			}
@@ -4465,7 +4713,8 @@ zfs_check_settable(const char *dsname, nvpair_t *pair, cred_t *cr)
 	if (issnap)
 		return (SET_ERROR(EINVAL));
 
-	if (nvpair_type(pair) == DATA_TYPE_NVLIST) {
+	if (nvpair_type(pair) == DATA_TYPE_NVLIST)
+	{
 		/*
 		 * dsl_prop_get_all_impl() returns properties in this
 		 * format.
@@ -4473,55 +4722,62 @@ zfs_check_settable(const char *dsname, nvpair_t *pair, cred_t *cr)
 		nvlist_t *attrs;
 		VERIFY(nvpair_value_nvlist(pair, &attrs) == 0);
 		VERIFY(nvlist_lookup_nvpair(attrs, ZPROP_VALUE,
-		    &pair) == 0);
+									&pair) == 0);
 	}
 
 	/*
 	 * Check that this value is valid for this pool version
 	 */
-	switch (prop) {
+	switch (prop)
+	{
 	case ZFS_PROP_COMPRESSION:
 		/*
 		 * If the user specified gzip compression, make sure
 		 * the SPA supports it. We ignore any errors here since
 		 * we'll catch them later.
 		 */
-		if (nvpair_value_uint64(pair, &intval) == 0) {
+		if (nvpair_value_uint64(pair, &intval) == 0)
+		{
 			compval = ZIO_COMPRESS_ALGO(intval);
 			if (compval >= ZIO_COMPRESS_GZIP_1 &&
-			    compval <= ZIO_COMPRESS_GZIP_9 &&
-			    zfs_earlier_version(dsname,
-			    SPA_VERSION_GZIP_COMPRESSION)) {
+				compval <= ZIO_COMPRESS_GZIP_9 &&
+				zfs_earlier_version(dsname,
+									SPA_VERSION_GZIP_COMPRESSION))
+			{
 				return (SET_ERROR(ENOTSUP));
 			}
 
 			if (compval == ZIO_COMPRESS_ZLE &&
-			    zfs_earlier_version(dsname,
-			    SPA_VERSION_ZLE_COMPRESSION))
+				zfs_earlier_version(dsname,
+									SPA_VERSION_ZLE_COMPRESSION))
 				return (SET_ERROR(ENOTSUP));
 
-			if (compval == ZIO_COMPRESS_LZ4) {
+			if (compval == ZIO_COMPRESS_LZ4)
+			{
 				spa_t *spa;
 
 				if ((err = spa_open(dsname, &spa, FTAG)) != 0)
 					return (err);
 
 				if (!spa_feature_is_enabled(spa,
-				    SPA_FEATURE_LZ4_COMPRESS)) {
+											SPA_FEATURE_LZ4_COMPRESS))
+				{
 					spa_close(spa, FTAG);
 					return (SET_ERROR(ENOTSUP));
 				}
 				spa_close(spa, FTAG);
 			}
 
-			if (compval == ZIO_COMPRESS_ZSTD) {
+			if (compval == ZIO_COMPRESS_ZSTD)
+			{
 				spa_t *spa;
 
 				if ((err = spa_open(dsname, &spa, FTAG)) != 0)
 					return (err);
 
 				if (!spa_feature_is_enabled(spa,
-				    SPA_FEATURE_ZSTD_COMPRESS)) {
+											SPA_FEATURE_ZSTD_COMPRESS))
+				{
 					spa_close(spa, FTAG);
 					return (SET_ERROR(ENOTSUP));
 				}
@@ -4539,7 +4795,8 @@ zfs_check_settable(const char *dsname, nvpair_t *pair, cred_t *cr)
 	case ZFS_PROP_RECORDSIZE:
 		/* Record sizes above 128k need the feature to be enabled */
 		if (nvpair_value_uint64(pair, &intval) == 0 &&
-		    intval > SPA_OLD_MAXBLOCKSIZE) {
+			intval > SPA_OLD_MAXBLOCKSIZE)
+		{
 			spa_t *spa;
 
 			/*
@@ -4547,14 +4804,15 @@ zfs_check_settable(const char *dsname, nvpair_t *pair, cred_t *cr)
 			 * unless the tunable has been changed.
 			 */
 			if (intval > zfs_max_recordsize ||
-			    intval > SPA_MAXBLOCKSIZE)
+				intval > SPA_MAXBLOCKSIZE)
 				return (SET_ERROR(ERANGE));
 
 			if ((err = spa_open(dsname, &spa, FTAG)) != 0)
 				return (err);
 
 			if (!spa_feature_is_enabled(spa,
-			    SPA_FEATURE_LARGE_BLOCKS)) {
+										SPA_FEATURE_LARGE_BLOCKS))
+			{
 				spa_close(spa, FTAG);
 				return (SET_ERROR(ENOTSUP));
 			}
@@ -4565,14 +4823,16 @@ zfs_check_settable(const char *dsname, nvpair_t *pair, cred_t *cr)
 	case ZFS_PROP_DNODESIZE:
 		/* Dnode sizes above 512 need the feature to be enabled */
 		if (nvpair_value_uint64(pair, &intval) == 0 &&
-		    intval != ZFS_DNSIZE_LEGACY) {
+			intval != ZFS_DNSIZE_LEGACY)
+		{
 			spa_t *spa;
 
 			if ((err = spa_open(dsname, &spa, FTAG)) != 0)
 				return (err);
 
 			if (!spa_feature_is_enabled(spa,
-			    SPA_FEATURE_LARGE_DNODE)) {
+										SPA_FEATURE_LARGE_DNODE))
+			{
 				spa_close(spa, FTAG);
 				return (SET_ERROR(ENOTSUP));
 			}
@@ -4596,10 +4856,11 @@ zfs_check_settable(const char *dsname, nvpair_t *pair, cred_t *cr)
 
 	case ZFS_PROP_ACLINHERIT:
 		if (nvpair_type(pair) == DATA_TYPE_UINT64 &&
-		    nvpair_value_uint64(pair, &intval) == 0) {
+			nvpair_value_uint64(pair, &intval) == 0)
+		{
 			if (intval == ZFS_ACL_PASSTHROUGH_X &&
-			    zfs_earlier_version(dsname,
-			    SPA_VERSION_PASSTHROUGH_X))
+				zfs_earlier_version(dsname,
+									SPA_VERSION_PASSTHROUGH_X))
 				return (SET_ERROR(ENOTSUP));
 		}
 		break;
@@ -4612,21 +4873,23 @@ zfs_check_settable(const char *dsname, nvpair_t *pair, cred_t *cr)
 
 		/* dedup feature version checks */
 		if (prop == ZFS_PROP_DEDUP &&
-		    zfs_earlier_version(dsname, SPA_VERSION_DEDUP))
+			zfs_earlier_version(dsname, SPA_VERSION_DEDUP))
 			return (SET_ERROR(ENOTSUP));
 
 		if (nvpair_type(pair) == DATA_TYPE_UINT64 &&
-		    nvpair_value_uint64(pair, &intval) == 0) {
+			nvpair_value_uint64(pair, &intval) == 0)
+		{
 			/* check prop value is enabled in features */
 			feature = zio_checksum_to_feature(
-			    intval & ZIO_CHECKSUM_MASK);
+				intval & ZIO_CHECKSUM_MASK);
 			if (feature == SPA_FEATURE_NONE)
 				break;
 
 			if ((err = spa_open(dsname, &spa, FTAG)) != 0)
 				return (err);
 
-			if (!spa_feature_is_enabled(spa, feature)) {
+			if (!spa_feature_is_enabled(spa, feature))
+			{
 				spa_close(spa, FTAG);
 				return (SET_ERROR(ENOTSUP));
 			}
@@ -4669,28 +4932,33 @@ zfs_check_clearable(const char *dataset, nvlist_t *props, nvlist_t **errlist)
 
 	VERIFY(nvlist_alloc(&errors, NV_UNIQUE_NAME, KM_SLEEP) == 0);
 
-	zc = kmem_alloc(sizeof (zfs_cmd_t), KM_SLEEP);
-	(void) strlcpy(zc->zc_name, dataset, sizeof (zc->zc_name));
+	zc = kmem_alloc(sizeof(zfs_cmd_t), KM_SLEEP);
+	(void)strlcpy(zc->zc_name, dataset, sizeof(zc->zc_name));
 	pair = nvlist_next_nvpair(props, NULL);
-	while (pair != NULL) {
+	while (pair != NULL)
+	{
 		next_pair = nvlist_next_nvpair(props, pair);
 
-		(void) strlcpy(zc->zc_value, nvpair_name(pair),
-		    sizeof (zc->zc_value));
+		(void)strlcpy(zc->zc_value, nvpair_name(pair),
+					  sizeof(zc->zc_value));
 		if ((err = zfs_check_settable(dataset, pair, CRED())) != 0 ||
-		    (err = zfs_secpolicy_inherit_prop(zc, NULL, CRED())) != 0) {
+			(err = zfs_secpolicy_inherit_prop(zc, NULL, CRED())) != 0)
+		{
 			VERIFY(nvlist_remove_nvpair(props, pair) == 0);
 			VERIFY(nvlist_add_int32(errors,
-			    zc->zc_value, err) == 0);
+									zc->zc_value, err) == 0);
 		}
 		pair = next_pair;
 	}
-	kmem_free(zc, sizeof (zfs_cmd_t));
+	kmem_free(zc, sizeof(zfs_cmd_t));
 
-	if ((pair = nvlist_next_nvpair(errors, NULL)) == NULL) {
+	if ((pair = nvlist_next_nvpair(errors, NULL)) == NULL)
+	{
 		nvlist_free(errors);
 		errors = NULL;
-	} else {
+	}
+	else
+	{
 		VERIFY(nvpair_value_int32(pair, &rv) == 0);
 	}
 
@@ -4705,31 +4973,36 @@ zfs_check_clearable(const char *dataset, nvlist_t *props, nvlist_t **errlist)
 static boolean_t
 propval_equals(nvpair_t *p1, nvpair_t *p2)
 {
-	if (nvpair_type(p1) == DATA_TYPE_NVLIST) {
+	if (nvpair_type(p1) == DATA_TYPE_NVLIST)
+	{
 		/* dsl_prop_get_all_impl() format */
 		nvlist_t *attrs;
 		VERIFY(nvpair_value_nvlist(p1, &attrs) == 0);
 		VERIFY(nvlist_lookup_nvpair(attrs, ZPROP_VALUE,
-		    &p1) == 0);
+									&p1) == 0);
 	}
 
-	if (nvpair_type(p2) == DATA_TYPE_NVLIST) {
+	if (nvpair_type(p2) == DATA_TYPE_NVLIST)
+	{
 		nvlist_t *attrs;
 		VERIFY(nvpair_value_nvlist(p2, &attrs) == 0);
 		VERIFY(nvlist_lookup_nvpair(attrs, ZPROP_VALUE,
-		    &p2) == 0);
+									&p2) == 0);
 	}
 
 	if (nvpair_type(p1) != nvpair_type(p2))
 		return (B_FALSE);
 
-	if (nvpair_type(p1) == DATA_TYPE_STRING) {
+	if (nvpair_type(p1) == DATA_TYPE_STRING)
+	{
 		char *valstr1, *valstr2;
 
 		VERIFY(nvpair_value_string(p1, (char **)&valstr1) == 0);
 		VERIFY(nvpair_value_string(p2, (char **)&valstr2) == 0);
 		return (strcmp(valstr1, valstr2) == 0);
-	} else {
+	}
+	else
+	{
 		uint64_t intval1, intval2;
 
 		VERIFY(nvpair_value_uint64(p1, &intval1) == 0);
@@ -4752,21 +5025,23 @@ props_reduce(nvlist_t *props, nvlist_t *origprops)
 		return; /* all props need to be received */
 
 	pair = nvlist_next_nvpair(props, NULL);
-	while (pair != NULL) {
+	while (pair != NULL)
+	{
 		const char *propname = nvpair_name(pair);
 		nvpair_t *match;
 
 		next_pair = nvlist_next_nvpair(props, pair);
 
 		if ((nvlist_lookup_nvpair(origprops, propname,
-		    &match) != 0) || !propval_equals(pair, match))
+								  &match) != 0) ||
+			!propval_equals(pair, match))
 			goto next; /* need to set received value */
 
 		/* don't clear the existing received value */
-		(void) nvlist_remove_nvpair(origprops, match);
+		(void)nvlist_remove_nvpair(origprops, match);
 		/* don't bother receiving the property */
-		(void) nvlist_remove_nvpair(props, pair);
-next:
+		(void)nvlist_remove_nvpair(props, pair);
+	next:
 		pair = next_pair;
 	}
 }
@@ -4797,25 +5072,28 @@ extract_delay_props(nvlist_t *props)
 		 * known, which is not possible prior to receipt of raw sends.
 		 */
 		ZFS_PROP_SHARESMB,
-		0
-	};
+		0};
 	int i;
 
 	VERIFY(nvlist_alloc(&delayprops, NV_UNIQUE_NAME, KM_SLEEP) == 0);
 
 	for (nvp = nvlist_next_nvpair(props, NULL); nvp != NULL;
-	    nvp = nvlist_next_nvpair(props, nvp)) {
+		 nvp = nvlist_next_nvpair(props, nvp))
+	{
 		/*
 		 * strcmp() is safe because zfs_prop_to_name() always returns
 		 * a bounded string.
 		 */
-		for (i = 0; delayable[i] != 0; i++) {
+		for (i = 0; delayable[i] != 0; i++)
+		{
 			if (strcmp(zfs_prop_to_name(delayable[i]),
-			    nvpair_name(nvp)) == 0) {
+					   nvpair_name(nvp)) == 0)
+			{
 				break;
 			}
 		}
-		if (delayable[i] != 0) {
+		if (delayable[i] != 0)
+		{
 			tmp = nvlist_prev_nvpair(props, nvp);
 			VERIFY(nvlist_add_nvpair(delayprops, nvp) == 0);
 			VERIFY(nvlist_remove_nvpair(props, nvp) == 0);
@@ -4823,7 +5101,8 @@ extract_delay_props(nvlist_t *props)
 		}
 	}
 
-	if (nvlist_empty(delayprops)) {
+	if (nvlist_empty(delayprops))
+	{
 		nvlist_free(delayprops);
 		delayprops = NULL;
 	}
@@ -4839,7 +5118,7 @@ zfs_allow_log_destroy(void *arg)
 		kmem_strfree(poolname);
 }
 
-#ifdef	ZFS_DEBUG
+#ifdef ZFS_DEBUG
 static boolean_t zfs_ioc_recv_inject_err;
 #endif
 
@@ -4849,10 +5128,10 @@ static boolean_t zfs_ioc_recv_inject_err;
  */
 static int
 zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
-    nvlist_t *localprops, nvlist_t *hidden_args, boolean_t force,
-    boolean_t resumable, int input_fd,
-    dmu_replay_record_t *begin_record, uint64_t *read_bytes,
-    uint64_t *errflags, nvlist_t **errors)
+				  nvlist_t *localprops, nvlist_t *hidden_args, boolean_t force,
+				  boolean_t resumable, int input_fd,
+				  dmu_replay_record_t *begin_record, uint64_t *read_bytes,
+				  uint64_t *errflags, nvlist_t **errors)
 {
 	dmu_recv_cookie_t drc;
 	int error = 0;
@@ -4877,8 +5156,8 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 
 	noff = off = zfs_file_off(input_fp);
 	error = dmu_recv_begin(tofs, tosnap, begin_record, force,
-	    resumable, localprops, hidden_args, origin, &drc, input_fp,
-	    &off);
+						   resumable, localprops, hidden_args, origin, &drc, input_fp,
+						   &off);
 	if (error != 0)
 		goto out;
 	tofs_was_redacted = dsl_get_redacted(drc.drc_ds);
@@ -4888,10 +5167,11 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 	 * to the new data. Note that we must call dmu_recv_stream() if
 	 * dmu_recv_begin() succeeds.
 	 */
-	if (recvprops != NULL && !drc.drc_newfs) {
+	if (recvprops != NULL && !drc.drc_newfs)
+	{
 		if (spa_version(dsl_dataset_get_spa(drc.drc_ds)) >=
-		    SPA_VERSION_RECVD_PROPS &&
-		    !dsl_prop_get_hasrecvd(tofs))
+				SPA_VERSION_RECVD_PROPS &&
+			!dsl_prop_get_hasrecvd(tofs))
 			first_recvd_props = B_TRUE;
 
 		/*
@@ -4899,7 +5179,8 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 		 * completely replace the existing received properties,
 		 * so stash away the existing ones.
 		 */
-		if (dsl_prop_get_received(tofs, &origrecvd) == 0) {
+		if (dsl_prop_get_received(tofs, &origrecvd) == 0)
+		{
 			nvlist_t *errlist = NULL;
 			/*
 			 * Don't bother writing a property if its value won't
@@ -4912,13 +5193,15 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 			if (!first_recvd_props)
 				props_reduce(recvprops, origrecvd);
 			if (zfs_check_clearable(tofs, origrecvd, &errlist) != 0)
-				(void) nvlist_merge(*errors, errlist, 0);
+				(void)nvlist_merge(*errors, errlist, 0);
 			nvlist_free(errlist);
 
 			if (clear_received_props(tofs, origrecvd,
-			    first_recvd_props ? NULL : recvprops) != 0)
+									 first_recvd_props ? NULL : recvprops) != 0)
 				*errflags |= ZPROP_ERR_NOCLEAR;
-		} else {
+		}
+		else
+		{
 			*errflags |= ZPROP_ERR_NOCLEAR;
 		}
 	}
@@ -4928,56 +5211,70 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 	 * we're doing the first receive after SPA_VERSION_RECVD_PROPS, in which
 	 * case "origrecvd" will take care of that.
 	 */
-	if (localprops != NULL && !drc.drc_newfs && !first_recvd_props) {
+	if (localprops != NULL && !drc.drc_newfs && !first_recvd_props)
+	{
 		objset_t *os;
-		if (dmu_objset_hold(tofs, FTAG, &os) == 0) {
-			if (dsl_prop_get_all(os, &origprops) != 0) {
+		if (dmu_objset_hold(tofs, FTAG, &os) == 0)
+		{
+			if (dsl_prop_get_all(os, &origprops) != 0)
+			{
 				*errflags |= ZPROP_ERR_NOCLEAR;
 			}
 			dmu_objset_rele(os, FTAG);
-		} else {
+		}
+		else
+		{
 			*errflags |= ZPROP_ERR_NOCLEAR;
 		}
 	}
 
-	if (recvprops != NULL) {
+	if (recvprops != NULL)
+	{
 		props_error = dsl_prop_set_hasrecvd(tofs);
 
-		if (props_error == 0) {
+		if (props_error == 0)
+		{
 			recv_delayprops = extract_delay_props(recvprops);
-			(void) zfs_set_prop_nvlist(tofs, ZPROP_SRC_RECEIVED,
-			    recvprops, *errors);
+			(void)zfs_set_prop_nvlist(tofs, ZPROP_SRC_RECEIVED,
+									  recvprops, *errors);
 		}
 	}
 
-	if (localprops != NULL) {
+	if (localprops != NULL)
+	{
 		nvlist_t *oprops = fnvlist_alloc();
 		nvlist_t *xprops = fnvlist_alloc();
 		nvpair_t *nvp = NULL;
 
-		while ((nvp = nvlist_next_nvpair(localprops, nvp)) != NULL) {
-			if (nvpair_type(nvp) == DATA_TYPE_BOOLEAN) {
+		while ((nvp = nvlist_next_nvpair(localprops, nvp)) != NULL)
+		{
+			if (nvpair_type(nvp) == DATA_TYPE_BOOLEAN)
+			{
 				/* -x property */
 				const char *name = nvpair_name(nvp);
 				zfs_prop_t prop = zfs_name_to_prop(name);
-				if (prop != ZPROP_INVAL) {
+				if (prop != ZPROP_INVAL)
+				{
 					if (!zfs_prop_inheritable(prop))
 						continue;
-				} else if (!zfs_prop_user(name))
+				}
+				else if (!zfs_prop_user(name))
 					continue;
 				fnvlist_add_boolean(xprops, name);
-			} else {
+			}
+			else
+			{
 				/* -o property=value */
 				fnvlist_add_nvpair(oprops, nvp);
 			}
 		}
 
 		local_delayprops = extract_delay_props(oprops);
-		(void) zfs_set_prop_nvlist(tofs, ZPROP_SRC_LOCAL,
-		    oprops, *errors);
+		(void)zfs_set_prop_nvlist(tofs, ZPROP_SRC_LOCAL,
+								  oprops, *errors);
 		inherited_delayprops = extract_delay_props(xprops);
-		(void) zfs_set_prop_nvlist(tofs, ZPROP_SRC_INHERITED,
-		    xprops, *errors);
+		(void)zfs_set_prop_nvlist(tofs, ZPROP_SRC_INHERITED,
+								  xprops, *errors);
 
 		nvlist_free(oprops);
 		nvlist_free(xprops);
@@ -4985,17 +5282,19 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 
 	error = dmu_recv_stream(&drc, &off);
 
-	if (error == 0) {
+	if (error == 0)
+	{
 		zfsvfs_t *zfsvfs = NULL;
 		zvol_state_handle_t *zv = NULL;
 
-		if (getzfsvfs(tofs, &zfsvfs) == 0) {
+		if (getzfsvfs(tofs, &zfsvfs) == 0)
+		{
 			/* online recv */
 			dsl_dataset_t *ds;
 			int end_err;
 			boolean_t stream_is_redacted = DMU_GET_FEATUREFLAGS(
-			    begin_record->drr_u.drr_begin.
-			    drr_versioninfo) & DMU_BACKUP_FEATURE_REDACTED;
+											   begin_record->drr_u.drr_begin.drr_versioninfo) &
+										   DMU_BACKUP_FEATURE_REDACTED;
 
 			ds = dmu_objset_ds(zfsvfs->z_os);
 			error = zfs_suspend_fs(zfsvfs);
@@ -5010,32 +5309,42 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 			 * dataset.  Otherwise, resume the filesystem.
 			 */
 			if (error == 0 && !drc.drc_newfs &&
-			    stream_is_redacted && !tofs_was_redacted) {
+				stream_is_redacted && !tofs_was_redacted)
+			{
 				error = zfs_end_fs(zfsvfs, ds);
-			} else if (error == 0) {
+			}
+			else if (error == 0)
+			{
 				error = zfs_resume_fs(zfsvfs, ds);
 			}
 			error = error ? error : end_err;
 			zfs_vfs_rele(zfsvfs);
-		} else if ((zv = zvol_suspend(tofs)) != NULL) {
+		}
+		else if ((zv = zvol_suspend(tofs)) != NULL)
+		{
 			error = dmu_recv_end(&drc, zvol_tag(zv));
 			zvol_resume(zv);
-		} else {
+		}
+		else
+		{
 			error = dmu_recv_end(&drc, NULL);
 		}
 
 		/* Set delayed properties now, after we're done receiving. */
-		if (recv_delayprops != NULL && error == 0) {
-			(void) zfs_set_prop_nvlist(tofs, ZPROP_SRC_RECEIVED,
-			    recv_delayprops, *errors);
+		if (recv_delayprops != NULL && error == 0)
+		{
+			(void)zfs_set_prop_nvlist(tofs, ZPROP_SRC_RECEIVED,
+									  recv_delayprops, *errors);
 		}
-		if (local_delayprops != NULL && error == 0) {
-			(void) zfs_set_prop_nvlist(tofs, ZPROP_SRC_LOCAL,
-			    local_delayprops, *errors);
+		if (local_delayprops != NULL && error == 0)
+		{
+			(void)zfs_set_prop_nvlist(tofs, ZPROP_SRC_LOCAL,
+									  local_delayprops, *errors);
 		}
-		if (inherited_delayprops != NULL && error == 0) {
-			(void) zfs_set_prop_nvlist(tofs, ZPROP_SRC_INHERITED,
-			    inherited_delayprops, *errors);
+		if (inherited_delayprops != NULL && error == 0)
+		{
+			(void)zfs_set_prop_nvlist(tofs, ZPROP_SRC_INHERITED,
+									  inherited_delayprops, *errors);
 		}
 	}
 
@@ -5048,22 +5357,26 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 	 * Since zfs_ioc_recv_inject_err is only in DEBUG kernels,
 	 * using ASSERT() will be just like a VERIFY.
 	 */
-	if (recv_delayprops != NULL) {
+	if (recv_delayprops != NULL)
+	{
 		ASSERT(nvlist_merge(recvprops, recv_delayprops, 0) == 0);
 		nvlist_free(recv_delayprops);
 	}
-	if (local_delayprops != NULL) {
+	if (local_delayprops != NULL)
+	{
 		ASSERT(nvlist_merge(localprops, local_delayprops, 0) == 0);
 		nvlist_free(local_delayprops);
 	}
-	if (inherited_delayprops != NULL) {
+	if (inherited_delayprops != NULL)
+	{
 		ASSERT(nvlist_merge(localprops, inherited_delayprops, 0) == 0);
 		nvlist_free(inherited_delayprops);
 	}
 	*read_bytes = off - noff;
 
-#ifdef	ZFS_DEBUG
-	if (zfs_ioc_recv_inject_err) {
+#ifdef ZFS_DEBUG
+	if (zfs_ioc_recv_inject_err)
+	{
 		zfs_ioc_recv_inject_err = B_FALSE;
 		error = 1;
 	}
@@ -5072,19 +5385,24 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 	/*
 	 * On error, restore the original props.
 	 */
-	if (error != 0 && recvprops != NULL && !drc.drc_newfs) {
-		if (clear_received_props(tofs, recvprops, NULL) != 0) {
+	if (error != 0 && recvprops != NULL && !drc.drc_newfs)
+	{
+		if (clear_received_props(tofs, recvprops, NULL) != 0)
+		{
 			/*
 			 * We failed to clear the received properties.
 			 * Since we may have left a $recvd value on the
 			 * system, we can't clear the $hasrecvd flag.
 			 */
 			*errflags |= ZPROP_ERR_NORESTORE;
-		} else if (first_recvd_props) {
+		}
+		else if (first_recvd_props)
+		{
 			dsl_prop_unset_hasrecvd(tofs);
 		}
 
-		if (origrecvd == NULL && !drc.drc_newfs) {
+		if (origrecvd == NULL && !drc.drc_newfs)
+		{
 			/* We failed to stash the original properties. */
 			*errflags |= ZPROP_ERR_NORESTORE;
 		}
@@ -5096,9 +5414,9 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 		 * first new-style receive.
 		 */
 		if (origrecvd != NULL &&
-		    zfs_set_prop_nvlist(tofs, (first_recvd_props ?
-		    ZPROP_SRC_LOCAL : ZPROP_SRC_RECEIVED),
-		    origrecvd, NULL) != 0) {
+			zfs_set_prop_nvlist(tofs, (first_recvd_props ? ZPROP_SRC_LOCAL : ZPROP_SRC_RECEIVED),
+								origrecvd, NULL) != 0)
+		{
 			/*
 			 * We stashed the original properties but failed to
 			 * restore them.
@@ -5107,12 +5425,14 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 		}
 	}
 	if (error != 0 && localprops != NULL && !drc.drc_newfs &&
-	    !first_recvd_props) {
+		!first_recvd_props)
+	{
 		nvlist_t *setprops;
 		nvlist_t *inheritprops;
 		nvpair_t *nvp;
 
-		if (origprops == NULL) {
+		if (origprops == NULL)
+		{
 			/* We failed to stash the original properties. */
 			*errflags |= ZPROP_ERR_NORESTORE;
 			goto out;
@@ -5122,12 +5442,14 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 		setprops = fnvlist_alloc();
 		inheritprops = fnvlist_alloc();
 		nvp = NULL;
-		while ((nvp = nvlist_next_nvpair(localprops, nvp)) != NULL) {
+		while ((nvp = nvlist_next_nvpair(localprops, nvp)) != NULL)
+		{
 			const char *name = nvpair_name(nvp);
 			const char *source;
 			nvlist_t *attrs;
 
-			if (!nvlist_exists(origprops, name)) {
+			if (!nvlist_exists(origprops, name))
+			{
 				/*
 				 * Property was not present or was explicitly
 				 * inherited before the receive, restore this.
@@ -5142,20 +5464,23 @@ zfs_ioc_recv_impl(char *tofs, char *tosnap, char *origin, nvlist_t *recvprops,
 			if (strcmp(source, ZPROP_SOURCE_VAL_RECVD) == 0)
 				continue;
 
-			if (strcmp(source, tofs) == 0) {
+			if (strcmp(source, tofs) == 0)
+			{
 				/* Property was locally set */
 				fnvlist_add_nvlist(setprops, name, attrs);
-			} else {
+			}
+			else
+			{
 				/* Property was implicitly inherited */
 				fnvlist_add_boolean(inheritprops, name);
 			}
 		}
 
 		if (zfs_set_prop_nvlist(tofs, ZPROP_SRC_LOCAL, setprops,
-		    NULL) != 0)
+								NULL) != 0)
 			*errflags |= ZPROP_ERR_NORESTORE;
 		if (zfs_set_prop_nvlist(tofs, ZPROP_SRC_INHERITED, inheritprops,
-		    NULL) != 0)
+								NULL) != 0)
 			*errflags |= ZPROP_ERR_NORESTORE;
 
 		nvlist_free(setprops);
@@ -5202,22 +5527,22 @@ zfs_ioc_recv(zfs_cmd_t *zc)
 	int error = 0;
 
 	if (dataset_namecheck(zc->zc_value, NULL, NULL) != 0 ||
-	    strchr(zc->zc_value, '@') == NULL ||
-	    strchr(zc->zc_value, '%'))
+		strchr(zc->zc_value, '@') == NULL ||
+		strchr(zc->zc_value, '%'))
 		return (SET_ERROR(EINVAL));
 
-	(void) strlcpy(tofs, zc->zc_value, sizeof (tofs));
+	(void)strlcpy(tofs, zc->zc_value, sizeof(tofs));
 	tosnap = strchr(tofs, '@');
 	*tosnap++ = '\0';
 
 	if (zc->zc_nvlist_src != 0 &&
-	    (error = get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-	    zc->zc_iflags, &recvdprops)) != 0)
+		(error = get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
+							zc->zc_iflags, &recvdprops)) != 0)
 		return (error);
 
 	if (zc->zc_nvlist_conf != 0 &&
-	    (error = get_nvlist(zc->zc_nvlist_conf, zc->zc_nvlist_conf_size,
-	    zc->zc_iflags, &localprops)) != 0)
+		(error = get_nvlist(zc->zc_nvlist_conf, zc->zc_nvlist_conf_size,
+							zc->zc_iflags, &localprops)) != 0)
 		return (error);
 
 	if (zc->zc_string[0])
@@ -5228,8 +5553,8 @@ zfs_ioc_recv(zfs_cmd_t *zc)
 	begin_record.drr_u.drr_begin = zc->zc_begin_record;
 
 	error = zfs_ioc_recv_impl(tofs, tosnap, origin, recvdprops, localprops,
-	    NULL, zc->zc_guid, B_FALSE, zc->zc_cookie, &begin_record,
-	    &zc->zc_cookie, &zc->zc_obj, &errors);
+							  NULL, zc->zc_guid, B_FALSE, zc->zc_cookie, &begin_record,
+							  &zc->zc_cookie, &zc->zc_obj, &errors);
 	nvlist_free(recvdprops);
 	nvlist_free(localprops);
 
@@ -5238,8 +5563,9 @@ zfs_ioc_recv(zfs_cmd_t *zc)
 	 * errors to the caller.
 	 */
 	if (zc->zc_nvlist_dst_size != 0 && errors != NULL &&
-	    (nvlist_smush(errors, zc->zc_nvlist_dst_size) != 0 ||
-	    put_nvlist(zc, errors) != 0)) {
+		(nvlist_smush(errors, zc->zc_nvlist_dst_size) != 0 ||
+		 put_nvlist(zc, errors) != 0))
+	{
 		/*
 		 * Caller made zc->zc_nvlist_dst less than the minimum expected
 		 * size or supplied an invalid address.
@@ -5274,17 +5600,17 @@ zfs_ioc_recv(zfs_cmd_t *zc)
  * }
  */
 static const zfs_ioc_key_t zfs_keys_recv_new[] = {
-	{"snapname",		DATA_TYPE_STRING,	0},
-	{"props",		DATA_TYPE_NVLIST,	ZK_OPTIONAL},
-	{"localprops",		DATA_TYPE_NVLIST,	ZK_OPTIONAL},
-	{"origin",		DATA_TYPE_STRING,	ZK_OPTIONAL},
-	{"begin_record",	DATA_TYPE_BYTE_ARRAY,	0},
-	{"input_fd",		DATA_TYPE_INT32,	0},
-	{"force",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"resumable",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"cleanup_fd",		DATA_TYPE_INT32,	ZK_OPTIONAL},
-	{"action_handle",	DATA_TYPE_UINT64,	ZK_OPTIONAL},
-	{"hidden_args",		DATA_TYPE_NVLIST,	ZK_OPTIONAL},
+	{"snapname", DATA_TYPE_STRING, 0},
+	{"props", DATA_TYPE_NVLIST, ZK_OPTIONAL},
+	{"localprops", DATA_TYPE_NVLIST, ZK_OPTIONAL},
+	{"origin", DATA_TYPE_STRING, ZK_OPTIONAL},
+	{"begin_record", DATA_TYPE_BYTE_ARRAY, 0},
+	{"input_fd", DATA_TYPE_INT32, 0},
+	{"force", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"resumable", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"cleanup_fd", DATA_TYPE_INT32, ZK_OPTIONAL},
+	{"action_handle", DATA_TYPE_UINT64, ZK_OPTIONAL},
+	{"hidden_args", DATA_TYPE_NVLIST, ZK_OPTIONAL},
 };
 
 static int
@@ -5310,11 +5636,11 @@ zfs_ioc_recv_new(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 	snapname = fnvlist_lookup_string(innvl, "snapname");
 
 	if (dataset_namecheck(snapname, NULL, NULL) != 0 ||
-	    strchr(snapname, '@') == NULL ||
-	    strchr(snapname, '%'))
+		strchr(snapname, '@') == NULL ||
+		strchr(snapname, '%'))
 		return (SET_ERROR(EINVAL));
 
-	(void) strlcpy(tofs, snapname, sizeof (tofs));
+	(void)strlcpy(tofs, snapname, sizeof(tofs));
 	tosnap = strchr(tofs, '@');
 	*tosnap++ = '\0';
 
@@ -5323,8 +5649,8 @@ zfs_ioc_recv_new(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 		return (error);
 
 	error = nvlist_lookup_byte_array(innvl, "begin_record",
-	    (uchar_t **)&begin_record, &begin_record_size);
-	if (error != 0 || begin_record_size != sizeof (*begin_record))
+									 (uchar_t **)&begin_record, &begin_record_size);
+	if (error != 0 || begin_record_size != sizeof(*begin_record))
 		return (SET_ERROR(EINVAL));
 
 	input_fd = fnvlist_lookup_int32(innvl, "input_fd");
@@ -5346,8 +5672,8 @@ zfs_ioc_recv_new(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 		return (error);
 
 	error = zfs_ioc_recv_impl(tofs, tosnap, origin, recvprops, localprops,
-	    hidden_args, force, resumable, input_fd, begin_record,
-	    &read_bytes, &errflags, &errors);
+							  hidden_args, force, resumable, input_fd, begin_record,
+							  &read_bytes, &errflags, &errors);
 
 	fnvlist_add_uint64(outnvl, "read_bytes", read_bytes);
 	fnvlist_add_uint64(outnvl, "error_flags", errflags);
@@ -5360,11 +5686,12 @@ zfs_ioc_recv_new(const char *fsname, nvlist_t *innvl, nvlist_t *outnvl)
 	return (error);
 }
 
-typedef struct dump_bytes_io {
-	zfs_file_t	*dbi_fp;
-	caddr_t		dbi_buf;
-	int		dbi_len;
-	int		dbi_err;
+typedef struct dump_bytes_io
+{
+	zfs_file_t *dbi_fp;
+	caddr_t dbi_buf;
+	int dbi_len;
+	int dbi_err;
 } dump_bytes_io_t;
 
 static void
@@ -5399,7 +5726,7 @@ dump_bytes(objset_t *os, void *buf, int len, void *arg)
 	 * them and they are used in vdev_file.c for a similar purpose.
 	 */
 	spa_taskq_dispatch_sync(dmu_objset_spa(os), ZIO_TYPE_FREE,
-	    ZIO_TASKQ_ISSUE, dump_bytes_cb, &dbi, TQ_SLEEP);
+							ZIO_TASKQ_ISSUE, dump_bytes_cb, &dbi, TQ_SLEEP);
 #endif /* HAVE_LARGE_STACKS */
 
 	return (dbi.dbi_err);
@@ -5434,7 +5761,8 @@ zfs_ioc_send(zfs_cmd_t *zc)
 	boolean_t rawok = (zc->zc_flags & 0x8);
 	boolean_t savedok = (zc->zc_flags & 0x10);
 
-	if (zc->zc_obj != 0) {
+	if (zc->zc_obj != 0)
+	{
 		dsl_pool_t *dp;
 		dsl_dataset_t *tosnap;
 
@@ -5443,19 +5771,21 @@ zfs_ioc_send(zfs_cmd_t *zc)
 			return (error);
 
 		error = dsl_dataset_hold_obj(dp, zc->zc_sendobj, FTAG, &tosnap);
-		if (error != 0) {
+		if (error != 0)
+		{
 			dsl_pool_rele(dp, FTAG);
 			return (error);
 		}
 
 		if (dsl_dir_is_clone(tosnap->ds_dir))
 			zc->zc_fromobj =
-			    dsl_dir_phys(tosnap->ds_dir)->dd_origin_obj;
+				dsl_dir_phys(tosnap->ds_dir)->dd_origin_obj;
 		dsl_dataset_rele(tosnap, FTAG);
 		dsl_pool_rele(dp, FTAG);
 	}
 
-	if (estimate) {
+	if (estimate)
+	{
 		dsl_pool_t *dp;
 		dsl_dataset_t *tosnap;
 		dsl_dataset_t *fromsnap = NULL;
@@ -5465,16 +5795,19 @@ zfs_ioc_send(zfs_cmd_t *zc)
 			return (error);
 
 		error = dsl_dataset_hold_obj(dp, zc->zc_sendobj,
-		    FTAG, &tosnap);
-		if (error != 0) {
+									 FTAG, &tosnap);
+		if (error != 0)
+		{
 			dsl_pool_rele(dp, FTAG);
 			return (error);
 		}
 
-		if (zc->zc_fromobj != 0) {
+		if (zc->zc_fromobj != 0)
+		{
 			error = dsl_dataset_hold_obj(dp, zc->zc_fromobj,
-			    FTAG, &fromsnap);
-			if (error != 0) {
+										 FTAG, &fromsnap);
+			if (error != 0)
+			{
 				dsl_dataset_rele(tosnap, FTAG);
 				dsl_pool_rele(dp, FTAG);
 				return (error);
@@ -5482,13 +5815,15 @@ zfs_ioc_send(zfs_cmd_t *zc)
 		}
 
 		error = dmu_send_estimate_fast(tosnap, fromsnap, NULL,
-		    compressok || rawok, savedok, &zc->zc_objset_type);
+									   compressok || rawok, savedok, &zc->zc_objset_type);
 
 		if (fromsnap != NULL)
 			dsl_dataset_rele(fromsnap, FTAG);
 		dsl_dataset_rele(tosnap, FTAG);
 		dsl_pool_rele(dp, FTAG);
-	} else {
+	}
+	else
+	{
 		zfs_file_t *fp;
 		dmu_send_outparams_t out = {0};
 
@@ -5500,8 +5835,8 @@ zfs_ioc_send(zfs_cmd_t *zc)
 		out.dso_arg = fp;
 		out.dso_dryrun = B_FALSE;
 		error = dmu_send_obj(zc->zc_name, zc->zc_sendobj,
-		    zc->zc_fromobj, embedok, large_block_ok, compressok,
-		    rawok, savedok, zc->zc_cookie, &off, &out);
+							 zc->zc_fromobj, embedok, large_block_ok, compressok,
+							 rawok, savedok, zc->zc_cookie, &off, &out);
 
 		zfs_file_put(fp);
 	}
@@ -5530,7 +5865,8 @@ zfs_ioc_send_progress(zfs_cmd_t *zc)
 		return (error);
 
 	error = dsl_dataset_hold(dp, zc->zc_name, FTAG, &ds);
-	if (error != 0) {
+	if (error != 0)
+	{
 		dsl_pool_rele(dp, FTAG);
 		return (error);
 	}
@@ -5545,18 +5881,22 @@ zfs_ioc_send_progress(zfs_cmd_t *zc)
 	 */
 
 	for (dsp = list_head(&ds->ds_sendstreams); dsp != NULL;
-	    dsp = list_next(&ds->ds_sendstreams, dsp)) {
+		 dsp = list_next(&ds->ds_sendstreams, dsp))
+	{
 		if (dsp->dss_outfd == zc->zc_cookie &&
-		    zfs_proc_is_caller(dsp->dss_proc))
+			zfs_proc_is_caller(dsp->dss_proc))
 			break;
 	}
 
-	if (dsp != NULL) {
+	if (dsp != NULL)
+	{
 		zc->zc_cookie = atomic_cas_64((volatile uint64_t *)dsp->dss_off,
-		    0, 0);
+									  0, 0);
 		/* This is the closest thing we have to atomic_read_64. */
 		zc->zc_objset_type = atomic_cas_64(&dsp->dss_blocks, 0, 0);
-	} else {
+	}
+	else
+	{
 		error = SET_ERROR(ENOENT);
 	}
 
@@ -5572,7 +5912,7 @@ zfs_ioc_inject_fault(zfs_cmd_t *zc)
 	int id, error;
 
 	error = zio_inject_fault(zc->zc_name, (int)zc->zc_guid, &id,
-	    &zc->zc_inject_record);
+							 &zc->zc_inject_record);
 
 	if (error == 0)
 		zc->zc_guid = (uint64_t)id;
@@ -5592,8 +5932,8 @@ zfs_ioc_inject_list_next(zfs_cmd_t *zc)
 	int id = (int)zc->zc_guid;
 	int error;
 
-	error = zio_inject_list_next(&id, zc->zc_name, sizeof (zc->zc_name),
-	    &zc->zc_inject_record);
+	error = zio_inject_list_next(&id, zc->zc_name, sizeof(zc->zc_name),
+								 &zc->zc_inject_record);
 
 	zc->zc_guid = id;
 
@@ -5611,7 +5951,7 @@ zfs_ioc_error_log(zfs_cmd_t *zc)
 		return (error);
 
 	error = spa_get_errlog(spa, (void *)(uintptr_t)zc->zc_nvlist_dst,
-	    &count);
+						   &count);
 	if (error == 0)
 		zc->zc_nvlist_dst_size = count;
 	else
@@ -5634,20 +5974,25 @@ zfs_ioc_clear(zfs_cmd_t *zc)
 	 */
 	mutex_enter(&spa_namespace_lock);
 	spa = spa_lookup(zc->zc_name);
-	if (spa == NULL) {
+	if (spa == NULL)
+	{
 		mutex_exit(&spa_namespace_lock);
 		return (SET_ERROR(EIO));
 	}
-	if (spa_get_log_state(spa) == SPA_LOG_MISSING) {
+	if (spa_get_log_state(spa) == SPA_LOG_MISSING)
+	{
 		/* we need to let spa_open/spa_load clear the chains */
 		spa_set_log_state(spa, SPA_LOG_CLEAR);
 	}
 	spa->spa_last_open_failed = 0;
 	mutex_exit(&spa_namespace_lock);
 
-	if (zc->zc_cookie & ZPOOL_NO_REWIND) {
+	if (zc->zc_cookie & ZPOOL_NO_REWIND)
+	{
 		error = spa_open(zc->zc_name, &spa, FTAG);
-	} else {
+	}
+	else
+	{
 		nvlist_t *policy;
 		nvlist_t *config = NULL;
 
@@ -5655,10 +6000,12 @@ zfs_ioc_clear(zfs_cmd_t *zc)
 			return (SET_ERROR(EINVAL));
 
 		if ((error = get_nvlist(zc->zc_nvlist_src,
-		    zc->zc_nvlist_src_size, zc->zc_iflags, &policy)) == 0) {
+								zc->zc_nvlist_src_size, zc->zc_iflags, &policy)) == 0)
+		{
 			error = spa_open_rewind(zc->zc_name, &spa, FTAG,
-			    policy, &config);
-			if (config != NULL) {
+									policy, &config);
+			if (config != NULL)
+			{
 				int err;
 
 				if ((err = put_nvlist(zc, config)) != 0)
@@ -5681,13 +6028,17 @@ zfs_ioc_clear(zfs_cmd_t *zc)
 
 	spa_vdev_state_enter(spa, SCL_NONE);
 
-	if (zc->zc_guid == 0) {
+	if (zc->zc_guid == 0)
+	{
 		vd = NULL;
-	} else {
+	}
+	else
+	{
 		vd = spa_lookup_by_guid(spa, zc->zc_guid, B_TRUE);
-		if (vd == NULL) {
+		if (vd == NULL)
+		{
 			error = SET_ERROR(ENODEV);
-			(void) spa_vdev_state_exit(spa, NULL, error);
+			(void)spa_vdev_state_exit(spa, NULL, error);
 			spa_close(spa, FTAG);
 			return (error);
 		}
@@ -5695,8 +6046,7 @@ zfs_ioc_clear(zfs_cmd_t *zc)
 
 	vdev_clear(spa, vd);
 
-	(void) spa_vdev_state_exit(spa, spa_suspended(spa) ?
-	    NULL : spa->spa_root_vdev, 0);
+	(void)spa_vdev_state_exit(spa, spa_suspended(spa) ? NULL : spa->spa_root_vdev, 0);
 
 	/*
 	 * Resume any suspended I/Os.
@@ -5720,7 +6070,7 @@ zfs_ioc_clear(zfs_cmd_t *zc)
  * outnvl is unused
  */
 static const zfs_ioc_key_t zfs_keys_pool_reopen[] = {
-	{"scrub_restart",	DATA_TYPE_BOOLEAN_VALUE,	ZK_OPTIONAL},
+	{"scrub_restart", DATA_TYPE_BOOLEAN_VALUE, ZK_OPTIONAL},
 };
 
 /* ARGSUSED */
@@ -5731,9 +6081,10 @@ zfs_ioc_pool_reopen(const char *pool, nvlist_t *innvl, nvlist_t *outnvl)
 	int error;
 	boolean_t rc, scrub_restart = B_TRUE;
 
-	if (innvl) {
+	if (innvl)
+	{
 		error = nvlist_lookup_boolean_value(innvl,
-		    "scrub_restart", &rc);
+											"scrub_restart", &rc);
 		if (error == 0)
 			scrub_restart = rc;
 	}
@@ -5752,11 +6103,11 @@ zfs_ioc_pool_reopen(const char *pool, nvlist_t *innvl, nvlist_t *outnvl)
 	 */
 
 	spa->spa_scrub_reopen = (!scrub_restart &&
-	    dsl_scan_scrubbing(spa->spa_dsl_pool));
+							 dsl_scan_scrubbing(spa->spa_dsl_pool));
 	vdev_reopen(spa->spa_root_vdev);
 	spa->spa_scrub_reopen = B_FALSE;
 
-	(void) spa_vdev_state_exit(spa, NULL, 0);
+	(void)spa_vdev_state_exit(spa, NULL, 0);
 	spa_close(spa, FTAG);
 	return (0);
 }
@@ -5777,9 +6128,9 @@ zfs_ioc_promote(zfs_cmd_t *zc)
 	char *cp;
 	int error;
 
-	zc->zc_name[sizeof (zc->zc_name) - 1] = '\0';
+	zc->zc_name[sizeof(zc->zc_name) - 1] = '\0';
 	if (dataset_namecheck(zc->zc_name, NULL, NULL) != 0 ||
-	    strchr(zc->zc_name, '%'))
+		strchr(zc->zc_name, '%'))
 		return (SET_ERROR(EINVAL));
 
 	error = dsl_pool_hold(zc->zc_name, FTAG, &dp);
@@ -5787,20 +6138,23 @@ zfs_ioc_promote(zfs_cmd_t *zc)
 		return (error);
 
 	error = dsl_dataset_hold(dp, zc->zc_name, FTAG, &ds);
-	if (error != 0) {
+	if (error != 0)
+	{
 		dsl_pool_rele(dp, FTAG);
 		return (error);
 	}
 
-	if (!dsl_dir_is_clone(ds->ds_dir)) {
+	if (!dsl_dir_is_clone(ds->ds_dir))
+	{
 		dsl_dataset_rele(ds, FTAG);
 		dsl_pool_rele(dp, FTAG);
 		return (SET_ERROR(EINVAL));
 	}
 
 	error = dsl_dataset_hold_obj(dp,
-	    dsl_dir_phys(ds->ds_dir)->dd_origin_obj, FTAG, &ods);
-	if (error != 0) {
+								 dsl_dir_phys(ds->ds_dir)->dd_origin_obj, FTAG, &ods);
+	if (error != 0)
+	{
 		dsl_dataset_rele(ds, FTAG);
 		dsl_pool_rele(dp, FTAG);
 		return (error);
@@ -5818,8 +6172,8 @@ zfs_ioc_promote(zfs_cmd_t *zc)
 	cp = strchr(origin, '@');
 	if (cp)
 		*cp = '\0';
-	(void) dmu_objset_find(origin,
-	    zfs_unmount_snap_cb, NULL, DS_FIND_SNAPSHOTS);
+	(void)dmu_objset_find(origin,
+						  zfs_unmount_snap_cb, NULL, DS_FIND_SNAPSHOTS);
 	return (dsl_dataset_promote(zc->zc_name, zc->zc_string));
 }
 
@@ -5849,7 +6203,7 @@ zfs_ioc_userspace_one(zfs_cmd_t *zc)
 		return (error);
 
 	error = zfs_userspace_one(zfsvfs,
-	    zc->zc_objset_type, zc->zc_value, zc->zc_guid, &zc->zc_cookie);
+							  zc->zc_objset_type, zc->zc_value, zc->zc_guid, &zc->zc_cookie);
 	zfsvfs_rele(zfsvfs, FTAG);
 
 	return (error);
@@ -5882,12 +6236,13 @@ zfs_ioc_userspace_many(zfs_cmd_t *zc)
 	void *buf = vmem_alloc(bufsize, KM_SLEEP);
 
 	error = zfs_userspace_many(zfsvfs, zc->zc_objset_type, &zc->zc_cookie,
-	    buf, &zc->zc_nvlist_dst_size);
+							   buf, &zc->zc_nvlist_dst_size);
 
-	if (error == 0) {
+	if (error == 0)
+	{
 		error = xcopyout(buf,
-		    (void *)(uintptr_t)zc->zc_nvlist_dst,
-		    zc->zc_nvlist_dst_size);
+						 (void *)(uintptr_t)zc->zc_nvlist_dst,
+						 zc->zc_nvlist_dst_size);
 	}
 	vmem_free(buf, bufsize);
 	zfsvfs_rele(zfsvfs, FTAG);
@@ -5908,8 +6263,10 @@ zfs_ioc_userspace_upgrade(zfs_cmd_t *zc)
 	int error = 0;
 	zfsvfs_t *zfsvfs;
 
-	if (getzfsvfs(zc->zc_name, &zfsvfs) == 0) {
-		if (!dmu_objset_userused_enabled(zfsvfs->z_os)) {
+	if (getzfsvfs(zc->zc_name, &zfsvfs) == 0)
+	{
+		if (!dmu_objset_userused_enabled(zfsvfs->z_os))
+		{
 			/*
 			 * If userused is not enabled, it may be because the
 			 * objset needs to be closed & reopened (to grow the
@@ -5919,34 +6276,41 @@ zfs_ioc_userspace_upgrade(zfs_cmd_t *zc)
 
 			ds = dmu_objset_ds(zfsvfs->z_os);
 			error = zfs_suspend_fs(zfsvfs);
-			if (error == 0) {
+			if (error == 0)
+			{
 				dmu_objset_refresh_ownership(ds, &newds,
-				    B_TRUE, zfsvfs);
+											 B_TRUE, zfsvfs);
 				error = zfs_resume_fs(zfsvfs, newds);
 			}
 		}
-		if (error == 0) {
+		if (error == 0)
+		{
 			mutex_enter(&zfsvfs->z_os->os_upgrade_lock);
-			if (zfsvfs->z_os->os_upgrade_id == 0) {
+			if (zfsvfs->z_os->os_upgrade_id == 0)
+			{
 				/* clear potential error code and retry */
 				zfsvfs->z_os->os_upgrade_status = 0;
 				mutex_exit(&zfsvfs->z_os->os_upgrade_lock);
 
 				dsl_pool_config_enter(
-				    dmu_objset_pool(zfsvfs->z_os), FTAG);
+					dmu_objset_pool(zfsvfs->z_os), FTAG);
 				dmu_objset_userspace_upgrade(zfsvfs->z_os);
 				dsl_pool_config_exit(
-				    dmu_objset_pool(zfsvfs->z_os), FTAG);
-			} else {
+					dmu_objset_pool(zfsvfs->z_os), FTAG);
+			}
+			else
+			{
 				mutex_exit(&zfsvfs->z_os->os_upgrade_lock);
 			}
 
 			taskq_wait_id(zfsvfs->z_os->os_spa->spa_upgrade_taskq,
-			    zfsvfs->z_os->os_upgrade_id);
+						  zfsvfs->z_os->os_upgrade_id);
 			error = zfsvfs->z_os->os_upgrade_status;
 		}
 		zfs_vfs_rele(zfsvfs);
-	} else {
+	}
+	else
+	{
 		objset_t *os;
 
 		/* XXX kind of reading contents without owning */
@@ -5955,13 +6319,16 @@ zfs_ioc_userspace_upgrade(zfs_cmd_t *zc)
 			return (error);
 
 		mutex_enter(&os->os_upgrade_lock);
-		if (os->os_upgrade_id == 0) {
+		if (os->os_upgrade_id == 0)
+		{
 			/* clear potential error code and retry */
 			os->os_upgrade_status = 0;
 			mutex_exit(&os->os_upgrade_lock);
 
 			dmu_objset_userspace_upgrade(os);
-		} else {
+		}
+		else
+		{
 			mutex_exit(&os->os_upgrade_lock);
 		}
 
@@ -5971,7 +6338,7 @@ zfs_ioc_userspace_upgrade(zfs_cmd_t *zc)
 		error = os->os_upgrade_status;
 
 		dsl_dataset_rele_flags(dmu_objset_ds(os), DS_HOLD_FLAG_DECRYPT,
-		    FTAG);
+							   FTAG);
 	}
 	return (error);
 }
@@ -5994,15 +6361,19 @@ zfs_ioc_id_quota_upgrade(zfs_cmd_t *zc)
 		return (error);
 
 	if (dmu_objset_userobjspace_upgradable(os) ||
-	    dmu_objset_projectquota_upgradable(os)) {
+		dmu_objset_projectquota_upgradable(os))
+	{
 		mutex_enter(&os->os_upgrade_lock);
-		if (os->os_upgrade_id == 0) {
+		if (os->os_upgrade_id == 0)
+		{
 			/* clear potential error code and retry */
 			os->os_upgrade_status = 0;
 			mutex_exit(&os->os_upgrade_lock);
 
 			dmu_objset_id_quota_upgrade(os);
-		} else {
+		}
+		else
+		{
 			mutex_exit(&os->os_upgrade_lock);
 		}
 
@@ -6010,7 +6381,9 @@ zfs_ioc_id_quota_upgrade(zfs_cmd_t *zc)
 
 		taskq_wait_id(os->os_spa->spa_upgrade_taskq, os->os_upgrade_id);
 		error = os->os_upgrade_status;
-	} else {
+	}
+	else
+	{
 		dsl_pool_rele(dmu_objset_pool(os), FTAG);
 	}
 
@@ -6026,8 +6399,7 @@ zfs_ioc_share(zfs_cmd_t *zc)
 }
 
 ace_t full_access[] = {
-	{(uid_t)-1, ACE_ALL_PERMS, ACE_EVERYONE, 0}
-};
+	{(uid_t)-1, ACE_ALL_PERMS, ACE_EVERYONE, 0}};
 
 /*
  * inputs:
@@ -6074,14 +6446,14 @@ zfs_ioc_tmp_snapshot(zfs_cmd_t *zc)
 		return (SET_ERROR(EBADF));
 
 	snap_name = kmem_asprintf("%s-%016llx", zc->zc_value,
-	    (u_longlong_t)ddi_get_lbolt64());
+							  (u_longlong_t)ddi_get_lbolt64());
 	hold_name = kmem_asprintf("%%%s", zc->zc_value);
 
 	int error = dsl_dataset_snapshot_tmp(zc->zc_name, snap_name, minor,
-	    hold_name);
+										 hold_name);
 	if (error == 0)
-		(void) strlcpy(zc->zc_value, snap_name,
-		    sizeof (zc->zc_value));
+		(void)strlcpy(zc->zc_value, snap_name,
+					  sizeof(zc->zc_value));
 	kmem_strfree(snap_name);
 	kmem_strfree(hold_name);
 	zfs_onexit_fd_rele(fp);
@@ -6133,8 +6505,8 @@ zfs_ioc_smb_acl(zfs_cmd_t *zc)
  * }
  */
 static const zfs_ioc_key_t zfs_keys_hold[] = {
-	{"holds",		DATA_TYPE_NVLIST,	0},
-	{"cleanup_fd",		DATA_TYPE_INT32,	ZK_OPTIONAL},
+	{"holds", DATA_TYPE_NVLIST, 0},
+	{"cleanup_fd", DATA_TYPE_INT32, ZK_OPTIONAL},
 };
 
 /* ARGSUSED */
@@ -6152,7 +6524,8 @@ zfs_ioc_hold(const char *pool, nvlist_t *args, nvlist_t *errlist)
 
 	/* make sure the user didn't pass us any invalid (empty) tags */
 	for (pair = nvlist_next_nvpair(holds, NULL); pair != NULL;
-	    pair = nvlist_next_nvpair(holds, pair)) {
+		 pair = nvlist_next_nvpair(holds, pair))
+	{
 		char *htag;
 
 		error = nvpair_value_string(pair, &htag);
@@ -6163,14 +6536,16 @@ zfs_ioc_hold(const char *pool, nvlist_t *args, nvlist_t *errlist)
 			return (SET_ERROR(EINVAL));
 	}
 
-	if (nvlist_lookup_int32(args, "cleanup_fd", &cleanup_fd) == 0) {
+	if (nvlist_lookup_int32(args, "cleanup_fd", &cleanup_fd) == 0)
+	{
 		fp = zfs_onexit_fd_hold(cleanup_fd, &minor);
 		if (fp == NULL)
 			return (SET_ERROR(EBADF));
 	}
 
 	error = dsl_dataset_user_hold(holds, minor, errlist);
-	if (fp != NULL) {
+	if (fp != NULL)
+	{
 		ASSERT3U(minor, !=, 0);
 		zfs_onexit_fd_rele(fp);
 	}
@@ -6208,7 +6583,7 @@ zfs_ioc_get_holds(const char *snapname, nvlist_t *args, nvlist_t *outnvl)
  * }
  */
 static const zfs_ioc_key_t zfs_keys_release[] = {
-	{"<snapname>...",	DATA_TYPE_NVLIST,	ZK_WILDCARDLIST},
+	{"<snapname>...", DATA_TYPE_NVLIST, ZK_WILDCARDLIST},
 };
 
 /* ARGSUSED */
@@ -6240,10 +6615,12 @@ zfs_ioc_events_next(zfs_cmd_t *zc)
 	if (fp == NULL)
 		return (SET_ERROR(EBADF));
 
-	do {
+	do
+	{
 		error = zfs_zevent_next(ze, &event,
-		    &zc->zc_nvlist_dst_size, &dropped);
-		if (event != NULL) {
+								&zc->zc_nvlist_dst_size, &dropped);
+		if (event != NULL)
+		{
 			zc->zc_cookie = dropped;
 			error = put_nvlist(zc, event);
 			nvlist_free(event);
@@ -6323,27 +6700,33 @@ zfs_ioc_space_written(zfs_cmd_t *zc)
 	if (error != 0)
 		return (error);
 	error = dsl_dataset_hold(dp, zc->zc_name, FTAG, &new);
-	if (error != 0) {
+	if (error != 0)
+	{
 		dsl_pool_rele(dp, FTAG);
 		return (error);
 	}
-	if (strchr(zc->zc_value, '#') != NULL) {
+	if (strchr(zc->zc_value, '#') != NULL)
+	{
 		zfs_bookmark_phys_t bmp;
 		error = dsl_bookmark_lookup(dp, zc->zc_value,
-		    new, &bmp);
-		if (error == 0) {
+									new, &bmp);
+		if (error == 0)
+		{
 			error = dsl_dataset_space_written_bookmark(&bmp, new,
-			    &zc->zc_cookie,
-			    &zc->zc_objset_type, &zc->zc_perm_action);
+													   &zc->zc_cookie,
+													   &zc->zc_objset_type, &zc->zc_perm_action);
 		}
-	} else {
+	}
+	else
+	{
 		dsl_dataset_t *old;
 		error = dsl_dataset_hold(dp, zc->zc_value, FTAG, &old);
 
-		if (error == 0) {
+		if (error == 0)
+		{
 			error = dsl_dataset_space_written(old, new,
-			    &zc->zc_cookie,
-			    &zc->zc_objset_type, &zc->zc_perm_action);
+											  &zc->zc_cookie,
+											  &zc->zc_objset_type, &zc->zc_perm_action);
 			dsl_dataset_rele(old, FTAG);
 		}
 	}
@@ -6364,7 +6747,7 @@ zfs_ioc_space_written(zfs_cmd_t *zc)
  * }
  */
 static const zfs_ioc_key_t zfs_keys_space_snaps[] = {
-	{"firstsnap",	DATA_TYPE_STRING,	0},
+	{"firstsnap", DATA_TYPE_STRING, 0},
 };
 
 static int
@@ -6383,20 +6766,24 @@ zfs_ioc_space_snaps(const char *lastsnap, nvlist_t *innvl, nvlist_t *outnvl)
 		return (error);
 
 	error = dsl_dataset_hold(dp, lastsnap, FTAG, &new);
-	if (error == 0 && !new->ds_is_snapshot) {
+	if (error == 0 && !new->ds_is_snapshot)
+	{
 		dsl_dataset_rele(new, FTAG);
 		error = SET_ERROR(EINVAL);
 	}
-	if (error != 0) {
+	if (error != 0)
+	{
 		dsl_pool_rele(dp, FTAG);
 		return (error);
 	}
 	error = dsl_dataset_hold(dp, firstsnap, FTAG, &old);
-	if (error == 0 && !old->ds_is_snapshot) {
+	if (error == 0 && !old->ds_is_snapshot)
+	{
 		dsl_dataset_rele(old, FTAG);
 		error = SET_ERROR(EINVAL);
 	}
-	if (error != 0) {
+	if (error != 0)
+	{
 		dsl_dataset_rele(new, FTAG);
 		dsl_pool_rele(dp, FTAG);
 		return (error);
@@ -6436,16 +6823,16 @@ zfs_ioc_space_snaps(const char *lastsnap, nvlist_t *innvl, nvlist_t *outnvl)
  * outnvl is unused
  */
 static const zfs_ioc_key_t zfs_keys_send_new[] = {
-	{"fd",			DATA_TYPE_INT32,	0},
-	{"fromsnap",		DATA_TYPE_STRING,	ZK_OPTIONAL},
-	{"largeblockok",	DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"embedok",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"compressok",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"rawok",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"savedok",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"resume_object",	DATA_TYPE_UINT64,	ZK_OPTIONAL},
-	{"resume_offset",	DATA_TYPE_UINT64,	ZK_OPTIONAL},
-	{"redactbook",		DATA_TYPE_STRING,	ZK_OPTIONAL},
+	{"fd", DATA_TYPE_INT32, 0},
+	{"fromsnap", DATA_TYPE_STRING, ZK_OPTIONAL},
+	{"largeblockok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"embedok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"compressok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"rawok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"savedok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"resume_object", DATA_TYPE_UINT64, ZK_OPTIONAL},
+	{"resume_offset", DATA_TYPE_UINT64, ZK_OPTIONAL},
+	{"redactbook", DATA_TYPE_STRING, ZK_OPTIONAL},
 };
 
 /* ARGSUSED */
@@ -6468,7 +6855,7 @@ zfs_ioc_send_new(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
 
 	fd = fnvlist_lookup_int32(innvl, "fd");
 
-	(void) nvlist_lookup_string(innvl, "fromsnap", &fromname);
+	(void)nvlist_lookup_string(innvl, "fromsnap", &fromname);
 
 	largeblockok = nvlist_exists(innvl, "largeblockok");
 	embedok = nvlist_exists(innvl, "embedok");
@@ -6476,10 +6863,10 @@ zfs_ioc_send_new(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
 	rawok = nvlist_exists(innvl, "rawok");
 	savedok = nvlist_exists(innvl, "savedok");
 
-	(void) nvlist_lookup_uint64(innvl, "resume_object", &resumeobj);
-	(void) nvlist_lookup_uint64(innvl, "resume_offset", &resumeoff);
+	(void)nvlist_lookup_uint64(innvl, "resume_object", &resumeobj);
+	(void)nvlist_lookup_uint64(innvl, "resume_offset", &resumeoff);
 
-	(void) nvlist_lookup_string(innvl, "redactbook", &redactbook);
+	(void)nvlist_lookup_string(innvl, "redactbook", &redactbook);
 
 	if ((fp = zfs_file_get(fd)) == NULL)
 		return (SET_ERROR(EBADF));
@@ -6491,8 +6878,8 @@ zfs_ioc_send_new(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
 	out.dso_arg = fp;
 	out.dso_dryrun = B_FALSE;
 	error = dmu_send(snapname, fromname, embedok, largeblockok,
-	    compressok, rawok, savedok, resumeobj, resumeoff,
-	    redactbook, fd, &off, &out);
+					 compressok, rawok, savedok, resumeobj, resumeoff,
+					 redactbook, fd, &off, &out);
 
 	zfs_file_put(fp);
 	return (error);
@@ -6533,17 +6920,17 @@ send_space_sum(objset_t *os, void *buf, int len, void *arg)
  * }
  */
 static const zfs_ioc_key_t zfs_keys_send_space[] = {
-	{"from",		DATA_TYPE_STRING,	ZK_OPTIONAL},
-	{"fromsnap",		DATA_TYPE_STRING,	ZK_OPTIONAL},
-	{"largeblockok",	DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"embedok",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"compressok",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"rawok",		DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
-	{"fd",			DATA_TYPE_INT32,	ZK_OPTIONAL},
-	{"redactbook",		DATA_TYPE_STRING,	ZK_OPTIONAL},
-	{"resume_object",	DATA_TYPE_UINT64,	ZK_OPTIONAL},
-	{"resume_offset",	DATA_TYPE_UINT64,	ZK_OPTIONAL},
-	{"bytes",		DATA_TYPE_UINT64,	ZK_OPTIONAL},
+	{"from", DATA_TYPE_STRING, ZK_OPTIONAL},
+	{"fromsnap", DATA_TYPE_STRING, ZK_OPTIONAL},
+	{"largeblockok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"embedok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"compressok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"rawok", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
+	{"fd", DATA_TYPE_INT32, ZK_OPTIONAL},
+	{"redactbook", DATA_TYPE_STRING, ZK_OPTIONAL},
+	{"resume_object", DATA_TYPE_UINT64, ZK_OPTIONAL},
+	{"resume_offset", DATA_TYPE_UINT64, ZK_OPTIONAL},
+	{"bytes", DATA_TYPE_UINT64, ZK_OPTIONAL},
 };
 
 static int
@@ -6573,11 +6960,12 @@ zfs_ioc_send_space(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
 		return (error);
 
 	error = dsl_dataset_hold(dp, snapname, FTAG, &tosnap);
-	if (error != 0) {
+	if (error != 0)
+	{
 		dsl_pool_rele(dp, FTAG);
 		return (error);
 	}
-	(void) nvlist_lookup_int32(innvl, "fd", &fd);
+	(void)nvlist_lookup_int32(innvl, "fd", &fd);
 
 	largeblockok = nvlist_exists(innvl, "largeblockok");
 	embedok = nvlist_exists(innvl, "embedok");
@@ -6586,16 +6974,20 @@ zfs_ioc_send_space(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
 	savedok = nvlist_exists(innvl, "savedok");
 	boolean_t from = (nvlist_lookup_string(innvl, "from", &fromname) == 0);
 	boolean_t altbook = (nvlist_lookup_string(innvl, "redactbook",
-	    &redactlist_book) == 0);
+											  &redactlist_book) == 0);
 
-	(void) nvlist_lookup_uint64(innvl, "resume_object", &resumeobj);
-	(void) nvlist_lookup_uint64(innvl, "resume_offset", &resumeoff);
-	(void) nvlist_lookup_uint64(innvl, "bytes", &resume_bytes);
+	(void)nvlist_lookup_uint64(innvl, "resume_object", &resumeobj);
+	(void)nvlist_lookup_uint64(innvl, "resume_offset", &resumeoff);
+	(void)nvlist_lookup_uint64(innvl, "bytes", &resume_bytes);
 
-	if (altbook) {
+	if (altbook)
+	{
 		full_estimate = B_TRUE;
-	} else if (from) {
-		if (strchr(fromname, '#')) {
+	}
+	else if (from)
+	{
+		if (strchr(fromname, '#'))
+		{
 			error = dsl_bookmark_lookup(dp, fromname, tosnap, &zbm);
 
 			/*
@@ -6609,32 +7001,40 @@ zfs_ioc_send_space(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
 			 * but returns EXDEV.  Ignore this error.
 			 */
 			if (error == EXDEV && zbm.zbm_redaction_obj != 0 &&
-			    zbm.zbm_guid ==
-			    dsl_dataset_phys(tosnap)->ds_guid)
+				zbm.zbm_guid ==
+					dsl_dataset_phys(tosnap)->ds_guid)
 				error = 0;
 
-			if (error != 0) {
+			if (error != 0)
+			{
 				dsl_dataset_rele(tosnap, FTAG);
 				dsl_pool_rele(dp, FTAG);
 				return (error);
 			}
 			if (zbm.zbm_redaction_obj != 0 || !(zbm.zbm_flags &
-			    ZBM_FLAG_HAS_FBN)) {
+												ZBM_FLAG_HAS_FBN))
+			{
 				full_estimate = B_TRUE;
 			}
-		} else if (strchr(fromname, '@')) {
+		}
+		else if (strchr(fromname, '@'))
+		{
 			error = dsl_dataset_hold(dp, fromname, FTAG, &fromsnap);
-			if (error != 0) {
+			if (error != 0)
+			{
 				dsl_dataset_rele(tosnap, FTAG);
 				dsl_pool_rele(dp, FTAG);
 				return (error);
 			}
 
-			if (!dsl_dataset_is_before(tosnap, fromsnap, 0)) {
+			if (!dsl_dataset_is_before(tosnap, fromsnap, 0))
+			{
 				full_estimate = B_TRUE;
 				dsl_dataset_rele(fromsnap, FTAG);
 			}
-		} else {
+		}
+		else
+		{
 			/*
 			 * from is not properly formatted as a snapshot or
 			 * bookmark
@@ -6645,7 +7045,8 @@ zfs_ioc_send_space(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
 		}
 	}
 
-	if (full_estimate) {
+	if (full_estimate)
+	{
 		dmu_send_outparams_t out = {0};
 		offset_t off = 0;
 		out.dso_outfunc = send_space_sum;
@@ -6658,12 +7059,14 @@ zfs_ioc_send_space(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
 		dsl_dataset_rele(tosnap, FTAG);
 		dsl_pool_rele(dp, FTAG);
 		error = dmu_send(snapname, fromname, embedok, largeblockok,
-		    compressok, rawok, savedok, resumeobj, resumeoff,
-		    redactlist_book, fd, &off, &out);
-	} else {
+						 compressok, rawok, savedok, resumeobj, resumeoff,
+						 redactlist_book, fd, &off, &out);
+	}
+	else
+	{
 		error = dmu_send_estimate_fast(tosnap, fromsnap,
-		    (from && strchr(fromname, '#') != NULL ? &zbm : NULL),
-		    compressok || rawok, savedok, &space);
+									   (from && strchr(fromname, '#') != NULL ? &zbm : NULL),
+									   compressok || rawok, savedok, &space);
 		space -= resume_bytes;
 		if (fromsnap != NULL)
 			dsl_dataset_rele(fromsnap, FTAG);
@@ -6691,7 +7094,7 @@ zfs_ioc_send_space(const char *snapname, nvlist_t *innvl, nvlist_t *outnvl)
  * onvl is unused
  */
 static const zfs_ioc_key_t zfs_keys_pool_sync[] = {
-	{"force",	DATA_TYPE_BOOLEAN_VALUE,	0},
+	{"force", DATA_TYPE_BOOLEAN_VALUE, 0},
 };
 
 /* ARGSUSED */
@@ -6705,13 +7108,15 @@ zfs_ioc_pool_sync(const char *pool, nvlist_t *innvl, nvlist_t *onvl)
 	if ((err = spa_open(pool, &spa, FTAG)) != 0)
 		return (err);
 
-	if (innvl) {
+	if (innvl)
+	{
 		err = nvlist_lookup_boolean_value(innvl, "force", &rc);
 		if (err == 0)
 			force = rc;
 	}
 
-	if (force) {
+	if (force)
+	{
 		spa_config_enter(spa, SCL_CONFIG, FTAG, RW_WRITER);
 		vdev_config_dirty(spa->spa_root_vdev);
 		spa_config_exit(spa, SCL_CONFIG, FTAG);
@@ -6733,8 +7138,8 @@ zfs_ioc_pool_sync(const char *pool, nvlist_t *innvl, nvlist_t *onvl)
  * }
  */
 static const zfs_ioc_key_t zfs_keys_load_key[] = {
-	{"hidden_args",	DATA_TYPE_NVLIST,	0},
-	{"noop",	DATA_TYPE_BOOLEAN,	ZK_OPTIONAL},
+	{"hidden_args", DATA_TYPE_NVLIST, 0},
+	{"noop", DATA_TYPE_BOOLEAN, ZK_OPTIONAL},
 };
 
 /* ARGSUSED */
@@ -6746,7 +7151,8 @@ zfs_ioc_load_key(const char *dsname, nvlist_t *innvl, nvlist_t *outnvl)
 	nvlist_t *hidden_args;
 	boolean_t noop = nvlist_exists(innvl, "noop");
 
-	if (strchr(dsname, '@') != NULL || strchr(dsname, '%') != NULL) {
+	if (strchr(dsname, '@') != NULL || strchr(dsname, '%') != NULL)
+	{
 		ret = SET_ERROR(EINVAL);
 		goto error;
 	}
@@ -6754,7 +7160,7 @@ zfs_ioc_load_key(const char *dsname, nvlist_t *innvl, nvlist_t *outnvl)
 	hidden_args = fnvlist_lookup_nvlist(innvl, ZPOOL_HIDDEN_ARGS);
 
 	ret = dsl_crypto_params_create_nvlist(DCP_CMD_NONE, NULL,
-	    hidden_args, &dcp);
+										  hidden_args, &dcp);
 	if (ret != 0)
 		goto error;
 
@@ -6785,7 +7191,8 @@ zfs_ioc_unload_key(const char *dsname, nvlist_t *innvl, nvlist_t *outnvl)
 {
 	int ret = 0;
 
-	if (strchr(dsname, '@') != NULL || strchr(dsname, '%') != NULL) {
+	if (strchr(dsname, '@') != NULL || strchr(dsname, '%') != NULL)
+	{
 		ret = (SET_ERROR(EINVAL));
 		goto out;
 	}
@@ -6812,9 +7219,9 @@ out:
  * outnvl is unused
  */
 static const zfs_ioc_key_t zfs_keys_change_key[] = {
-	{"crypt_cmd",	DATA_TYPE_UINT64,	ZK_OPTIONAL},
-	{"hidden_args",	DATA_TYPE_NVLIST,	ZK_OPTIONAL},
-	{"props",	DATA_TYPE_NVLIST,	ZK_OPTIONAL},
+	{"crypt_cmd", DATA_TYPE_UINT64, ZK_OPTIONAL},
+	{"hidden_args", DATA_TYPE_NVLIST, ZK_OPTIONAL},
+	{"props", DATA_TYPE_NVLIST, ZK_OPTIONAL},
 };
 
 /* ARGSUSED */
@@ -6826,14 +7233,15 @@ zfs_ioc_change_key(const char *dsname, nvlist_t *innvl, nvlist_t *outnvl)
 	dsl_crypto_params_t *dcp = NULL;
 	nvlist_t *args = NULL, *hidden_args = NULL;
 
-	if (strchr(dsname, '@') != NULL || strchr(dsname, '%') != NULL) {
+	if (strchr(dsname, '@') != NULL || strchr(dsname, '%') != NULL)
+	{
 		ret = (SET_ERROR(EINVAL));
 		goto error;
 	}
 
-	(void) nvlist_lookup_uint64(innvl, "crypt_cmd", &cmd);
-	(void) nvlist_lookup_nvlist(innvl, "props", &args);
-	(void) nvlist_lookup_nvlist(innvl, ZPOOL_HIDDEN_ARGS, &hidden_args);
+	(void)nvlist_lookup_uint64(innvl, "crypt_cmd", &cmd);
+	(void)nvlist_lookup_nvlist(innvl, "props", &args);
+	(void)nvlist_lookup_nvlist(innvl, ZPOOL_HIDDEN_ARGS, &hidden_args);
 
 	ret = dsl_crypto_params_create_nvlist(cmd, args, hidden_args, &dcp);
 	if (ret != 0)
@@ -6856,8 +7264,8 @@ static zfs_ioc_vec_t zfs_ioc_vec[ZFS_IOC_LAST - ZFS_IOC_FIRST];
 
 static void
 zfs_ioctl_register_legacy(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func,
-    zfs_secpolicy_func_t *secpolicy, zfs_ioc_namecheck_t namecheck,
-    boolean_t log_history, zfs_ioc_poolcheck_t pool_check)
+						  zfs_secpolicy_func_t *secpolicy, zfs_ioc_namecheck_t namecheck,
+						  boolean_t log_history, zfs_ioc_poolcheck_t pool_check)
 {
 	zfs_ioc_vec_t *vec = &zfs_ioc_vec[ioc - ZFS_IOC_FIRST];
 
@@ -6877,12 +7285,12 @@ zfs_ioctl_register_legacy(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func,
  * See the block comment at the beginning of this file for details on
  * each argument to this function.
  */
-void
-zfs_ioctl_register(const char *name, zfs_ioc_t ioc, zfs_ioc_func_t *func,
-    zfs_secpolicy_func_t *secpolicy, zfs_ioc_namecheck_t namecheck,
-    zfs_ioc_poolcheck_t pool_check, boolean_t smush_outnvlist,
-    boolean_t allow_log, const zfs_ioc_key_t *nvl_keys, size_t num_keys)
+void zfs_ioctl_register(const char *name, zfs_ioc_t ioc, zfs_ioc_func_t *func,
+						zfs_secpolicy_func_t *secpolicy, zfs_ioc_namecheck_t namecheck,
+						zfs_ioc_poolcheck_t pool_check, boolean_t smush_outnvlist,
+						boolean_t allow_log, const zfs_ioc_key_t *nvl_keys, size_t num_keys)
 {
+	zfs_dbgmsg("zfs_ioctl_register() is called in ZFS kernel module\n");
 	zfs_ioc_vec_t *vec = &zfs_ioc_vec[ioc - ZFS_IOC_FIRST];
 
 	ASSERT3U(ioc, >=, ZFS_IOC_FIRST);
@@ -6906,269 +7314,908 @@ zfs_ioctl_register(const char *name, zfs_ioc_t ioc, zfs_ioc_func_t *func,
 
 static void
 zfs_ioctl_register_pool(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func,
-    zfs_secpolicy_func_t *secpolicy, boolean_t log_history,
-    zfs_ioc_poolcheck_t pool_check)
+						zfs_secpolicy_func_t *secpolicy, boolean_t log_history,
+						zfs_ioc_poolcheck_t pool_check)
 {
 	zfs_ioctl_register_legacy(ioc, func, secpolicy,
-	    POOL_NAME, log_history, pool_check);
+							  POOL_NAME, log_history, pool_check);
 }
 
-void
-zfs_ioctl_register_dataset_nolog(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func,
-    zfs_secpolicy_func_t *secpolicy, zfs_ioc_poolcheck_t pool_check)
+void zfs_ioctl_register_dataset_nolog(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func,
+									  zfs_secpolicy_func_t *secpolicy, zfs_ioc_poolcheck_t pool_check)
 {
 	zfs_ioctl_register_legacy(ioc, func, secpolicy,
-	    DATASET_NAME, B_FALSE, pool_check);
+							  DATASET_NAME, B_FALSE, pool_check);
 }
 
 static void
 zfs_ioctl_register_pool_modify(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func)
 {
 	zfs_ioctl_register_legacy(ioc, func, zfs_secpolicy_config,
-	    POOL_NAME, B_TRUE, POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY);
+							  POOL_NAME, B_TRUE, POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY);
 }
 
 static void
 zfs_ioctl_register_pool_meta(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func,
-    zfs_secpolicy_func_t *secpolicy)
+							 zfs_secpolicy_func_t *secpolicy)
 {
 	zfs_ioctl_register_legacy(ioc, func, secpolicy,
-	    NO_NAME, B_FALSE, POOL_CHECK_NONE);
+							  NO_NAME, B_FALSE, POOL_CHECK_NONE);
 }
 
 static void
 zfs_ioctl_register_dataset_read_secpolicy(zfs_ioc_t ioc,
-    zfs_ioc_legacy_func_t *func, zfs_secpolicy_func_t *secpolicy)
+										  zfs_ioc_legacy_func_t *func, zfs_secpolicy_func_t *secpolicy)
 {
 	zfs_ioctl_register_legacy(ioc, func, secpolicy,
-	    DATASET_NAME, B_FALSE, POOL_CHECK_SUSPENDED);
+							  DATASET_NAME, B_FALSE, POOL_CHECK_SUSPENDED);
 }
 
 static void
 zfs_ioctl_register_dataset_read(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func)
 {
 	zfs_ioctl_register_dataset_read_secpolicy(ioc, func,
-	    zfs_secpolicy_read);
+											  zfs_secpolicy_read);
 }
 
 static void
 zfs_ioctl_register_dataset_modify(zfs_ioc_t ioc, zfs_ioc_legacy_func_t *func,
-    zfs_secpolicy_func_t *secpolicy)
+								  zfs_secpolicy_func_t *secpolicy)
 {
 	zfs_ioctl_register_legacy(ioc, func, secpolicy,
-	    DATASET_NAME, B_TRUE, POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY);
+							  DATASET_NAME, B_TRUE, POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY);
+}
+
+static int
+mlec_open_objset(const char *path, void *tag, objset_t **osp, dsl_dataset_t **dsl_dataset)
+{
+	
+	int err;
+
+	/*
+	 * We can't own an objset if it's redacted.  Therefore, we do this
+	 * dance: hold the objset, then acquire a long hold on its dataset, then
+	 * release the pool (which is held as part of holding the objset).
+	 */
+	err = dmu_objset_hold(path, tag, osp);
+	if (err != 0) {
+		zfs_dbgmsg("failed to hold dataset %s", path);
+		return (err);
+	}
+
+	dsl_dataset_long_hold(dmu_objset_ds(*osp), tag);
+	dsl_pool_rele(dmu_objset_pool(*osp), tag);
+
+	// err = dsl_dataset_hold(dmu_objset_pool(*osp), path, tag, dsl_dataset);
+
+	return (err);
+}
+
+static void
+mlec_close_objset(objset_t *os, void *tag, dsl_dataset_t *dsl_dataset)
+{
+	dsl_dataset_long_rele(dmu_objset_ds(os), tag);
+	dsl_dataset_rele(dmu_objset_ds(os), tag);
+	// dmu_objset_rele(os, tag);
+}
+
+static int
+mlec_zfs_sa_setup(objset_t *osp, sa_attr_type_t **sa_table)
+{
+	uint64_t sa_obj = 0;
+	int error;
+
+	error = zap_lookup(osp, MASTER_NODE_OBJ, ZFS_SA_ATTRS, 8, 1, &sa_obj);
+	if (error != 0 && error != ENOENT)
+		return (error);
+
+	error = sa_setup(osp, sa_obj, zfs_attr_table, ZPL_END, sa_table);
+	return (error);
+}
+
+static int
+mlec_zfs_grab_sa_handle(objset_t *osp, uint64_t obj, sa_handle_t **hdlp,
+    dmu_buf_t **db, void *tag)
+{
+	dmu_object_info_t doi;
+	int error;
+
+	if ((error = sa_buf_hold(osp, obj, tag, db)) != 0)
+		return (error);
+
+	dmu_object_info_from_db(*db, &doi);
+	if ((doi.doi_bonus_type != DMU_OT_SA &&
+	    doi.doi_bonus_type != DMU_OT_ZNODE) ||
+	    (doi.doi_bonus_type == DMU_OT_ZNODE &&
+	    doi.doi_bonus_size < sizeof (znode_phys_t))) {
+		sa_buf_rele(*db, tag);
+		return (SET_ERROR(ENOTSUP));
+	}
+
+	error = sa_handle_get(osp, obj, NULL, SA_HDL_PRIVATE, hdlp);
+	if (error != 0) {
+		sa_buf_rele(*db, tag);
+		return (error);
+	}
+
+	return (0);
+}
+
+static int mlec_get_dn_fsize(dsl_dataset_t *dsl_dataset, uint64_t object_id, uint64_t *fsize, sa_handle_t **hdl, dmu_buf_t **db) {
+	// The size come from sa attributes
+	sa_attr_type_t *sa_table;
+	int error;
+
+	error = mlec_zfs_sa_setup(dsl_dataset->ds_objset, &sa_table);
+	if (error != 0) {
+		return (error);
+	}
+		
+
+	error = mlec_zfs_grab_sa_handle(dsl_dataset->ds_objset, object_id, hdl, db, FTAG);
+	if (error != 0)
+		return (error);
+
+	sa_bulk_attr_t bulk[12];
+	int idx = 0;
+	SA_ADD_BULK_ATTR(bulk, idx, sa_table[ZPL_SIZE], NULL, fsize, 8);
+
+	if (sa_bulk_lookup(*hdl, bulk, idx) != 0){
+		return 1;
+	}
+	
+	return 0;
+}
+
+typedef struct raidz_info {
+	uint64_t dcols;
+	uint64_t nparity;
+	uint64_t ashift;
+	uint64_t b;
+	uint64_t s;
+	uint64_t f;
+	uint64_t o;
+	uint64_t q;
+	uint64_t r;
+	uint64_t bc;
+	uint64_t tot;
+	uint64_t acols;
+	uint64_t scols;
+	uint64_t num_stripes;
+} raidz_info_t;
+
+
+static int mlec_get_raidz_info(vdev_t *top, vdev_raidz_t *vrt, uint64_t fsize, raidz_info_t *info) {
+	uint64_t dcols = vrt->vd_logical_width;
+	uint64_t nparity = vrt->vd_nparity;
+	uint64_t ashift = top->vdev_ashift;
+
+	// uint64_t b = 0;
+	/* The zio's size in units of the vdev's minimum sector size. */
+	uint64_t s = fsize >> ashift;
+	/* The first column for this stripe. */
+	// uint64_t f = b % dcols;
+	/* The starting byte offset on each child vdev. */
+	// Question? why is this / dcols, not (dcols - 1)? 
+	// uint64_t o = (b / dcols) << ashift;
+	uint64_t q, r, bc, acols, scols, tot;
+	// uint64_t col, c;
+
+	/*
+	 * "Quotient": The number of data sectors for this stripe on all but
+	 * the "big column" child vdevs that also contain "remainder" data.
+	 */
+	q = s / (dcols - nparity);
+
+	/*
+	 * "Remainder": The number of partial stripe data sectors in this I/O.
+	 * This will add a sector to some, but not all, child vdevs.
+	 */
+	r = s - q * (dcols - nparity);
+
+	/* The number of "big columns" - those which contain remainder data. */
+	bc = (r == 0 ? 0 : r + nparity);
+
+	/*
+	 * The total number of data and parity sectors associated with
+	 * this I/O.
+	 */
+	tot = s + nparity * (q + (r == 0 ? 0 : 1));
+
+	/*
+	 * acols: The columns that will be accessed.
+	 * scols: The columns that will be accessed or skipped.
+	 */
+	if (q == 0) {
+		/* Our I/O request doesn't span all child vdevs. */
+		acols = bc;
+		scols = MIN(dcols, roundup(bc, nparity + 1));
+	} else {
+		acols = dcols;
+		scols = dcols;
+	}
+
+	// Assign into info
+	info->dcols = vrt->vd_logical_width;
+	info->nparity = vrt->vd_nparity;
+	info->ashift = top->vdev_ashift;
+	info->q = q;
+	info->r = r;
+	info->bc = bc;
+	info->tot = tot;
+	info->acols = acols;
+	info->scols = scols;
+
+	// MLEC custom stuff
+	uint64_t num_stripes = q / (dcols - nparity) + (r != 0 ? 1 : 0);
+	info->num_stripes = num_stripes;
+
+	return 0;
+}
+
+static int
+zfs_get_vdev_children_status(vdev_t *vdev, int64_t *child_status) {
+	zfs_dbgmsg("Trying to get children status");
+	// Check how many children it has
+	if (vdev->vdev_children == 0) {
+		return 1;
+	}
+
+	for (int i = 0; i < vdev->vdev_children; i++) {
+		zfs_dbgmsg("Trying to acquire SCL_STATE_ALL writer lock");
+		// spa_config_enter(vdev->vdev_spa, SCL_STATE_ALL, FTAG, RW_WRITER);
+		zfs_dbgmsg("Acquired SCL_STATE_ALL writer lock on vdev");
+
+		vdev_t *vd = vdev->vdev_child[i];
+
+		// Recreate the content of vdev_open
+
+		vd->vdev_open_thread = curthread;
+		vd->vdev_validate_thread = curthread;
+		zfs_dbgmsg("Current vdev state %llu", (longlong_t) vd->vdev_state);
+		int error = vdev_validate(vd);
+		child_status[i] = error;
+		// vdev_close(vd);
+		// child_status[i] = vdev_reopen(vd);
+		vd->vdev_open_thread = NULL;
+		vd->vdev_validate_thread = NULL;
+
+		// spa_config_exit(vdev->vdev_spa, SCL_STATE_ALL, FTAG);
+		zfs_dbgmsg("child status %d is %lld", i, child_status[i]);
+		zfs_dbgmsg("Released writer SCL_STATE_ALL lock");
+	}
+
+	return 0;
+}
+
+static int
+mlec_dump_objset(objset_t *os, nvlist_t *out)
+{
+	zfs_dbgmsg("mlec_dump_objset called on objset %llu", (u_longlong_t)dmu_objset_id(os));
+	uint64_t object;
+	char osname[ZFS_MAX_DATASET_NAME_LEN];
+	int error;
+	
+	dmu_objset_name(os, osname);
+	zfs_dbgmsg("Object set name %s", osname);
+
+	if (BP_IS_HOLE(os->os_rootbp))
+		return -1;
+
+	vdev_t *vdev = vdev_lookup_top(os->os_spa, 0);
+
+	int num_object = 0;
+	object = 0;
+	while ((error = dmu_object_next(os, &object, B_FALSE, 0)) == 0) {
+		// Check whether dnode is a plain file
+		zfs_dbgmsg("dumping object %llu", (u_longlong_t)object);
+		dnode_t *dn;
+		dnode_hold(os, object, FTAG, &dn);
+		zfs_dbgmsg("dnode ref count after hold in dump objset %llu", (u_longlong_t)dn->dn_holds.rc_count);
+		
+		if (dn->dn_type == DMU_OT_PLAIN_FILE_CONTENTS) {
+			char path[MAXPATHLEN * 2];
+			int obj_to_path_error = zfs_obj_to_path(os, object, path, sizeof(path));
+			if (obj_to_path_error) {
+				zfs_dbgmsg("Error retrieving dnode path, error %d", obj_to_path_error);
+			}
+			zfs_dbgmsg("dnode %lld:%lld, type %d, path %s", dmu_objset_id(os), object, dn->dn_type, path);
+
+			// Set that into the list
+			int nv_error = 0;
+
+			nvlist_t *attributes;
+			nv_error += nvlist_alloc(&attributes, NV_UNIQUE_NAME, 0);
+			if (nv_error) {
+				zfs_dbgmsg("Error while allocating nvlist");
+				dnode_rele(dn, FTAG);
+				return -1;
+			}
+
+			nv_error += nvlist_add_int64(attributes, "objset", dmu_objset_id(os));
+			nv_error += nvlist_add_int64(attributes, "object", object);
+			nv_error += nvlist_add_int64(attributes, "type", dn->dn_type);
+			nv_error += nvlist_add_string(attributes, "path", path);
+
+			zfs_dbgmsg("Got dnode basic attributes");
+			
+			// Get the fsize
+			uint64_t fsize;
+			sa_handle_t *hdl;
+			dmu_buf_t *db;
+			if (mlec_get_dn_fsize(os->os_dsl_dataset, object, &fsize, &hdl, &db)) {
+				dnode_rele(dn, FTAG);
+				sa_handle_destroy(hdl);
+				sa_buf_rele(db, FTAG);
+				return -1;
+			}
+			sa_handle_destroy(hdl);
+			sa_buf_rele(db, FTAG);
+			zfs_dbgmsg("Got fsize");
+
+			nv_error += nvlist_add_int64(attributes, "fsize", fsize);
+
+			// Get the failure information
+			int64_t child_status[vdev->vdev_children];
+			raidz_info_t info;
+
+			info.dcols = 3;
+			info.nparity = 1;
+			info.r = 0;
+
+			mlec_get_raidz_info(vdev, vdev->vdev_tsd, fsize, &info);
+			zfs_get_vdev_children_status(vdev, child_status);
+
+			zfs_dbgmsg("Got raidz info");
+			
+			// Get child status
+			nv_error += nvlist_add_int64(attributes, "dcols", info.dcols);
+			nv_error += nvlist_add_int64(attributes, "nparity", info.nparity);
+			nv_error += nvlist_add_int64_array(attributes, "child_status", child_status, vdev->vdev_children);
+
+			// Get number of stripes
+			uint64_t number_of_stripes = info.q / (info.dcols - info.nparity);
+			nv_error += nvlist_add_int64(attributes, "num_stripes", number_of_stripes);
+			nv_error += nvlist_add_int64(attributes, "num_remainder_stripes", info.r == 0 ? 0 : 1);
+			// zfs_dbgmsg("failed chunks dnode size %lld, num full stripe sectors %lld, num partial stripe sectors %lld", fsize, info.q, info.r);
+
+			// Set that into the out nvlist
+			char index[5];
+			sprintf(index, "%d", num_object);
+			zfs_dbgmsg("Adding attributes for dnode %s to the outnvl", index);
+			nv_error += nvlist_add_nvlist(out, index, attributes);
+
+			if (nv_error) {
+				zfs_dbgmsg("Error while handling nvlist");
+				dnode_rele(dn, FTAG);
+				return -1;
+			}
+
+			num_object++;
+		}
+		
+		dnode_rele(dn, FTAG);
+		zfs_dbgmsg("dnode ref count after release in dump objset %llu", (u_longlong_t)dn->dn_holds.rc_count);
+	}
+
+	return num_object;
+}
+
+/*ARGSUSED*/
+static int
+mlec_dump_one_objset(const char *dsname, nvlist_t *arg)
+{
+	int error;
+	objset_t *os;
+	dsl_dataset_t *ds;
+
+	nvlist_t *out = (nvlist_t *) arg;
+
+	spa_t *spa;
+	spa_open(dsname, &spa, FTAG);
+
+	error = mlec_open_objset(dsname, FTAG, &os, &ds);
+	if (error != 0) {
+		zfs_dbgmsg("mlec_open_objset failed");
+		return (0);
+	}
+	spa_config_enter(os->os_spa, SCL_ALL, FTAG, RW_READER);
+
+
+	// nvlist_add_int64(out, "children", 101);
+	mlec_dump_objset(os, out);
+	spa_config_exit(os->os_spa, SCL_ALL, FTAG);
+
+	mlec_close_objset(os, FTAG, ds);
+	spa_close(spa, FTAG);
+
+	return (0);
+}
+
+// static int mlec_get_dsl_dataset(spa_t *spa, uint64_t objset_id, dsl_dataset_t **dsl_dataset) {
+// 	if (dsl_dataset_hold_obj(spa->spa_dsl_pool, objset_id, FTAG, dsl_dataset))
+// 	{
+// 		zfs_dbgmsg("dsl_dataset open failed");
+// 		spa_close(spa, FTAG);
+// 		return 1;
+// 	}
+
+// 	return 0;
+// }
+
+static int
+zfs_ioctl_failed_chunks(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
+{
+	zfs_dbgmsg("zfs pool_failed_chunks called");
+	spa_t *spa;
+
+	if (spa_open(poolname, &spa, FTAG)) {
+		zfs_dbgmsg("spa cannot be opened");
+		return 2;
+	}
+
+	// Acquire configuration reader
+	spa_config_enter(spa, SCL_ALL, FTAG, RW_READER);
+
+	/**
+	 * #1. Get the top vdev and dsl dataset
+	 */
+	vdev_t *top = vdev_lookup_top(spa, 0);
+	vdev_raidz_t *vrt = top->vdev_tsd;
+	zfs_dbgmsg("vdev raidz width %d, parity %d", vrt->vd_logical_width, vrt->vd_nparity);
+	zfs_dbgmsg("vdev ashift %lld, sector size %d", top->vdev_ashift, 1 << top->vdev_ashift);
+	
+	// Get the number of chunks, and number of stripes
+	// int innvl_err = 0;
+
+	objset_t *objset;
+	dmu_objset_hold(poolname, FTAG, &objset);
+
+	/**
+	 * #2. Get all the dnode on the drive, while dumping the dnode
+	 */
+	int num_object = mlec_dump_objset(objset, outnvl);
+	zfs_dbgmsg("mlec_dump_objset contains %d objects", num_object);
+
+	
+	dmu_objset_rele(objset, FTAG);
+	spa_config_exit(spa, SCL_ALL, FTAG);
+	spa_close(spa, FTAG);
+
+	return 0;
+}
+
+// Always return 0
+static int
+zfs_pool_failed_chunks_sec_policy(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
+{
+	return 0;
+}
+
+static const zfs_ioc_key_t zfs_keys_failed_chunks[] = {
+};
+
+static int
+zfs_ioc_pool_all_dnode(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
+{
+	zfs_dbgmsg("zfs pool_all_dnode called on pool %s", poolname);
+	// spa_t *spa;
+
+	// if (spa_open(poolname, &spa, FTAG)) {
+	// 	zfs_dbgmsg("spa cannot be opened");
+	// 	spa_close(spa, FTAG);
+	// 	return 2;
+	// }
+
+	// error = dmu_objset_find_impl(spa, spa_name(spa), mlec_dump_one_objset, outnvl, DS_FIND_CHILDREN);
+	// dmu_objset_find(poolname, mlec_dump_one_objset,
+	// 	    outnvl, DS_FIND_CHILDREN);
+	
+	mlec_dump_one_objset(poolname, outnvl);
+
+	// spa_close(spa, FTAG);
+
+	return 0;
+}
+
+// Always return 0
+static int
+zfs_pool_all_dnode_sec_policy(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
+{
+	return 0;
+}
+
+static const zfs_ioc_key_t zfs_keys_all_dnode[] = {
+};
+
+static int
+zfs_ioc_pool_easy_scan(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
+{
+	zfs_dbgmsg("zfs_ioc_pool_easy_scan called");
+	spa_t *spa;
+
+	if (spa_open(poolname, &spa, FTAG)) {
+		zfs_dbgmsg("spa cannot be opened");
+		return 2;
+	}
+
+	spa_config_enter(spa, SCL_ALL, FTAG, RW_READER);
+
+	vdev_t *top_vdev = vdev_lookup_top(spa, 0);
+	int64_t child_status[top_vdev->vdev_children];
+
+	zfs_get_vdev_children_status(top_vdev, child_status);
+
+	nvlist_add_int64_array(outnvl, "children_status", child_status, top_vdev->vdev_children);
+	nvlist_add_int64(outnvl, "children", top_vdev->vdev_children);
+
+	spa_config_exit(spa, SCL_ALL, FTAG);
+	spa_close(spa, FTAG);
+
+	return 0;
+}
+
+// Always return 0
+static int
+zfs_pool_easy_scan_sec_policy(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
+{
+	return 0;
+}
+
+static const zfs_ioc_key_t zfs_keys_easy_scan[] = {
+};
+
+static const zfs_ioc_key_t zfs_keys_mlec_receive_repair_data[] = {
+	{"data", DATA_TYPE_ANY, 0},
+	{"objset_id", DATA_TYPE_UINT64, 0},
+	{"dn_object_id", DATA_TYPE_UINT64, 0},
+	{"blk_id", DATA_TYPE_UINT64, 0},
+	{"col_idx", DATA_TYPE_UINT64, 0},
+	{"optional", DATA_TYPE_NVLIST, ZK_OPTIONAL},
+};
+
+static int
+zfs_mlec_receive_repair_data(const char *poolname, nvlist_t *innvl, nvlist_t *outnvl)
+{
+	// Retrieve the byte array from the nvlist
+	unsigned char *retrieved_data = NULL;
+	uint_t retrieved_data_size = 0;
+
+	uint16_t innvl_err = 0;
+	innvl_err += nvlist_lookup_byte_array(innvl, "data", &retrieved_data, &retrieved_data_size);
+
+	uint64_t objset_id;
+	innvl_err += nvlist_lookup_uint64(innvl, "objset_id", &objset_id);
+
+	uint64_t dn_object_id;
+	innvl_err += nvlist_lookup_uint64(innvl, "dn_object_id", &dn_object_id);
+
+	uint64_t blk_id;
+	innvl_err += nvlist_lookup_uint64(innvl, "blk_id", &blk_id);
+
+	uint64_t col_idx;
+	innvl_err += nvlist_lookup_uint64(innvl, "col_idx", &col_idx);
+
+	if (innvl_err)
+	{
+		zfs_dbgmsg("Error while parsing input to zfs_mlec_test");
+		return innvl_err;
+	}
+
+	zfs_dbgmsg("zfs_mlec_test() is called with objset %lld, dn %lld, blk %lld\n", objset_id, dn_object_id, blk_id);
+
+	// Now we should identify the block, row, and column, and call repair
+	// Reference to zio.c:zio_write() to how to initialize a zio write
+	// 1. Find the spa using the pool name
+	spa_t *spa;
+	spa_open(poolname, &spa, FTAG);
+
+	// Acquire the spa read config lock
+	spa_config_enter(spa, SCL_ALL, FTAG, RW_READER);
+
+	// 3. Find the vdev
+	vdev_t *vdev_top = vdev_lookup_top(spa, 0);
+	zfs_dbgmsg("Found the vdev %s, number of children %lld, first child path %s",
+			   vdev_top->vdev_devid,
+			   vdev_top->vdev_children,
+			   vdev_top->vdev_child[0]->vdev_physpath);
+
+	// 3. Find the dnode that is associated with the input data
+	dsl_pool_t *dsl_pool;
+	dsl_dataset_t *dsl_dataset;
+	if (dsl_pool_hold(poolname, FTAG, &dsl_pool)) {
+		zfs_dbgmsg("dsl_pool hold failed");
+		spa_close(spa, FTAG);
+		return 1;
+	}
+
+	if (dsl_dataset_hold_obj(spa->spa_dsl_pool, objset_id, FTAG, &dsl_dataset))
+	{
+		zfs_dbgmsg("dsl_dataset open failed");
+		spa_close(spa, FTAG);
+		return 1;
+	}
+
+	// 4. Find the dnode pointing to the FILE that we want to repair
+	dnode_t *dnode_repair;
+	if (dnode_hold(dsl_dataset->ds_objset, dn_object_id, FTAG, &dnode_repair))
+	{
+		zfs_dbgmsg("dnode_t open failed");
+		dsl_dataset_rele(dsl_dataset, FTAG);
+		spa_close(spa, FTAG);
+		return 1;
+	}
+
+	// 5. Find the blkptr pointing to the ACTUAL BLOCK that we are writing the data to
+	blkptr_t blk = dnode_repair->dn_phys->dn_blkptr[blk_id];
+
+	// 4. Get the binary data into abd
+	// Linear abd is required for adb_to_buf
+	abd_t *repair_adb = abd_alloc_linear(retrieved_data_size, B_FALSE);
+	abd_copy_from_buf(repair_adb, retrieved_data, retrieved_data_size);
+
+	// 5. We will use the rewrite pipeline
+	// zio_t *repair_zio = zio_rewrite(NULL, spa, 0, &blk, repair_adb, retrieved_data_size, NULL, NULL, ZIO_PRIORITY_NOW, ZIO_FLAG_CANFAIL, NULL);
+	// The old ioctl pipeline impl
+	zio_t *repair_pio = zio_root(spa, NULL, NULL, ZIO_FLAG_IO_REPAIR);
+	repair_pio->io_type = ZIO_TYPE_MLEC_WRITE_DATA;
+
+	// zio_t *repair_zio = zio_write_phys(repair_pio, vdev_top, 0, retrieved_data_size, repair_adb, 0, NULL, NULL, ZIO_PRIORITY_NOW, ZIO_FLAG_CANFAIL, NULL)
+
+	zio_t *repair_zio = zio_ioctl(repair_pio, spa, vdev_top, 0, NULL, NULL, ZIO_FLAG_IO_REPAIR);
+	repair_zio->io_abd = repair_adb;
+	repair_zio->io_size = retrieved_data_size;
+	repair_zio->mlec_write_target = &blk;
+	repair_zio->mlec_write_col_idx = col_idx;
+
+	zfs_dbgmsg("abd opened and buffer copied, content is %s", (char *) abd_to_buf(repair_zio->io_abd));
+
+	// 5. Call the zio pipeline
+	zio_nowait(repair_zio);
+
+	// Close the spa, otherwise the pool is always busy
+	abd_free(repair_adb);
+	dnode_rele(dnode_repair, FTAG);
+	dsl_pool_rele(dsl_pool, FTAG);
+	dsl_dataset_rele(dsl_dataset, FTAG);
+	
+	spa_config_exit(spa, SCL_ALL, FTAG);
+	spa_close(spa, FTAG);
+
+	return 0;
+}
+
+// Always return 0
+static int
+zfs_mlec_receive_repair_data_secpolicy(zfs_cmd_t *zc, nvlist_t *innvl, cred_t *cr)
+{
+	return 0;
 }
 
 static void
 zfs_ioctl_init(void)
 {
+	// MLEC stuff
+	zfs_ioctl_register("failed-chunks", ZFS_IOC_POOL_FAILED_CHUNKS,
+						zfs_ioctl_failed_chunks, zfs_pool_failed_chunks_sec_policy, POOL_NAME,
+						POOL_CHECK_NONE, B_FALSE, B_TRUE, zfs_keys_failed_chunks, ARRAY_SIZE(zfs_keys_failed_chunks));
+	zfs_ioctl_register("all-dnode", ZFS_IOC_POOL_ALL_DNODE,
+						zfs_ioc_pool_all_dnode, zfs_pool_all_dnode_sec_policy, POOL_NAME,
+						POOL_CHECK_NONE, B_FALSE, B_TRUE, zfs_keys_all_dnode, ARRAY_SIZE(zfs_keys_all_dnode));
+	zfs_ioctl_register("easy-scrub", ZFS_IOC_POOL_EASY_SCAN,
+						zfs_ioc_pool_easy_scan, zfs_pool_easy_scan_sec_policy , POOL_NAME,
+					   POOL_CHECK_NONE, B_FALSE, B_TRUE, zfs_keys_easy_scan, ARRAY_SIZE(zfs_keys_easy_scan));
+	zfs_ioctl_register("mlec-receive-data", ZFS_MLEC_RECEIVE_DATA,
+					   zfs_mlec_receive_repair_data, zfs_mlec_receive_repair_data_secpolicy, POOL_NAME,
+					   POOL_CHECK_NONE, B_FALSE, B_TRUE, zfs_keys_mlec_receive_repair_data, ARRAY_SIZE(zfs_keys_mlec_receive_repair_data));
+
 	zfs_ioctl_register("snapshot", ZFS_IOC_SNAPSHOT,
-	    zfs_ioc_snapshot, zfs_secpolicy_snapshot, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_snapshot, ARRAY_SIZE(zfs_keys_snapshot));
+					   zfs_ioc_snapshot, zfs_secpolicy_snapshot, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_snapshot, ARRAY_SIZE(zfs_keys_snapshot));
 
 	zfs_ioctl_register("log_history", ZFS_IOC_LOG_HISTORY,
-	    zfs_ioc_log_history, zfs_secpolicy_log_history, NO_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_FALSE,
-	    zfs_keys_log_history, ARRAY_SIZE(zfs_keys_log_history));
+					   zfs_ioc_log_history, zfs_secpolicy_log_history, NO_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_FALSE,
+					   zfs_keys_log_history, ARRAY_SIZE(zfs_keys_log_history));
 
 	zfs_ioctl_register("space_snaps", ZFS_IOC_SPACE_SNAPS,
-	    zfs_ioc_space_snaps, zfs_secpolicy_read, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
-	    zfs_keys_space_snaps, ARRAY_SIZE(zfs_keys_space_snaps));
+					   zfs_ioc_space_snaps, zfs_secpolicy_read, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
+					   zfs_keys_space_snaps, ARRAY_SIZE(zfs_keys_space_snaps));
 
 	zfs_ioctl_register("send", ZFS_IOC_SEND_NEW,
-	    zfs_ioc_send_new, zfs_secpolicy_send_new, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
-	    zfs_keys_send_new, ARRAY_SIZE(zfs_keys_send_new));
+					   zfs_ioc_send_new, zfs_secpolicy_send_new, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
+					   zfs_keys_send_new, ARRAY_SIZE(zfs_keys_send_new));
 
 	zfs_ioctl_register("send_space", ZFS_IOC_SEND_SPACE,
-	    zfs_ioc_send_space, zfs_secpolicy_read, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
-	    zfs_keys_send_space, ARRAY_SIZE(zfs_keys_send_space));
+					   zfs_ioc_send_space, zfs_secpolicy_read, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
+					   zfs_keys_send_space, ARRAY_SIZE(zfs_keys_send_space));
 
 	zfs_ioctl_register("create", ZFS_IOC_CREATE,
-	    zfs_ioc_create, zfs_secpolicy_create_clone, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_create, ARRAY_SIZE(zfs_keys_create));
+					   zfs_ioc_create, zfs_secpolicy_create_clone, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_create, ARRAY_SIZE(zfs_keys_create));
 
 	zfs_ioctl_register("clone", ZFS_IOC_CLONE,
-	    zfs_ioc_clone, zfs_secpolicy_create_clone, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_clone, ARRAY_SIZE(zfs_keys_clone));
+					   zfs_ioc_clone, zfs_secpolicy_create_clone, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_clone, ARRAY_SIZE(zfs_keys_clone));
 
 	zfs_ioctl_register("remap", ZFS_IOC_REMAP,
-	    zfs_ioc_remap, zfs_secpolicy_none, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_TRUE,
-	    zfs_keys_remap, ARRAY_SIZE(zfs_keys_remap));
+					   zfs_ioc_remap, zfs_secpolicy_none, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_TRUE,
+					   zfs_keys_remap, ARRAY_SIZE(zfs_keys_remap));
 
 	zfs_ioctl_register("destroy_snaps", ZFS_IOC_DESTROY_SNAPS,
-	    zfs_ioc_destroy_snaps, zfs_secpolicy_destroy_snaps, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_destroy_snaps, ARRAY_SIZE(zfs_keys_destroy_snaps));
+					   zfs_ioc_destroy_snaps, zfs_secpolicy_destroy_snaps, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_destroy_snaps, ARRAY_SIZE(zfs_keys_destroy_snaps));
 
 	zfs_ioctl_register("hold", ZFS_IOC_HOLD,
-	    zfs_ioc_hold, zfs_secpolicy_hold, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_hold, ARRAY_SIZE(zfs_keys_hold));
+					   zfs_ioc_hold, zfs_secpolicy_hold, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_hold, ARRAY_SIZE(zfs_keys_hold));
 	zfs_ioctl_register("release", ZFS_IOC_RELEASE,
-	    zfs_ioc_release, zfs_secpolicy_release, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_release, ARRAY_SIZE(zfs_keys_release));
+					   zfs_ioc_release, zfs_secpolicy_release, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_release, ARRAY_SIZE(zfs_keys_release));
 
 	zfs_ioctl_register("get_holds", ZFS_IOC_GET_HOLDS,
-	    zfs_ioc_get_holds, zfs_secpolicy_read, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
-	    zfs_keys_get_holds, ARRAY_SIZE(zfs_keys_get_holds));
+					   zfs_ioc_get_holds, zfs_secpolicy_read, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
+					   zfs_keys_get_holds, ARRAY_SIZE(zfs_keys_get_holds));
 
 	zfs_ioctl_register("rollback", ZFS_IOC_ROLLBACK,
-	    zfs_ioc_rollback, zfs_secpolicy_rollback, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_TRUE,
-	    zfs_keys_rollback, ARRAY_SIZE(zfs_keys_rollback));
+					   zfs_ioc_rollback, zfs_secpolicy_rollback, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_TRUE,
+					   zfs_keys_rollback, ARRAY_SIZE(zfs_keys_rollback));
 
 	zfs_ioctl_register("bookmark", ZFS_IOC_BOOKMARK,
-	    zfs_ioc_bookmark, zfs_secpolicy_bookmark, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_bookmark, ARRAY_SIZE(zfs_keys_bookmark));
+					   zfs_ioc_bookmark, zfs_secpolicy_bookmark, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_bookmark, ARRAY_SIZE(zfs_keys_bookmark));
 
 	zfs_ioctl_register("get_bookmarks", ZFS_IOC_GET_BOOKMARKS,
-	    zfs_ioc_get_bookmarks, zfs_secpolicy_read, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
-	    zfs_keys_get_bookmarks, ARRAY_SIZE(zfs_keys_get_bookmarks));
+					   zfs_ioc_get_bookmarks, zfs_secpolicy_read, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE,
+					   zfs_keys_get_bookmarks, ARRAY_SIZE(zfs_keys_get_bookmarks));
 
 	zfs_ioctl_register("get_bookmark_props", ZFS_IOC_GET_BOOKMARK_PROPS,
-	    zfs_ioc_get_bookmark_props, zfs_secpolicy_read, ENTITY_NAME,
-	    POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE, zfs_keys_get_bookmark_props,
-	    ARRAY_SIZE(zfs_keys_get_bookmark_props));
+					   zfs_ioc_get_bookmark_props, zfs_secpolicy_read, ENTITY_NAME,
+					   POOL_CHECK_SUSPENDED, B_FALSE, B_FALSE, zfs_keys_get_bookmark_props,
+					   ARRAY_SIZE(zfs_keys_get_bookmark_props));
 
 	zfs_ioctl_register("destroy_bookmarks", ZFS_IOC_DESTROY_BOOKMARKS,
-	    zfs_ioc_destroy_bookmarks, zfs_secpolicy_destroy_bookmarks,
-	    POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_destroy_bookmarks,
-	    ARRAY_SIZE(zfs_keys_destroy_bookmarks));
+					   zfs_ioc_destroy_bookmarks, zfs_secpolicy_destroy_bookmarks,
+					   POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_destroy_bookmarks,
+					   ARRAY_SIZE(zfs_keys_destroy_bookmarks));
 
 	zfs_ioctl_register("receive", ZFS_IOC_RECV_NEW,
-	    zfs_ioc_recv_new, zfs_secpolicy_recv_new, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_recv_new, ARRAY_SIZE(zfs_keys_recv_new));
+					   zfs_ioc_recv_new, zfs_secpolicy_recv_new, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_recv_new, ARRAY_SIZE(zfs_keys_recv_new));
 	zfs_ioctl_register("load-key", ZFS_IOC_LOAD_KEY,
-	    zfs_ioc_load_key, zfs_secpolicy_load_key,
-	    DATASET_NAME, POOL_CHECK_SUSPENDED, B_TRUE, B_TRUE,
-	    zfs_keys_load_key, ARRAY_SIZE(zfs_keys_load_key));
+					   zfs_ioc_load_key, zfs_secpolicy_load_key,
+					   DATASET_NAME, POOL_CHECK_SUSPENDED, B_TRUE, B_TRUE,
+					   zfs_keys_load_key, ARRAY_SIZE(zfs_keys_load_key));
 	zfs_ioctl_register("unload-key", ZFS_IOC_UNLOAD_KEY,
-	    zfs_ioc_unload_key, zfs_secpolicy_load_key,
-	    DATASET_NAME, POOL_CHECK_SUSPENDED, B_TRUE, B_TRUE,
-	    zfs_keys_unload_key, ARRAY_SIZE(zfs_keys_unload_key));
+					   zfs_ioc_unload_key, zfs_secpolicy_load_key,
+					   DATASET_NAME, POOL_CHECK_SUSPENDED, B_TRUE, B_TRUE,
+					   zfs_keys_unload_key, ARRAY_SIZE(zfs_keys_unload_key));
 	zfs_ioctl_register("change-key", ZFS_IOC_CHANGE_KEY,
-	    zfs_ioc_change_key, zfs_secpolicy_change_key,
-	    DATASET_NAME, POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY,
-	    B_TRUE, B_TRUE, zfs_keys_change_key,
-	    ARRAY_SIZE(zfs_keys_change_key));
+					   zfs_ioc_change_key, zfs_secpolicy_change_key,
+					   DATASET_NAME, POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY,
+					   B_TRUE, B_TRUE, zfs_keys_change_key,
+					   ARRAY_SIZE(zfs_keys_change_key));
 
 	zfs_ioctl_register("sync", ZFS_IOC_POOL_SYNC,
-	    zfs_ioc_pool_sync, zfs_secpolicy_none, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_FALSE,
-	    zfs_keys_pool_sync, ARRAY_SIZE(zfs_keys_pool_sync));
+					   zfs_ioc_pool_sync, zfs_secpolicy_none, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_FALSE,
+					   zfs_keys_pool_sync, ARRAY_SIZE(zfs_keys_pool_sync));
 	zfs_ioctl_register("reopen", ZFS_IOC_POOL_REOPEN, zfs_ioc_pool_reopen,
-	    zfs_secpolicy_config, POOL_NAME, POOL_CHECK_SUSPENDED, B_TRUE,
-	    B_TRUE, zfs_keys_pool_reopen, ARRAY_SIZE(zfs_keys_pool_reopen));
+					   zfs_secpolicy_config, POOL_NAME, POOL_CHECK_SUSPENDED, B_TRUE,
+					   B_TRUE, zfs_keys_pool_reopen, ARRAY_SIZE(zfs_keys_pool_reopen));
 
 	zfs_ioctl_register("channel_program", ZFS_IOC_CHANNEL_PROGRAM,
-	    zfs_ioc_channel_program, zfs_secpolicy_config,
-	    POOL_NAME, POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE,
-	    B_TRUE, zfs_keys_channel_program,
-	    ARRAY_SIZE(zfs_keys_channel_program));
+					   zfs_ioc_channel_program, zfs_secpolicy_config,
+					   POOL_NAME, POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE,
+					   B_TRUE, zfs_keys_channel_program,
+					   ARRAY_SIZE(zfs_keys_channel_program));
 
 	zfs_ioctl_register("redact", ZFS_IOC_REDACT,
-	    zfs_ioc_redact, zfs_secpolicy_config, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_redact, ARRAY_SIZE(zfs_keys_redact));
+					   zfs_ioc_redact, zfs_secpolicy_config, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_redact, ARRAY_SIZE(zfs_keys_redact));
 
 	zfs_ioctl_register("zpool_checkpoint", ZFS_IOC_POOL_CHECKPOINT,
-	    zfs_ioc_pool_checkpoint, zfs_secpolicy_config, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_pool_checkpoint, ARRAY_SIZE(zfs_keys_pool_checkpoint));
+					   zfs_ioc_pool_checkpoint, zfs_secpolicy_config, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_pool_checkpoint, ARRAY_SIZE(zfs_keys_pool_checkpoint));
 
 	zfs_ioctl_register("zpool_discard_checkpoint",
-	    ZFS_IOC_POOL_DISCARD_CHECKPOINT, zfs_ioc_pool_discard_checkpoint,
-	    zfs_secpolicy_config, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_pool_discard_checkpoint,
-	    ARRAY_SIZE(zfs_keys_pool_discard_checkpoint));
+					   ZFS_IOC_POOL_DISCARD_CHECKPOINT, zfs_ioc_pool_discard_checkpoint,
+					   zfs_secpolicy_config, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_pool_discard_checkpoint,
+					   ARRAY_SIZE(zfs_keys_pool_discard_checkpoint));
 
 	zfs_ioctl_register("initialize", ZFS_IOC_POOL_INITIALIZE,
-	    zfs_ioc_pool_initialize, zfs_secpolicy_config, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_pool_initialize, ARRAY_SIZE(zfs_keys_pool_initialize));
+					   zfs_ioc_pool_initialize, zfs_secpolicy_config, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_pool_initialize, ARRAY_SIZE(zfs_keys_pool_initialize));
 
 	zfs_ioctl_register("trim", ZFS_IOC_POOL_TRIM,
-	    zfs_ioc_pool_trim, zfs_secpolicy_config, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
-	    zfs_keys_pool_trim, ARRAY_SIZE(zfs_keys_pool_trim));
+					   zfs_ioc_pool_trim, zfs_secpolicy_config, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_TRUE, B_TRUE,
+					   zfs_keys_pool_trim, ARRAY_SIZE(zfs_keys_pool_trim));
 
 	zfs_ioctl_register("wait", ZFS_IOC_WAIT,
-	    zfs_ioc_wait, zfs_secpolicy_none, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_FALSE,
-	    zfs_keys_pool_wait, ARRAY_SIZE(zfs_keys_pool_wait));
+					   zfs_ioc_wait, zfs_secpolicy_none, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_FALSE,
+					   zfs_keys_pool_wait, ARRAY_SIZE(zfs_keys_pool_wait));
 
 	zfs_ioctl_register("wait_fs", ZFS_IOC_WAIT_FS,
-	    zfs_ioc_wait_fs, zfs_secpolicy_none, DATASET_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_FALSE,
-	    zfs_keys_fs_wait, ARRAY_SIZE(zfs_keys_fs_wait));
+					   zfs_ioc_wait_fs, zfs_secpolicy_none, DATASET_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_FALSE,
+					   zfs_keys_fs_wait, ARRAY_SIZE(zfs_keys_fs_wait));
 
 	zfs_ioctl_register("set_bootenv", ZFS_IOC_SET_BOOTENV,
-	    zfs_ioc_set_bootenv, zfs_secpolicy_config, POOL_NAME,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_TRUE,
-	    zfs_keys_set_bootenv, ARRAY_SIZE(zfs_keys_set_bootenv));
+					   zfs_ioc_set_bootenv, zfs_secpolicy_config, POOL_NAME,
+					   POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY, B_FALSE, B_TRUE,
+					   zfs_keys_set_bootenv, ARRAY_SIZE(zfs_keys_set_bootenv));
 
 	zfs_ioctl_register("get_bootenv", ZFS_IOC_GET_BOOTENV,
-	    zfs_ioc_get_bootenv, zfs_secpolicy_none, POOL_NAME,
-	    POOL_CHECK_SUSPENDED, B_FALSE, B_TRUE,
-	    zfs_keys_get_bootenv, ARRAY_SIZE(zfs_keys_get_bootenv));
+					   zfs_ioc_get_bootenv, zfs_secpolicy_none, POOL_NAME,
+					   POOL_CHECK_SUSPENDED, B_FALSE, B_TRUE,
+					   zfs_keys_get_bootenv, ARRAY_SIZE(zfs_keys_get_bootenv));
 
 	/* IOCTLS that use the legacy function signature */
 
 	zfs_ioctl_register_legacy(ZFS_IOC_POOL_FREEZE, zfs_ioc_pool_freeze,
-	    zfs_secpolicy_config, NO_NAME, B_FALSE, POOL_CHECK_READONLY);
+							  zfs_secpolicy_config, NO_NAME, B_FALSE, POOL_CHECK_READONLY);
 
 	zfs_ioctl_register_pool(ZFS_IOC_POOL_CREATE, zfs_ioc_pool_create,
-	    zfs_secpolicy_config, B_TRUE, POOL_CHECK_NONE);
+							zfs_secpolicy_config, B_TRUE, POOL_CHECK_NONE);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_POOL_SCAN,
-	    zfs_ioc_pool_scan);
+								   zfs_ioc_pool_scan);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_POOL_UPGRADE,
-	    zfs_ioc_pool_upgrade);
+								   zfs_ioc_pool_upgrade);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_ADD,
-	    zfs_ioc_vdev_add);
+								   zfs_ioc_vdev_add);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_REMOVE,
-	    zfs_ioc_vdev_remove);
+								   zfs_ioc_vdev_remove);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_SET_STATE,
-	    zfs_ioc_vdev_set_state);
+								   zfs_ioc_vdev_set_state);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_ATTACH,
-	    zfs_ioc_vdev_attach);
+								   zfs_ioc_vdev_attach);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_DETACH,
-	    zfs_ioc_vdev_detach);
+								   zfs_ioc_vdev_detach);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_SETPATH,
-	    zfs_ioc_vdev_setpath);
+								   zfs_ioc_vdev_setpath);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_SETFRU,
-	    zfs_ioc_vdev_setfru);
+								   zfs_ioc_vdev_setfru);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_POOL_SET_PROPS,
-	    zfs_ioc_pool_set_props);
+								   zfs_ioc_pool_set_props);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_SPLIT,
-	    zfs_ioc_vdev_split);
+								   zfs_ioc_vdev_split);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_POOL_REGUID,
-	    zfs_ioc_pool_reguid);
+								   zfs_ioc_pool_reguid);
 
 	zfs_ioctl_register_pool_meta(ZFS_IOC_POOL_CONFIGS,
-	    zfs_ioc_pool_configs, zfs_secpolicy_none);
+								 zfs_ioc_pool_configs, zfs_secpolicy_none);
 	zfs_ioctl_register_pool_meta(ZFS_IOC_POOL_TRYIMPORT,
-	    zfs_ioc_pool_tryimport, zfs_secpolicy_config);
+								 zfs_ioc_pool_tryimport, zfs_secpolicy_config);
 	zfs_ioctl_register_pool_meta(ZFS_IOC_INJECT_FAULT,
-	    zfs_ioc_inject_fault, zfs_secpolicy_inject);
+								 zfs_ioc_inject_fault, zfs_secpolicy_inject);
 	zfs_ioctl_register_pool_meta(ZFS_IOC_CLEAR_FAULT,
-	    zfs_ioc_clear_fault, zfs_secpolicy_inject);
+								 zfs_ioc_clear_fault, zfs_secpolicy_inject);
 	zfs_ioctl_register_pool_meta(ZFS_IOC_INJECT_LIST_NEXT,
-	    zfs_ioc_inject_list_next, zfs_secpolicy_inject);
+								 zfs_ioc_inject_list_next, zfs_secpolicy_inject);
 
 	/*
 	 * pool destroy, and export don't log the history as part of
@@ -7176,94 +8223,94 @@ zfs_ioctl_init(void)
 	 * does the logging of those commands.
 	 */
 	zfs_ioctl_register_pool(ZFS_IOC_POOL_DESTROY, zfs_ioc_pool_destroy,
-	    zfs_secpolicy_config, B_FALSE, POOL_CHECK_SUSPENDED);
+							zfs_secpolicy_config, B_FALSE, POOL_CHECK_SUSPENDED);
 	zfs_ioctl_register_pool(ZFS_IOC_POOL_EXPORT, zfs_ioc_pool_export,
-	    zfs_secpolicy_config, B_FALSE, POOL_CHECK_SUSPENDED);
+							zfs_secpolicy_config, B_FALSE, POOL_CHECK_SUSPENDED);
 
 	zfs_ioctl_register_pool(ZFS_IOC_POOL_STATS, zfs_ioc_pool_stats,
-	    zfs_secpolicy_read, B_FALSE, POOL_CHECK_NONE);
+							zfs_secpolicy_read, B_FALSE, POOL_CHECK_NONE);
 	zfs_ioctl_register_pool(ZFS_IOC_POOL_GET_PROPS, zfs_ioc_pool_get_props,
-	    zfs_secpolicy_read, B_FALSE, POOL_CHECK_NONE);
+							zfs_secpolicy_read, B_FALSE, POOL_CHECK_NONE);
 
 	zfs_ioctl_register_pool(ZFS_IOC_ERROR_LOG, zfs_ioc_error_log,
-	    zfs_secpolicy_inject, B_FALSE, POOL_CHECK_SUSPENDED);
+							zfs_secpolicy_inject, B_FALSE, POOL_CHECK_SUSPENDED);
 	zfs_ioctl_register_pool(ZFS_IOC_DSOBJ_TO_DSNAME,
-	    zfs_ioc_dsobj_to_dsname,
-	    zfs_secpolicy_diff, B_FALSE, POOL_CHECK_SUSPENDED);
+							zfs_ioc_dsobj_to_dsname,
+							zfs_secpolicy_diff, B_FALSE, POOL_CHECK_SUSPENDED);
 	zfs_ioctl_register_pool(ZFS_IOC_POOL_GET_HISTORY,
-	    zfs_ioc_pool_get_history,
-	    zfs_secpolicy_config, B_FALSE, POOL_CHECK_SUSPENDED);
+							zfs_ioc_pool_get_history,
+							zfs_secpolicy_config, B_FALSE, POOL_CHECK_SUSPENDED);
 
 	zfs_ioctl_register_pool(ZFS_IOC_POOL_IMPORT, zfs_ioc_pool_import,
-	    zfs_secpolicy_config, B_TRUE, POOL_CHECK_NONE);
+							zfs_secpolicy_config, B_TRUE, POOL_CHECK_NONE);
 
 	zfs_ioctl_register_pool(ZFS_IOC_CLEAR, zfs_ioc_clear,
-	    zfs_secpolicy_config, B_TRUE, POOL_CHECK_READONLY);
+							zfs_secpolicy_config, B_TRUE, POOL_CHECK_READONLY);
 
 	zfs_ioctl_register_dataset_read(ZFS_IOC_SPACE_WRITTEN,
-	    zfs_ioc_space_written);
+									zfs_ioc_space_written);
 	zfs_ioctl_register_dataset_read(ZFS_IOC_OBJSET_RECVD_PROPS,
-	    zfs_ioc_objset_recvd_props);
+									zfs_ioc_objset_recvd_props);
 	zfs_ioctl_register_dataset_read(ZFS_IOC_NEXT_OBJ,
-	    zfs_ioc_next_obj);
+									zfs_ioc_next_obj);
 	zfs_ioctl_register_dataset_read(ZFS_IOC_GET_FSACL,
-	    zfs_ioc_get_fsacl);
+									zfs_ioc_get_fsacl);
 	zfs_ioctl_register_dataset_read(ZFS_IOC_OBJSET_STATS,
-	    zfs_ioc_objset_stats);
+									zfs_ioc_objset_stats);
 	zfs_ioctl_register_dataset_read(ZFS_IOC_OBJSET_ZPLPROPS,
-	    zfs_ioc_objset_zplprops);
+									zfs_ioc_objset_zplprops);
 	zfs_ioctl_register_dataset_read(ZFS_IOC_DATASET_LIST_NEXT,
-	    zfs_ioc_dataset_list_next);
+									zfs_ioc_dataset_list_next);
 	zfs_ioctl_register_dataset_read(ZFS_IOC_SNAPSHOT_LIST_NEXT,
-	    zfs_ioc_snapshot_list_next);
+									zfs_ioc_snapshot_list_next);
 	zfs_ioctl_register_dataset_read(ZFS_IOC_SEND_PROGRESS,
-	    zfs_ioc_send_progress);
+									zfs_ioc_send_progress);
 
 	zfs_ioctl_register_dataset_read_secpolicy(ZFS_IOC_DIFF,
-	    zfs_ioc_diff, zfs_secpolicy_diff);
+											  zfs_ioc_diff, zfs_secpolicy_diff);
 	zfs_ioctl_register_dataset_read_secpolicy(ZFS_IOC_OBJ_TO_STATS,
-	    zfs_ioc_obj_to_stats, zfs_secpolicy_diff);
+											  zfs_ioc_obj_to_stats, zfs_secpolicy_diff);
 	zfs_ioctl_register_dataset_read_secpolicy(ZFS_IOC_OBJ_TO_PATH,
-	    zfs_ioc_obj_to_path, zfs_secpolicy_diff);
+											  zfs_ioc_obj_to_path, zfs_secpolicy_diff);
 	zfs_ioctl_register_dataset_read_secpolicy(ZFS_IOC_USERSPACE_ONE,
-	    zfs_ioc_userspace_one, zfs_secpolicy_userspace_one);
+											  zfs_ioc_userspace_one, zfs_secpolicy_userspace_one);
 	zfs_ioctl_register_dataset_read_secpolicy(ZFS_IOC_USERSPACE_MANY,
-	    zfs_ioc_userspace_many, zfs_secpolicy_userspace_many);
+											  zfs_ioc_userspace_many, zfs_secpolicy_userspace_many);
 	zfs_ioctl_register_dataset_read_secpolicy(ZFS_IOC_SEND,
-	    zfs_ioc_send, zfs_secpolicy_send);
+											  zfs_ioc_send, zfs_secpolicy_send);
 
 	zfs_ioctl_register_dataset_modify(ZFS_IOC_SET_PROP, zfs_ioc_set_prop,
-	    zfs_secpolicy_none);
+									  zfs_secpolicy_none);
 	zfs_ioctl_register_dataset_modify(ZFS_IOC_DESTROY, zfs_ioc_destroy,
-	    zfs_secpolicy_destroy);
+									  zfs_secpolicy_destroy);
 	zfs_ioctl_register_dataset_modify(ZFS_IOC_RENAME, zfs_ioc_rename,
-	    zfs_secpolicy_rename);
+									  zfs_secpolicy_rename);
 	zfs_ioctl_register_dataset_modify(ZFS_IOC_RECV, zfs_ioc_recv,
-	    zfs_secpolicy_recv);
+									  zfs_secpolicy_recv);
 	zfs_ioctl_register_dataset_modify(ZFS_IOC_PROMOTE, zfs_ioc_promote,
-	    zfs_secpolicy_promote);
+									  zfs_secpolicy_promote);
 	zfs_ioctl_register_dataset_modify(ZFS_IOC_INHERIT_PROP,
-	    zfs_ioc_inherit_prop, zfs_secpolicy_inherit_prop);
+									  zfs_ioc_inherit_prop, zfs_secpolicy_inherit_prop);
 	zfs_ioctl_register_dataset_modify(ZFS_IOC_SET_FSACL, zfs_ioc_set_fsacl,
-	    zfs_secpolicy_set_fsacl);
+									  zfs_secpolicy_set_fsacl);
 
 	zfs_ioctl_register_dataset_nolog(ZFS_IOC_SHARE, zfs_ioc_share,
-	    zfs_secpolicy_share, POOL_CHECK_NONE);
+									 zfs_secpolicy_share, POOL_CHECK_NONE);
 	zfs_ioctl_register_dataset_nolog(ZFS_IOC_SMB_ACL, zfs_ioc_smb_acl,
-	    zfs_secpolicy_smb_acl, POOL_CHECK_NONE);
+									 zfs_secpolicy_smb_acl, POOL_CHECK_NONE);
 	zfs_ioctl_register_dataset_nolog(ZFS_IOC_USERSPACE_UPGRADE,
-	    zfs_ioc_userspace_upgrade, zfs_secpolicy_userspace_upgrade,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY);
+									 zfs_ioc_userspace_upgrade, zfs_secpolicy_userspace_upgrade,
+									 POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY);
 	zfs_ioctl_register_dataset_nolog(ZFS_IOC_TMP_SNAPSHOT,
-	    zfs_ioc_tmp_snapshot, zfs_secpolicy_tmp_snapshot,
-	    POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY);
+									 zfs_ioc_tmp_snapshot, zfs_secpolicy_tmp_snapshot,
+									 POOL_CHECK_SUSPENDED | POOL_CHECK_READONLY);
 
 	zfs_ioctl_register_legacy(ZFS_IOC_EVENTS_NEXT, zfs_ioc_events_next,
-	    zfs_secpolicy_config, NO_NAME, B_FALSE, POOL_CHECK_NONE);
+							  zfs_secpolicy_config, NO_NAME, B_FALSE, POOL_CHECK_NONE);
 	zfs_ioctl_register_legacy(ZFS_IOC_EVENTS_CLEAR, zfs_ioc_events_clear,
-	    zfs_secpolicy_config, NO_NAME, B_FALSE, POOL_CHECK_NONE);
+							  zfs_secpolicy_config, NO_NAME, B_FALSE, POOL_CHECK_NONE);
 	zfs_ioctl_register_legacy(ZFS_IOC_EVENTS_SEEK, zfs_ioc_events_seek,
-	    zfs_secpolicy_config, NO_NAME, B_FALSE, POOL_CHECK_NONE);
+							  zfs_secpolicy_config, NO_NAME, B_FALSE, POOL_CHECK_NONE);
 
 	zfs_ioctl_init_os();
 }
@@ -7287,7 +8334,8 @@ zfs_check_input_nvpairs(nvlist_t *innvl, const zfs_ioc_vec_t *vec)
 	 * examine each input pair
 	 */
 	for (nvpair_t *pair = nvlist_next_nvpair(innvl, NULL);
-	    pair != NULL; pair = nvlist_next_nvpair(innvl, pair)) {
+		 pair != NULL; pair = nvlist_next_nvpair(innvl, pair))
+	{
 		char *name = nvpair_name(pair);
 		data_type_t type = nvpair_type(pair);
 		boolean_t identified = B_FALSE;
@@ -7295,16 +8343,18 @@ zfs_check_input_nvpairs(nvlist_t *innvl, const zfs_ioc_vec_t *vec)
 		/*
 		 * check pair against the documented names and type
 		 */
-		for (int k = 0; k < vec->zvec_nvl_key_count; k++) {
+		for (int k = 0; k < vec->zvec_nvl_key_count; k++)
+		{
 			/* if not a wild card name, check for an exact match */
 			if ((nvl_keys[k].zkey_flags & ZK_WILDCARDLIST) == 0 &&
-			    strcmp(nvl_keys[k].zkey_name, name) != 0)
+				strcmp(nvl_keys[k].zkey_name, name) != 0)
 				continue;
 
 			identified = B_TRUE;
 
 			if (nvl_keys[k].zkey_type != DATA_TYPE_ANY &&
-			    nvl_keys[k].zkey_type != type) {
+				nvl_keys[k].zkey_type != type)
+			{
 				return (SET_ERROR(ZFS_ERR_IOC_ARG_BADTYPE));
 			}
 
@@ -7317,18 +8367,21 @@ zfs_check_input_nvpairs(nvlist_t *innvl, const zfs_ioc_vec_t *vec)
 
 		/* allow an 'optional' key, everything else is invalid */
 		if (!identified &&
-		    (strcmp(name, "optional") != 0 ||
-		    type != DATA_TYPE_NVLIST)) {
+			(strcmp(name, "optional") != 0 ||
+			 type != DATA_TYPE_NVLIST))
+		{
 			return (SET_ERROR(ZFS_ERR_IOC_ARG_UNAVAIL));
 		}
 	}
 
 	/* verify that all required keys were found */
-	for (int k = 0; k < vec->zvec_nvl_key_count; k++) {
+	for (int k = 0; k < vec->zvec_nvl_key_count; k++)
+	{
 		if (nvl_keys[k].zkey_flags & ZK_OPTIONAL)
 			continue;
 
-		if (nvl_keys[k].zkey_flags & ZK_WILDCARDLIST) {
+		if (nvl_keys[k].zkey_flags & ZK_WILDCARDLIST)
+		{
 			/* at least one non-optional key is expected here */
 			if (!required_keys_found)
 				return (SET_ERROR(ZFS_ERR_IOC_ARG_REQUIRED));
@@ -7344,19 +8397,20 @@ zfs_check_input_nvpairs(nvlist_t *innvl, const zfs_ioc_vec_t *vec)
 
 static int
 pool_status_check(const char *name, zfs_ioc_namecheck_t type,
-    zfs_ioc_poolcheck_t check)
+				  zfs_ioc_poolcheck_t check)
 {
 	spa_t *spa;
 	int error;
 
 	ASSERT(type == POOL_NAME || type == DATASET_NAME ||
-	    type == ENTITY_NAME);
+		   type == ENTITY_NAME);
 
 	if (check & POOL_CHECK_NONE)
 		return (0);
 
 	error = spa_open(name, &spa, FTAG);
-	if (error == 0) {
+	if (error == 0)
+	{
 		if ((check & POOL_CHECK_SUSPENDED) && spa_suspended(spa))
 			error = SET_ERROR(EAGAIN);
 		else if ((check & POOL_CHECK_READONLY) && !spa_writeable(spa))
@@ -7366,8 +8420,7 @@ pool_status_check(const char *name, zfs_ioc_namecheck_t type,
 	return (error);
 }
 
-int
-zfsdev_getminor(zfs_file_t *fp, minor_t *minorp)
+int zfsdev_getminor(zfs_file_t *fp, minor_t *minorp)
 {
 	zfsdev_state_t *zs, *fpd;
 
@@ -7379,12 +8432,14 @@ zfsdev_getminor(zfs_file_t *fp, minor_t *minorp)
 
 	mutex_enter(&zfsdev_state_lock);
 
-	for (zs = zfsdev_state_list; zs != NULL; zs = zs->zs_next) {
+	for (zs = zfsdev_state_list; zs != NULL; zs = zs->zs_next)
+	{
 
 		if (zs->zs_minor == -1)
 			continue;
 
-		if (fpd == zs) {
+		if (fpd == zs)
+		{
 			*minorp = fpd->zs_minor;
 			mutex_exit(&zfsdev_state_lock);
 			return (0);
@@ -7401,10 +8456,13 @@ zfsdev_get_state_impl(minor_t minor, enum zfsdev_state_type which)
 {
 	zfsdev_state_t *zs;
 
-	for (zs = zfsdev_state_list; zs != NULL; zs = zs->zs_next) {
-		if (zs->zs_minor == minor) {
+	for (zs = zfsdev_state_list; zs != NULL; zs = zs->zs_next)
+	{
+		if (zs->zs_minor == minor)
+		{
 			membar_consumer();
-			switch (which) {
+			switch (which)
+			{
 			case ZST_ONEXIT:
 				return (zs->zs_onexit);
 			case ZST_ZEVENT:
@@ -7440,10 +8498,12 @@ zfsdev_minor_alloc(void)
 
 	ASSERT(MUTEX_HELD(&zfsdev_state_lock));
 
-	for (m = last_minor + 1; m != last_minor; m++) {
+	for (m = last_minor + 1; m != last_minor; m++)
+	{
 		if (m > ZFSDEV_MAX_MINOR)
 			m = 1;
-		if (zfsdev_get_state_impl(m, ZST_ALL) == NULL) {
+		if (zfsdev_get_state_impl(m, ZST_ALL) == NULL)
+		{
 			last_minor = m;
 			return (m);
 		}
@@ -7452,9 +8512,9 @@ zfsdev_minor_alloc(void)
 	return (0);
 }
 
-long
-zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
+long zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 {
+	zfs_dbgmsg("zfsdev_ioctl_common called with vecnum %d", vecnum);
 	int error, cmd;
 	const zfs_ioc_vec_t *vec;
 	char *saved_poolname = NULL;
@@ -7466,11 +8526,13 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 
 	cmd = vecnum;
 	error = 0;
-	if (vecnum >= sizeof (zfs_ioc_vec) / sizeof (zfs_ioc_vec[0]))
+	if (vecnum >= sizeof(zfs_ioc_vec) / sizeof(zfs_ioc_vec[0]))
 		return (SET_ERROR(ZFS_ERR_IOC_CMD_UNAVAIL));
 
 	vec = &zfs_ioc_vec[vecnum];
 
+	zfs_dbgmsg("zfsdev_ioctl_common called with %s, %s, %d\n", vec->zvec_name, zc->zc_name, flag);
+
 	/*
 	 * The registered ioctl list may be sparse, verify that either
 	 * a normal or legacy handler are registered.
@@ -7480,7 +8542,8 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 
 	zc->zc_iflags = flag & FKIOCTL;
 	max_nvlist_src_size = zfs_max_nvlist_src_size_os();
-	if (zc->zc_nvlist_src_size > max_nvlist_src_size) {
+	if (zc->zc_nvlist_src_size > max_nvlist_src_size)
+	{
 		/*
 		 * Make sure the user doesn't pass in an insane value for
 		 * zc_nvlist_src_size.  We have to check, since we will end
@@ -7494,11 +8557,12 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 		 * needs to be expanded to hold the nvlist.  See
 		 * zcmd_expand_dst_nvlist() for details.
 		 */
-		error = SET_ERROR(EINVAL);	/* User's size too big */
-
-	} else if (zc->zc_nvlist_src_size != 0) {
+		error = SET_ERROR(EINVAL); /* User's size too big */
+	}
+	else if (zc->zc_nvlist_src_size != 0)
+	{
 		error = get_nvlist(zc->zc_nvlist_src, zc->zc_nvlist_src_size,
-		    zc->zc_iflags, &innvl);
+						   zc->zc_iflags, &innvl);
 		if (error != 0)
 			goto out;
 	}
@@ -7507,14 +8571,15 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 	 * Ensure that all pool/dataset names are valid before we pass down to
 	 * the lower layers.
 	 */
-	zc->zc_name[sizeof (zc->zc_name) - 1] = '\0';
-	switch (vec->zvec_namecheck) {
+	zc->zc_name[sizeof(zc->zc_name) - 1] = '\0';
+	switch (vec->zvec_namecheck)
+	{
 	case POOL_NAME:
 		if (pool_namecheck(zc->zc_name, NULL, NULL) != 0)
 			error = SET_ERROR(EINVAL);
 		else
 			error = pool_status_check(zc->zc_name,
-			    vec->zvec_namecheck, vec->zvec_pool_check);
+									  vec->zvec_namecheck, vec->zvec_pool_check);
 		break;
 
 	case DATASET_NAME:
@@ -7522,21 +8587,26 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 			error = SET_ERROR(EINVAL);
 		else
 			error = pool_status_check(zc->zc_name,
-			    vec->zvec_namecheck, vec->zvec_pool_check);
+									  vec->zvec_namecheck, vec->zvec_pool_check);
 		break;
 
 	case ENTITY_NAME:
-		if (entity_namecheck(zc->zc_name, NULL, NULL) != 0) {
+		if (entity_namecheck(zc->zc_name, NULL, NULL) != 0)
+		{
 			error = SET_ERROR(EINVAL);
-		} else {
+		}
+		else
+		{
 			error = pool_status_check(zc->zc_name,
-			    vec->zvec_namecheck, vec->zvec_pool_check);
+									  vec->zvec_namecheck, vec->zvec_pool_check);
 		}
 		break;
 
 	case NO_NAME:
 		break;
 	}
+
+	zfs_dbgmsg("zfsdev_ioctl_common(): pool valid");
 	/*
 	 * Ensure that all input pairs are valid before we pass them down
 	 * to the lower layers.
@@ -7545,13 +8615,20 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 	 * required pairs since zfs_check_input_nvpairs() confirmed that
 	 * they exist and are of the correct type.
 	 */
-	if (error == 0 && vec->zvec_func != NULL) {
+	if (error == 0 && vec->zvec_func != NULL)
+	{
+		zfs_dbgmsg("zfsdev_ioctl_common(): checking input nvlist");
 		error = zfs_check_input_nvpairs(innvl, vec);
-		if (error != 0)
+		if (error != 0) {
+			zfs_dbgmsg("nvlist not good %d!", error);
 			goto out;
+		}
 	}
 
-	if (error == 0) {
+	zfs_dbgmsg("zfsdev_ioctl_common(): nvlist good");
+
+	if (error == 0)
+	{
 		cookie = spl_fstrans_mark();
 		error = vec->zvec_secpolicy(zc, innvl, CRED());
 		spl_fstrans_unmark(cookie);
@@ -7560,6 +8637,8 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 	if (error != 0)
 		goto out;
 
+	zfs_dbgmsg("zfsdev_ioctl_common(): input valid");
+
 	/* legacy ioctls can modify zc_name */
 	/*
 	 * Can't use kmem_strdup() as we might truncate the string and
@@ -7571,7 +8650,10 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 	strlcpy(saved_poolname, zc->zc_name, saved_poolname_len);
 	saved_poolname[strcspn(saved_poolname, "/@#")] = '\0';
 
-	if (vec->zvec_func != NULL) {
+	zfs_dbgmsg("pool name copied");
+
+	if (vec->zvec_func != NULL)
+	{
 		nvlist_t *outnvl;
 		int puterror = 0;
 		spa_t *spa;
@@ -7583,56 +8665,74 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 		 * Add the innvl to the lognv before calling the func,
 		 * in case the func changes the innvl.
 		 */
-		if (vec->zvec_allow_log) {
+		if (vec->zvec_allow_log)
+		{
 			lognv = fnvlist_alloc();
 			fnvlist_add_string(lognv, ZPOOL_HIST_IOCTL,
-			    vec->zvec_name);
-			if (!nvlist_empty(innvl)) {
+							   vec->zvec_name);
+			if (!nvlist_empty(innvl))
+			{
 				fnvlist_add_nvlist(lognv, ZPOOL_HIST_INPUT_NVL,
-				    innvl);
+								   innvl);
 			}
 		}
 
+		zfs_dbgmsg("allow log good");
+
 		outnvl = fnvlist_alloc();
+		zfs_dbgmsg("out nvl allocation good");
 		cookie = spl_fstrans_mark();
 		error = vec->zvec_func(zc->zc_name, innvl, outnvl);
 		spl_fstrans_unmark(cookie);
 
+		zfs_dbgmsg("spl cookie good, error now %d", error);
 		/*
 		 * Some commands can partially execute, modify state, and still
 		 * return an error.  In these cases, attempt to record what
 		 * was modified.
 		 */
 		if ((error == 0 ||
-		    (cmd == ZFS_IOC_CHANNEL_PROGRAM && error != EINVAL)) &&
-		    vec->zvec_allow_log &&
-		    spa_open(zc->zc_name, &spa, FTAG) == 0) {
-			if (!nvlist_empty(outnvl)) {
+			 (cmd == ZFS_IOC_CHANNEL_PROGRAM && error != EINVAL)) &&
+			vec->zvec_allow_log &&
+			spa_open(zc->zc_name, &spa, FTAG) == 0)
+		{
+			zfs_dbgmsg("spa_open good");
+			if (!nvlist_empty(outnvl))
+			{
 				size_t out_size = fnvlist_size(outnvl);
-				if (out_size > zfs_history_output_max) {
+				if (out_size > zfs_history_output_max)
+				{
 					fnvlist_add_int64(lognv,
-					    ZPOOL_HIST_OUTPUT_SIZE, out_size);
-				} else {
+									  ZPOOL_HIST_OUTPUT_SIZE, out_size);
+				}
+				else
+				{
 					fnvlist_add_nvlist(lognv,
-					    ZPOOL_HIST_OUTPUT_NVL, outnvl);
+									   ZPOOL_HIST_OUTPUT_NVL, outnvl);
 				}
 			}
-			if (error != 0) {
+			zfs_dbgmsg("log nv good");
+			if (error != 0)
+			{
 				fnvlist_add_int64(lognv, ZPOOL_HIST_ERRNO,
-				    error);
+								  error);
 			}
 			fnvlist_add_int64(lognv, ZPOOL_HIST_ELAPSED_NS,
-			    gethrtime() - start_time);
-			(void) spa_history_log_nvl(spa, lognv);
+							  gethrtime() - start_time);
+			(void)spa_history_log_nvl(spa, lognv);
+			zfs_dbgmsg("spa history log god");
 			spa_close(spa, FTAG);
+			zfs_dbgmsg("spa close good");
 		}
 		fnvlist_free(lognv);
 
-		if (!nvlist_empty(outnvl) || zc->zc_nvlist_dst_size != 0) {
+		if (!nvlist_empty(outnvl) || zc->zc_nvlist_dst_size != 0)
+		{
 			int smusherror = 0;
-			if (vec->zvec_smush_outnvlist) {
+			if (vec->zvec_smush_outnvlist)
+			{
 				smusherror = nvlist_smush(outnvl,
-				    zc->zc_nvlist_dst_size);
+										  zc->zc_nvlist_dst_size);
 			}
 			if (smusherror == 0)
 				puterror = put_nvlist(zc, outnvl);
@@ -7642,7 +8742,9 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 			error = puterror;
 
 		nvlist_free(outnvl);
-	} else {
+	}
+	else
+	{
 		cookie = spl_fstrans_mark();
 		error = vec->zvec_legacy_func(zc);
 		spl_fstrans_unmark(cookie);
@@ -7650,20 +8752,21 @@ zfsdev_ioctl_common(uint_t vecnum, zfs_cmd_t *zc, int flag)
 
 out:
 	nvlist_free(innvl);
-	if (error == 0 && vec->zvec_allow_log) {
+	if (error == 0 && vec->zvec_allow_log)
+	{
 		char *s = tsd_get(zfs_allow_log_key);
 		if (s != NULL)
 			kmem_strfree(s);
-		(void) tsd_set(zfs_allow_log_key, kmem_strdup(saved_poolname));
+		(void)tsd_set(zfs_allow_log_key, kmem_strdup(saved_poolname));
 	}
 	if (saved_poolname != NULL)
 		kmem_free(saved_poolname, saved_poolname_len);
 
+	zfs_dbgmsg("Returning good");
 	return (error);
 }
 
-int
-zfs_kmod_init(void)
+int zfs_kmod_init(void)
 {
 	int error;
 
@@ -7676,7 +8779,7 @@ zfs_kmod_init(void)
 	zfs_ioctl_init();
 
 	mutex_init(&zfsdev_state_lock, NULL, MUTEX_DEFAULT, NULL);
-	zfsdev_state_list = kmem_zalloc(sizeof (zfsdev_state_t), KM_SLEEP);
+	zfsdev_state_list = kmem_zalloc(sizeof(zfsdev_state_t), KM_SLEEP);
 	zfsdev_state_list->zs_minor = -1;
 
 	if ((error = zfsdev_attach()) != 0)
@@ -7695,8 +8798,7 @@ out:
 	return (error);
 }
 
-void
-zfs_kmod_fini(void)
+void zfs_kmod_fini(void)
 {
 	zfsdev_state_t *zs, *zsnext = NULL;
 
@@ -7704,16 +8806,17 @@ zfs_kmod_fini(void)
 
 	mutex_destroy(&zfsdev_state_lock);
 
-	for (zs = zfsdev_state_list; zs != NULL; zs = zsnext) {
+	for (zs = zfsdev_state_list; zs != NULL; zs = zsnext)
+	{
 		zsnext = zs->zs_next;
 		if (zs->zs_onexit)
 			zfs_onexit_destroy(zs->zs_onexit);
 		if (zs->zs_zevent)
 			zfs_zevent_destroy(zs->zs_zevent);
-		kmem_free(zs, sizeof (zfsdev_state_t));
+		kmem_free(zs, sizeof(zfsdev_state_t));
 	}
 
-	zfs_ereport_taskq_fini();	/* run before zfs_fini() on Linux */
+	zfs_ereport_taskq_fini(); /* run before zfs_fini() on Linux */
 	zfs_fini();
 	spa_fini();
 	zvol_fini();
@@ -7725,8 +8828,8 @@ zfs_kmod_fini(void)
 
 /* BEGIN CSTYLED */
 ZFS_MODULE_PARAM(zfs, zfs_, max_nvlist_src_size, ULONG, ZMOD_RW,
-    "Maximum size in bytes allowed for src nvlist passed with ZFS ioctls");
+				 "Maximum size in bytes allowed for src nvlist passed with ZFS ioctls");
 
 ZFS_MODULE_PARAM(zfs, zfs_, history_output_max, ULONG, ZMOD_RW,
-    "Maximum size in bytes of ZFS ioctl output that will be logged");
+				 "Maximum size in bytes of ZFS ioctl output that will be logged");
 /* END CSTYLED */
diff --git a/module/zfs/zio.c b/module/zfs/zio.c
index c367ef721..971cd0fb0 100644
--- a/module/zfs/zio.c
+++ b/module/zfs/zio.c
@@ -57,16 +57,16 @@
  * I/O type descriptions
  * ==========================================================================
  */
+// We aer initializing 8 elements
 const char *zio_type_name[ZIO_TYPES] = {
 	/*
 	 * Note: Linux kernel thread name length is limited
 	 * so these names will differ from upstream open zfs.
 	 */
-	"z_null", "z_rd", "z_wr", "z_fr", "z_cl", "z_ioctl", "z_trim"
-};
+	"z_null", "z_rd", "z_wr", "z_fr", "z_cl", "z_ioctl", "z_trim", "z_mlecw"};
 
 int zio_dva_throttle_enabled = B_TRUE;
-int zio_deadman_log_all = B_FALSE;
+int zio_deadman_log_all = B_TRUE;
 
 /*
  * ==========================================================================
@@ -85,9 +85,9 @@ uint64_t zio_buf_cache_frees[SPA_MAXBLOCKSIZE >> SPA_MINBLOCKSHIFT];
 /* Mark IOs as "slow" if they take longer than 30 seconds */
 int zio_slow_io_ms = (30 * MILLISEC);
 
-#define	BP_SPANB(indblkshift, level) \
+#define BP_SPANB(indblkshift, level) \
 	(((uint64_t)1) << ((level) * ((indblkshift) - SPA_BLKPTRSHIFT)))
-#define	COMPARE_META_LEVEL	0x80000000ul
+#define COMPARE_META_LEVEL 0x80000000ul
 /*
  * The following actions directly effect the spa's sync-to-convergence logic.
  * The values below define the sync pass when we start performing the action.
@@ -116,13 +116,13 @@ int zio_slow_io_ms = (30 * MILLISEC);
  */
 int zfs_sync_pass_deferred_free = 2; /* defer frees starting in this pass */
 int zfs_sync_pass_dont_compress = 8; /* don't compress starting in this pass */
-int zfs_sync_pass_rewrite = 2; /* rewrite new bps starting in this pass */
+int zfs_sync_pass_rewrite = 2;		 /* rewrite new bps starting in this pass */
 
 /*
  * An allocating zio is one that either currently has the DVA allocate
  * stage set or will have it later in its lifetime.
  */
-#define	IO_IS_ALLOCATING(zio) ((zio)->io_orig_pipeline & ZIO_STAGE_DVA_ALLOCATE)
+#define IO_IS_ALLOCATING(zio) ((zio)->io_orig_pipeline & ZIO_STAGE_DVA_ALLOCATE)
 
 /*
  * Enable smaller cores by excluding metadata
@@ -141,30 +141,29 @@ static inline void __zio_execute(zio_t *zio);
 
 static void zio_taskq_dispatch(zio_t *, zio_taskq_type_t, boolean_t);
 
-void
-zio_init(void)
+void zio_init(void)
 {
 	size_t c;
 
 	zio_cache = kmem_cache_create("zio_cache",
-	    sizeof (zio_t), 0, NULL, NULL, NULL, NULL, NULL, 0);
+								  sizeof(zio_t), 0, NULL, NULL, NULL, NULL, NULL, 0);
 	zio_link_cache = kmem_cache_create("zio_link_cache",
-	    sizeof (zio_link_t), 0, NULL, NULL, NULL, NULL, NULL, 0);
+									   sizeof(zio_link_t), 0, NULL, NULL, NULL, NULL, NULL, 0);
 
 	/*
 	 * For small buffers, we want a cache for each multiple of
 	 * SPA_MINBLOCKSIZE.  For larger buffers, we want a cache
 	 * for each quarter-power of 2.
 	 */
-	for (c = 0; c < SPA_MAXBLOCKSIZE >> SPA_MINBLOCKSHIFT; c++) {
+	for (c = 0; c < SPA_MAXBLOCKSIZE >> SPA_MINBLOCKSHIFT; c++)
+	{
 		size_t size = (c + 1) << SPA_MINBLOCKSHIFT;
 		size_t p2 = size;
 		size_t align = 0;
 		size_t data_cflags, cflags;
 
 		data_cflags = KMC_NODEBUG;
-		cflags = (zio_exclude_metadata || size > zio_buf_debug_limit) ?
-		    KMC_NODEBUG : 0;
+		cflags = (zio_exclude_metadata || size > zio_buf_debug_limit) ? KMC_NODEBUG : 0;
 
 #if defined(_ILP32) && defined(_KERNEL)
 		/*
@@ -196,41 +195,47 @@ zio_init(void)
 		 */
 		align = 8 * SPA_MINBLOCKSIZE;
 #else
-		if (size < PAGESIZE) {
+		if (size < PAGESIZE)
+		{
 			align = SPA_MINBLOCKSIZE;
-		} else if (IS_P2ALIGNED(size, p2 >> 2)) {
+		}
+		else if (IS_P2ALIGNED(size, p2 >> 2))
+		{
 			align = PAGESIZE;
 		}
 #endif
 
-		if (align != 0) {
+		if (align != 0)
+		{
 			char name[36];
-			if (cflags == data_cflags) {
+			if (cflags == data_cflags)
+			{
 				/*
 				 * Resulting kmem caches would be identical.
 				 * Save memory by creating only one.
 				 */
-				(void) snprintf(name, sizeof (name),
-				    "zio_buf_comb_%lu", (ulong_t)size);
+				(void)snprintf(name, sizeof(name),
+							   "zio_buf_comb_%lu", (ulong_t)size);
 				zio_buf_cache[c] = kmem_cache_create(name,
-				    size, align, NULL, NULL, NULL, NULL, NULL,
-				    cflags);
+													 size, align, NULL, NULL, NULL, NULL, NULL,
+													 cflags);
 				zio_data_buf_cache[c] = zio_buf_cache[c];
 				continue;
 			}
-			(void) snprintf(name, sizeof (name), "zio_buf_%lu",
-			    (ulong_t)size);
+			(void)snprintf(name, sizeof(name), "zio_buf_%lu",
+						   (ulong_t)size);
 			zio_buf_cache[c] = kmem_cache_create(name, size,
-			    align, NULL, NULL, NULL, NULL, NULL, cflags);
+												 align, NULL, NULL, NULL, NULL, NULL, cflags);
 
-			(void) snprintf(name, sizeof (name), "zio_data_buf_%lu",
-			    (ulong_t)size);
+			(void)snprintf(name, sizeof(name), "zio_data_buf_%lu",
+						   (ulong_t)size);
 			zio_data_buf_cache[c] = kmem_cache_create(name, size,
-			    align, NULL, NULL, NULL, NULL, NULL, data_cflags);
+													  align, NULL, NULL, NULL, NULL, NULL, data_cflags);
 		}
 	}
 
-	while (--c != 0) {
+	while (--c != 0)
+	{
 		ASSERT(zio_buf_cache[c] != NULL);
 		if (zio_buf_cache[c - 1] == NULL)
 			zio_buf_cache[c - 1] = zio_buf_cache[c];
@@ -245,18 +250,18 @@ zio_init(void)
 	lz4_init();
 }
 
-void
-zio_fini(void)
+void zio_fini(void)
 {
 	size_t n = SPA_MAXBLOCKSIZE >> SPA_MINBLOCKSHIFT;
 
 #if defined(ZFS_DEBUG) && !defined(_KERNEL)
-	for (size_t i = 0; i < n; i++) {
+	for (size_t i = 0; i < n; i++)
+	{
 		if (zio_buf_cache_allocs[i] != zio_buf_cache_frees[i])
-			(void) printf("zio_fini: [%d] %llu != %llu\n",
-			    (int)((i + 1) << SPA_MINBLOCKSHIFT),
-			    (long long unsigned)zio_buf_cache_allocs[i],
-			    (long long unsigned)zio_buf_cache_frees[i]);
+			(void)printf("zio_fini: [%d] %llu != %llu\n",
+						 (int)((i + 1) << SPA_MINBLOCKSHIFT),
+						 (long long unsigned)zio_buf_cache_allocs[i],
+						 (long long unsigned)zio_buf_cache_frees[i]);
 	}
 #endif
 
@@ -265,11 +270,13 @@ zio_fini(void)
 	 * and zio_data_buf_cache. Do a wasteful but trivially correct scan to
 	 * sort it out.
 	 */
-	for (size_t i = 0; i < n; i++) {
+	for (size_t i = 0; i < n; i++)
+	{
 		kmem_cache_t *cache = zio_buf_cache[i];
 		if (cache == NULL)
 			continue;
-		for (size_t j = i; j < n; j++) {
+		for (size_t j = i; j < n; j++)
+		{
 			if (cache == zio_buf_cache[j])
 				zio_buf_cache[j] = NULL;
 			if (cache == zio_data_buf_cache[j])
@@ -278,18 +285,21 @@ zio_fini(void)
 		kmem_cache_destroy(cache);
 	}
 
-	for (size_t i = 0; i < n; i++) {
+	for (size_t i = 0; i < n; i++)
+	{
 		kmem_cache_t *cache = zio_data_buf_cache[i];
 		if (cache == NULL)
 			continue;
-		for (size_t j = i; j < n; j++) {
+		for (size_t j = i; j < n; j++)
+		{
 			if (cache == zio_data_buf_cache[j])
 				zio_data_buf_cache[j] = NULL;
 		}
 		kmem_cache_destroy(cache);
 	}
 
-	for (size_t i = 0; i < n; i++) {
+	for (size_t i = 0; i < n; i++)
+	{
 		VERIFY3P(zio_buf_cache[i], ==, NULL);
 		VERIFY3P(zio_data_buf_cache[i], ==, NULL);
 	}
@@ -343,8 +353,7 @@ zio_data_buf_alloc(size_t size)
 	return (kmem_cache_alloc(zio_data_buf_cache[c], KM_PUSHPAGE));
 }
 
-void
-zio_buf_free(void *buf, size_t size)
+void zio_buf_free(void *buf, size_t size)
 {
 	size_t c = (size - 1) >> SPA_MINBLOCKSHIFT;
 
@@ -356,8 +365,7 @@ zio_buf_free(void *buf, size_t size)
 	kmem_cache_free(zio_buf_cache[c], buf);
 }
 
-void
-zio_data_buf_free(void *buf, size_t size)
+void zio_data_buf_free(void *buf, size_t size)
 {
 	size_t c = (size - 1) >> SPA_MINBLOCKSHIFT;
 
@@ -369,7 +377,7 @@ zio_data_buf_free(void *buf, size_t size)
 static void
 zio_abd_free(void *abd, size_t size)
 {
-	(void) size;
+	(void)size;
 	abd_free((abd_t *)abd);
 }
 
@@ -378,11 +386,10 @@ zio_abd_free(void *abd, size_t size)
  * Push and pop I/O transform buffers
  * ==========================================================================
  */
-void
-zio_push_transform(zio_t *zio, abd_t *data, uint64_t size, uint64_t bufsize,
-    zio_transform_func_t *transform)
+void zio_push_transform(zio_t *zio, abd_t *data, uint64_t size, uint64_t bufsize,
+						zio_transform_func_t *transform)
 {
-	zio_transform_t *zt = kmem_alloc(sizeof (zio_transform_t), KM_SLEEP);
+	zio_transform_t *zt = kmem_alloc(sizeof(zio_transform_t), KM_SLEEP);
 
 	zt->zt_orig_abd = zio->io_abd;
 	zt->zt_orig_size = zio->io_size;
@@ -396,15 +403,15 @@ zio_push_transform(zio_t *zio, abd_t *data, uint64_t size, uint64_t bufsize,
 	zio->io_size = size;
 }
 
-void
-zio_pop_transforms(zio_t *zio)
+void zio_pop_transforms(zio_t *zio)
 {
 	zio_transform_t *zt;
 
-	while ((zt = zio->io_transform_stack) != NULL) {
+	while ((zt = zio->io_transform_stack) != NULL)
+	{
 		if (zt->zt_transform != NULL)
 			zt->zt_transform(zio,
-			    zt->zt_orig_abd, zt->zt_orig_size);
+							 zt->zt_orig_abd, zt->zt_orig_size);
 
 		if (zt->zt_bufsize != 0)
 			abd_free(zio->io_abd);
@@ -413,7 +420,7 @@ zio_pop_transforms(zio_t *zio)
 		zio->io_size = zt->zt_orig_size;
 		zio->io_transform_stack = zt->zt_next;
 
-		kmem_free(zt, sizeof (zio_transform_t));
+		kmem_free(zt, sizeof(zio_transform_t));
 	}
 }
 
@@ -434,11 +441,12 @@ zio_subblock(zio_t *zio, abd_t *data, uint64_t size)
 static void
 zio_decompress(zio_t *zio, abd_t *data, uint64_t size)
 {
-	if (zio->io_error == 0) {
+	if (zio->io_error == 0)
+	{
 		void *tmp = abd_borrow_buf(data, size);
 		int ret = zio_decompress_data(BP_GET_COMPRESS(zio->io_bp),
-		    zio->io_abd, tmp, zio->io_size, size,
-		    &zio->io_prop.zp_complevel);
+									  zio->io_abd, tmp, zio->io_size, size,
+									  &zio->io_prop.zp_complevel);
 		abd_return_buf_copy(data, tmp, size);
 
 		if (zio_injection_enabled && ret == 0)
@@ -475,10 +483,12 @@ zio_decrypt(zio_t *zio, abd_t *data, uint64_t size)
 	 * be possible to verify this since it does not require an encryption
 	 * key.
 	 */
-	if (BP_HAS_INDIRECT_MAC_CKSUM(bp)) {
+	if (BP_HAS_INDIRECT_MAC_CKSUM(bp))
+	{
 		zio_crypt_decode_mac_bp(bp, mac);
 
-		if (BP_GET_COMPRESS(bp) != ZIO_COMPRESS_OFF) {
+		if (BP_GET_COMPRESS(bp) != ZIO_COMPRESS_OFF)
+		{
 			/*
 			 * We haven't decompressed the data yet, but
 			 * zio_crypt_do_indirect_mac_checksum() requires
@@ -488,24 +498,28 @@ zio_decrypt(zio_t *zio, abd_t *data, uint64_t size)
 			 */
 			tmp = zio_buf_alloc(lsize);
 			ret = zio_decompress_data(BP_GET_COMPRESS(bp),
-			    zio->io_abd, tmp, zio->io_size, lsize,
-			    &zio->io_prop.zp_complevel);
-			if (ret != 0) {
+									  zio->io_abd, tmp, zio->io_size, lsize,
+									  &zio->io_prop.zp_complevel);
+			if (ret != 0)
+			{
 				ret = SET_ERROR(EIO);
 				goto error;
 			}
 			ret = zio_crypt_do_indirect_mac_checksum(B_FALSE,
-			    tmp, lsize, BP_SHOULD_BYTESWAP(bp), mac);
+													 tmp, lsize, BP_SHOULD_BYTESWAP(bp), mac);
 			zio_buf_free(tmp, lsize);
-		} else {
+		}
+		else
+		{
 			ret = zio_crypt_do_indirect_mac_checksum_abd(B_FALSE,
-			    zio->io_abd, size, BP_SHOULD_BYTESWAP(bp), mac);
+														 zio->io_abd, size, BP_SHOULD_BYTESWAP(bp), mac);
 		}
 		abd_copy(data, zio->io_abd, size);
 
-		if (zio_injection_enabled && ot != DMU_OT_DNODE && ret == 0) {
+		if (zio_injection_enabled && ot != DMU_OT_DNODE && ret == 0)
+		{
 			ret = zio_handle_decrypt_injection(spa,
-			    &zio->io_bookmark, ot, ECKSUM);
+											   &zio->io_bookmark, ot, ECKSUM);
 		}
 		if (ret != 0)
 			goto error;
@@ -518,17 +532,22 @@ zio_decrypt(zio_t *zio, abd_t *data, uint64_t size)
 	 * nice to separate this out into its own flag, but for the moment
 	 * enum zio_flag is out of bits.
 	 */
-	if (BP_IS_AUTHENTICATED(bp)) {
-		if (ot == DMU_OT_OBJSET) {
+	if (BP_IS_AUTHENTICATED(bp))
+	{
+		if (ot == DMU_OT_OBJSET)
+		{
 			ret = spa_do_crypt_objset_mac_abd(B_FALSE, spa,
-			    dsobj, zio->io_abd, size, BP_SHOULD_BYTESWAP(bp));
-		} else {
+											  dsobj, zio->io_abd, size, BP_SHOULD_BYTESWAP(bp));
+		}
+		else
+		{
 			zio_crypt_decode_mac_bp(bp, mac);
 			ret = spa_do_crypt_mac_abd(B_FALSE, spa, dsobj,
-			    zio->io_abd, size, mac);
-			if (zio_injection_enabled && ret == 0) {
+									   zio->io_abd, size, mac);
+			if (zio_injection_enabled && ret == 0)
+			{
 				ret = zio_handle_decrypt_injection(spa,
-				    &zio->io_bookmark, ot, ECKSUM);
+												   &zio->io_bookmark, ot, ECKSUM);
 			}
 		}
 		abd_copy(data, zio->io_abd, size);
@@ -541,17 +560,20 @@ zio_decrypt(zio_t *zio, abd_t *data, uint64_t size)
 
 	zio_crypt_decode_params_bp(bp, salt, iv);
 
-	if (ot == DMU_OT_INTENT_LOG) {
-		tmp = abd_borrow_buf_copy(zio->io_abd, sizeof (zil_chain_t));
+	if (ot == DMU_OT_INTENT_LOG)
+	{
+		tmp = abd_borrow_buf_copy(zio->io_abd, sizeof(zil_chain_t));
 		zio_crypt_decode_mac_zil(tmp, mac);
-		abd_return_buf(zio->io_abd, tmp, sizeof (zil_chain_t));
-	} else {
+		abd_return_buf(zio->io_abd, tmp, sizeof(zil_chain_t));
+	}
+	else
+	{
 		zio_crypt_decode_mac_bp(bp, mac);
 	}
 
 	ret = spa_do_crypt_abd(B_FALSE, spa, &zio->io_bookmark, BP_GET_TYPE(bp),
-	    BP_GET_DEDUP(bp), BP_SHOULD_BYTESWAP(bp), salt, iv, mac, size, data,
-	    zio->io_abd, &no_crypt);
+						   BP_GET_DEDUP(bp), BP_SHOULD_BYTESWAP(bp), salt, iv, mac, size, data,
+						   zio->io_abd, &no_crypt);
 	if (no_crypt)
 		abd_copy(data, zio->io_abd, size);
 
@@ -568,14 +590,18 @@ error:
 	 * If there was a decryption / authentication error return EIO as
 	 * the io_error. If this was not a speculative zio, create an ereport.
 	 */
-	if (ret == ECKSUM) {
+	if (ret == ECKSUM)
+	{
 		zio->io_error = SET_ERROR(EIO);
-		if ((zio->io_flags & ZIO_FLAG_SPECULATIVE) == 0) {
+		if ((zio->io_flags & ZIO_FLAG_SPECULATIVE) == 0)
+		{
 			spa_log_error(spa, &zio->io_bookmark);
-			(void) zfs_ereport_post(FM_EREPORT_ZFS_AUTHENTICATION,
-			    spa, NULL, &zio->io_bookmark, zio, 0);
+			(void)zfs_ereport_post(FM_EREPORT_ZFS_AUTHENTICATION,
+								   spa, NULL, &zio->io_bookmark, zio, 0);
 		}
-	} else {
+	}
+	else
+	{
 		zio->io_error = ret;
 	}
 }
@@ -623,8 +649,7 @@ zio_unique_parent(zio_t *cio)
 	return (pio);
 }
 
-void
-zio_add_child(zio_t *pio, zio_t *cio)
+void zio_add_child(zio_t *pio, zio_t *cio)
 {
 	zio_link_t *zl = kmem_cache_alloc(zio_link_cache, KM_SLEEP);
 
@@ -684,12 +709,14 @@ zio_wait_for_children(zio_t *zio, uint8_t childbits, enum zio_wait_type wait)
 
 	mutex_enter(&zio->io_lock);
 	ASSERT(zio->io_stall == NULL);
-	for (int c = 0; c < ZIO_CHILD_TYPES; c++) {
+	for (int c = 0; c < ZIO_CHILD_TYPES; c++)
+	{
 		if (!(ZIO_CHILD_BIT_IS_SET(childbits, c)))
 			continue;
 
 		uint64_t *countp = &zio->io_children[c][wait];
-		if (*countp != 0) {
+		if (*countp != 0)
+		{
 			zio->io_stage >>= 1;
 			ASSERT3U(zio->io_stage, !=, ZIO_STAGE_OPEN);
 			zio->io_stall = countp;
@@ -701,10 +728,9 @@ zio_wait_for_children(zio_t *zio, uint8_t childbits, enum zio_wait_type wait)
 	return (waiting);
 }
 
-__attribute__((always_inline))
-static inline void
+__attribute__((always_inline)) static inline void
 zio_notify_parent(zio_t *pio, zio_t *zio, enum zio_wait_type wait,
-    zio_t **next_to_executep)
+				  zio_t **next_to_executep)
 {
 	uint64_t *countp = &pio->io_children[zio->io_child_type][wait];
 	int *errorp = &pio->io_child_error[zio->io_child_type];
@@ -717,10 +743,10 @@ zio_notify_parent(zio_t *pio, zio_t *zio, enum zio_wait_type wait,
 
 	(*countp)--;
 
-	if (*countp == 0 && pio->io_stall == countp) {
+	if (*countp == 0 && pio->io_stall == countp)
+	{
 		zio_taskq_type_t type =
-		    pio->io_stage < ZIO_STAGE_VDEV_IO_START ? ZIO_TASKQ_ISSUE :
-		    ZIO_TASKQ_INTERRUPT;
+			pio->io_stage < ZIO_STAGE_VDEV_IO_START ? ZIO_TASKQ_ISSUE : ZIO_TASKQ_INTERRUPT;
 		pio->io_stall = NULL;
 		mutex_exit(&pio->io_lock);
 
@@ -745,12 +771,17 @@ zio_notify_parent(zio_t *pio, zio_t *zio, enum zio_wait_type wait,
 		 * parent-child relationships, as we do with the "mega zio"
 		 * of writes for spa_sync(), and the chain of ZIL blocks.
 		 */
-		if (next_to_executep != NULL && *next_to_executep == NULL) {
+		if (next_to_executep != NULL && *next_to_executep == NULL)
+		{
 			*next_to_executep = pio;
-		} else {
+		}
+		else
+		{
 			zio_taskq_dispatch(pio, type, B_FALSE);
 		}
-	} else {
+	}
+	else
+	{
 		mutex_exit(&pio->io_lock);
 	}
 }
@@ -762,8 +793,7 @@ zio_inherit_child_errors(zio_t *zio, enum zio_child c)
 		zio->io_error = zio->io_child_error[c];
 }
 
-int
-zio_bookmark_compare(const void *x1, const void *x2)
+int zio_bookmark_compare(const void *x1, const void *x2)
 {
 	const zio_t *z1 = x1;
 	const zio_t *z2 = x2;
@@ -803,11 +833,11 @@ zio_bookmark_compare(const void *x1, const void *x2)
  */
 static zio_t *
 zio_create(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
-    abd_t *data, uint64_t lsize, uint64_t psize, zio_done_func_t *done,
-    void *private, zio_type_t type, zio_priority_t priority,
-    enum zio_flag flags, vdev_t *vd, uint64_t offset,
-    const zbookmark_phys_t *zb, enum zio_stage stage,
-    enum zio_stage pipeline)
+		   abd_t *data, uint64_t lsize, uint64_t psize, zio_done_func_t *done,
+		   void *private, zio_type_t type, zio_priority_t priority,
+		   enum zio_flag flags, vdev_t *vd, uint64_t offset,
+		   const zbookmark_phys_t *zb, enum zio_stage stage,
+		   enum zio_stage pipeline)
 {
 	zio_t *zio;
 
@@ -822,15 +852,15 @@ zio_create(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
 	IMPLY(lsize != psize, (flags & ZIO_FLAG_RAW_COMPRESS) != 0);
 
 	zio = kmem_cache_alloc(zio_cache, KM_SLEEP);
-	bzero(zio, sizeof (zio_t));
+	bzero(zio, sizeof(zio_t));
 
 	mutex_init(&zio->io_lock, NULL, MUTEX_NOLOCKDEP, NULL);
 	cv_init(&zio->io_cv, NULL, CV_DEFAULT, NULL);
 
-	list_create(&zio->io_parent_list, sizeof (zio_link_t),
-	    offsetof(zio_link_t, zl_parent_node));
-	list_create(&zio->io_child_list, sizeof (zio_link_t),
-	    offsetof(zio_link_t, zl_child_node));
+	list_create(&zio->io_parent_list, sizeof(zio_link_t),
+				offsetof(zio_link_t, zl_parent_node));
+	list_create(&zio->io_child_list, sizeof(zio_link_t),
+				offsetof(zio_link_t, zl_child_node));
 	metaslab_trace_init(&zio->io_alloc_list);
 
 	if (vd != NULL)
@@ -842,13 +872,14 @@ zio_create(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
 	else
 		zio->io_child_type = ZIO_CHILD_LOGICAL;
 
-	if (bp != NULL) {
+	if (bp != NULL)
+	{
 		zio->io_bp = (blkptr_t *)bp;
 		zio->io_bp_copy = *bp;
 		zio->io_bp_orig = *bp;
 		if (type != ZIO_TYPE_WRITE ||
-		    zio->io_child_type == ZIO_CHILD_DDT)
-			zio->io_bp = &zio->io_bp_copy;	/* so caller can free */
+			zio->io_child_type == ZIO_CHILD_DDT)
+			zio->io_bp = &zio->io_bp_copy; /* so caller can free */
 		if (zio->io_child_type == ZIO_CHILD_LOGICAL)
 			zio->io_logical = zio;
 		if (zio->io_child_type > ZIO_CHILD_GANG && BP_IS_GANG(bp))
@@ -877,7 +908,8 @@ zio_create(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
 	if (zb != NULL)
 		zio->io_bookmark = *zb;
 
-	if (pio != NULL) {
+	if (pio != NULL)
+	{
 		zio->io_metaslab_class = pio->io_metaslab_class;
 		if (zio->io_logical == NULL)
 			zio->io_logical = pio->io_logical;
@@ -904,13 +936,13 @@ zio_destroy(zio_t *zio)
 
 zio_t *
 zio_null(zio_t *pio, spa_t *spa, vdev_t *vd, zio_done_func_t *done,
-    void *private, enum zio_flag flags)
+		 void *private, enum zio_flag flags)
 {
 	zio_t *zio;
 
 	zio = zio_create(pio, spa, 0, NULL, NULL, 0, 0, done, private,
-	    ZIO_TYPE_NULL, ZIO_PRIORITY_NOW, flags, vd, 0, NULL,
-	    ZIO_STAGE_OPEN, ZIO_INTERLOCK_PIPELINE);
+					 ZIO_TYPE_NULL, ZIO_PRIORITY_NOW, flags, vd, 0, NULL,
+					 ZIO_STAGE_OPEN, ZIO_INTERLOCK_PIPELINE);
 
 	return (zio);
 }
@@ -923,16 +955,17 @@ zio_root(spa_t *spa, zio_done_func_t *done, void *private, enum zio_flag flags)
 
 static int
 zfs_blkptr_verify_log(spa_t *spa, const blkptr_t *bp,
-    enum blk_verify_flag blk_verify, const char *fmt, ...)
+					  enum blk_verify_flag blk_verify, const char *fmt, ...)
 {
 	va_list adx;
 	char buf[256];
 
 	va_start(adx, fmt);
-	(void) vsnprintf(buf, sizeof (buf), fmt, adx);
+	(void)vsnprintf(buf, sizeof(buf), fmt, adx);
 	va_end(adx);
 
-	switch (blk_verify) {
+	switch (blk_verify)
+	{
 	case BLK_VERIFY_HALT:
 		dprintf_bp(bp, "blkptr at %p dprintf_bp():", bp);
 		zfs_panic_recover("%s: %s", spa_name(spa), buf);
@@ -962,43 +995,50 @@ zfs_blkptr_verify_log(spa_t *spa, const blkptr_t *bp,
  */
 boolean_t
 zfs_blkptr_verify(spa_t *spa, const blkptr_t *bp, boolean_t config_held,
-    enum blk_verify_flag blk_verify)
+				  enum blk_verify_flag blk_verify)
 {
 	int errors = 0;
 
-	if (!DMU_OT_IS_VALID(BP_GET_TYPE(bp))) {
+	if (!DMU_OT_IS_VALID(BP_GET_TYPE(bp)))
+	{
 		errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-		    "blkptr at %p has invalid TYPE %llu",
-		    bp, (longlong_t)BP_GET_TYPE(bp));
+										"blkptr at %p has invalid TYPE %llu",
+										bp, (longlong_t)BP_GET_TYPE(bp));
 	}
 	if (BP_GET_CHECKSUM(bp) >= ZIO_CHECKSUM_FUNCTIONS ||
-	    BP_GET_CHECKSUM(bp) <= ZIO_CHECKSUM_ON) {
+		BP_GET_CHECKSUM(bp) <= ZIO_CHECKSUM_ON)
+	{
 		errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-		    "blkptr at %p has invalid CHECKSUM %llu",
-		    bp, (longlong_t)BP_GET_CHECKSUM(bp));
+										"blkptr at %p has invalid CHECKSUM %llu",
+										bp, (longlong_t)BP_GET_CHECKSUM(bp));
 	}
 	if (BP_GET_COMPRESS(bp) >= ZIO_COMPRESS_FUNCTIONS ||
-	    BP_GET_COMPRESS(bp) <= ZIO_COMPRESS_ON) {
+		BP_GET_COMPRESS(bp) <= ZIO_COMPRESS_ON)
+	{
 		errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-		    "blkptr at %p has invalid COMPRESS %llu",
-		    bp, (longlong_t)BP_GET_COMPRESS(bp));
+										"blkptr at %p has invalid COMPRESS %llu",
+										bp, (longlong_t)BP_GET_COMPRESS(bp));
 	}
-	if (BP_GET_LSIZE(bp) > SPA_MAXBLOCKSIZE) {
+	if (BP_GET_LSIZE(bp) > SPA_MAXBLOCKSIZE)
+	{
 		errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-		    "blkptr at %p has invalid LSIZE %llu",
-		    bp, (longlong_t)BP_GET_LSIZE(bp));
+										"blkptr at %p has invalid LSIZE %llu",
+										bp, (longlong_t)BP_GET_LSIZE(bp));
 	}
-	if (BP_GET_PSIZE(bp) > SPA_MAXBLOCKSIZE) {
+	if (BP_GET_PSIZE(bp) > SPA_MAXBLOCKSIZE)
+	{
 		errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-		    "blkptr at %p has invalid PSIZE %llu",
-		    bp, (longlong_t)BP_GET_PSIZE(bp));
+										"blkptr at %p has invalid PSIZE %llu",
+										bp, (longlong_t)BP_GET_PSIZE(bp));
 	}
 
-	if (BP_IS_EMBEDDED(bp)) {
-		if (BPE_GET_ETYPE(bp) >= NUM_BP_EMBEDDED_TYPES) {
+	if (BP_IS_EMBEDDED(bp))
+	{
+		if (BPE_GET_ETYPE(bp) >= NUM_BP_EMBEDDED_TYPES)
+		{
 			errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-			    "blkptr at %p has invalid ETYPE %llu",
-			    bp, (longlong_t)BPE_GET_ETYPE(bp));
+											"blkptr at %p has invalid ETYPE %llu",
+											bp, (longlong_t)BPE_GET_ETYPE(bp));
 		}
 	}
 
@@ -1021,30 +1061,35 @@ zfs_blkptr_verify(spa_t *spa, const blkptr_t *bp, boolean_t config_held,
 	 * allows the birth time of log blocks (and dmu_sync()-ed blocks
 	 * that are in the log) to be arbitrarily large.
 	 */
-	for (int i = 0; i < BP_GET_NDVAS(bp); i++) {
+	for (int i = 0; i < BP_GET_NDVAS(bp); i++)
+	{
 		const dva_t *dva = &bp->blk_dva[i];
 		uint64_t vdevid = DVA_GET_VDEV(dva);
 
-		if (vdevid >= spa->spa_root_vdev->vdev_children) {
+		if (vdevid >= spa->spa_root_vdev->vdev_children)
+		{
 			errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-			    "blkptr at %p DVA %u has invalid VDEV %llu",
-			    bp, i, (longlong_t)vdevid);
+											"blkptr at %p DVA %u has invalid VDEV %llu",
+											bp, i, (longlong_t)vdevid);
 			continue;
 		}
 		vdev_t *vd = spa->spa_root_vdev->vdev_child[vdevid];
-		if (vd == NULL) {
+		if (vd == NULL)
+		{
 			errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-			    "blkptr at %p DVA %u has invalid VDEV %llu",
-			    bp, i, (longlong_t)vdevid);
+											"blkptr at %p DVA %u has invalid VDEV %llu",
+											bp, i, (longlong_t)vdevid);
 			continue;
 		}
-		if (vd->vdev_ops == &vdev_hole_ops) {
+		if (vd->vdev_ops == &vdev_hole_ops)
+		{
 			errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-			    "blkptr at %p DVA %u has hole VDEV %llu",
-			    bp, i, (longlong_t)vdevid);
+											"blkptr at %p DVA %u has hole VDEV %llu",
+											bp, i, (longlong_t)vdevid);
 			continue;
 		}
-		if (vd->vdev_ops == &vdev_missing_ops) {
+		if (vd->vdev_ops == &vdev_missing_ops)
+		{
 			/*
 			 * "missing" vdevs are valid during import, but we
 			 * don't have their detailed info (e.g. asize), so
@@ -1056,10 +1101,11 @@ zfs_blkptr_verify(spa_t *spa, const blkptr_t *bp, boolean_t config_held,
 		uint64_t asize = DVA_GET_ASIZE(dva);
 		if (DVA_GET_GANG(dva))
 			asize = vdev_gang_header_asize(vd);
-		if (offset + asize > vd->vdev_asize) {
+		if (offset + asize > vd->vdev_asize)
+		{
 			errors += zfs_blkptr_verify_log(spa, bp, blk_verify,
-			    "blkptr at %p DVA %u has invalid OFFSET %llu",
-			    bp, i, (longlong_t)offset);
+											"blkptr at %p DVA %u has invalid OFFSET %llu",
+											bp, i, (longlong_t)offset);
 		}
 	}
 	if (errors > 0)
@@ -1073,7 +1119,7 @@ zfs_blkptr_verify(spa_t *spa, const blkptr_t *bp, boolean_t config_held,
 boolean_t
 zfs_dva_valid(spa_t *spa, const dva_t *dva, const blkptr_t *bp)
 {
-	(void) bp;
+	(void)bp;
 	uint64_t vdevid = DVA_GET_VDEV(dva);
 
 	if (vdevid >= spa->spa_root_vdev->vdev_children)
@@ -1086,7 +1132,8 @@ zfs_dva_valid(spa_t *spa, const dva_t *dva, const blkptr_t *bp)
 	if (vd->vdev_ops == &vdev_hole_ops)
 		return (B_FALSE);
 
-	if (vd->vdev_ops == &vdev_missing_ops) {
+	if (vd->vdev_ops == &vdev_missing_ops)
+	{
 		return (B_FALSE);
 	}
 
@@ -1103,43 +1150,41 @@ zfs_dva_valid(spa_t *spa, const dva_t *dva, const blkptr_t *bp)
 
 zio_t *
 zio_read(zio_t *pio, spa_t *spa, const blkptr_t *bp,
-    abd_t *data, uint64_t size, zio_done_func_t *done, void *private,
-    zio_priority_t priority, enum zio_flag flags, const zbookmark_phys_t *zb)
+		 abd_t *data, uint64_t size, zio_done_func_t *done, void *private,
+		 zio_priority_t priority, enum zio_flag flags, const zbookmark_phys_t *zb)
 {
 	zio_t *zio;
 
 	zio = zio_create(pio, spa, BP_PHYSICAL_BIRTH(bp), bp,
-	    data, size, size, done, private,
-	    ZIO_TYPE_READ, priority, flags, NULL, 0, zb,
-	    ZIO_STAGE_OPEN, (flags & ZIO_FLAG_DDT_CHILD) ?
-	    ZIO_DDT_CHILD_READ_PIPELINE : ZIO_READ_PIPELINE);
+					 data, size, size, done, private,
+					 ZIO_TYPE_READ, priority, flags, NULL, 0, zb,
+					 ZIO_STAGE_OPEN, (flags & ZIO_FLAG_DDT_CHILD) ? ZIO_DDT_CHILD_READ_PIPELINE : ZIO_READ_PIPELINE);
 
 	return (zio);
 }
 
 zio_t *
 zio_write(zio_t *pio, spa_t *spa, uint64_t txg, blkptr_t *bp,
-    abd_t *data, uint64_t lsize, uint64_t psize, const zio_prop_t *zp,
-    zio_done_func_t *ready, zio_done_func_t *children_ready,
-    zio_done_func_t *physdone, zio_done_func_t *done,
-    void *private, zio_priority_t priority, enum zio_flag flags,
-    const zbookmark_phys_t *zb)
+		  abd_t *data, uint64_t lsize, uint64_t psize, const zio_prop_t *zp,
+		  zio_done_func_t *ready, zio_done_func_t *children_ready,
+		  zio_done_func_t *physdone, zio_done_func_t *done,
+		  void *private, zio_priority_t priority, enum zio_flag flags,
+		  const zbookmark_phys_t *zb)
 {
 	zio_t *zio;
 
 	ASSERT(zp->zp_checksum >= ZIO_CHECKSUM_OFF &&
-	    zp->zp_checksum < ZIO_CHECKSUM_FUNCTIONS &&
-	    zp->zp_compress >= ZIO_COMPRESS_OFF &&
-	    zp->zp_compress < ZIO_COMPRESS_FUNCTIONS &&
-	    DMU_OT_IS_VALID(zp->zp_type) &&
-	    zp->zp_level < 32 &&
-	    zp->zp_copies > 0 &&
-	    zp->zp_copies <= spa_max_replication(spa));
+		   zp->zp_checksum < ZIO_CHECKSUM_FUNCTIONS &&
+		   zp->zp_compress >= ZIO_COMPRESS_OFF &&
+		   zp->zp_compress < ZIO_COMPRESS_FUNCTIONS &&
+		   DMU_OT_IS_VALID(zp->zp_type) &&
+		   zp->zp_level < 32 &&
+		   zp->zp_copies > 0 &&
+		   zp->zp_copies <= spa_max_replication(spa));
 
 	zio = zio_create(pio, spa, txg, bp, data, lsize, psize, done, private,
-	    ZIO_TYPE_WRITE, priority, flags, NULL, 0, zb,
-	    ZIO_STAGE_OPEN, (flags & ZIO_FLAG_DDT_CHILD) ?
-	    ZIO_DDT_CHILD_WRITE_PIPELINE : ZIO_WRITE_PIPELINE);
+					 ZIO_TYPE_WRITE, priority, flags, NULL, 0, zb,
+					 ZIO_STAGE_OPEN, (flags & ZIO_FLAG_DDT_CHILD) ? ZIO_DDT_CHILD_WRITE_PIPELINE : ZIO_WRITE_PIPELINE);
 
 	zio->io_ready = ready;
 	zio->io_children_ready = children_ready;
@@ -1155,7 +1200,8 @@ zio_write(zio_t *pio, spa_t *spa, uint64_t txg, blkptr_t *bp,
 	 * case.
 	 */
 	if (data == NULL &&
-	    (zio->io_prop.zp_dedup_verify || zio->io_prop.zp_encrypt)) {
+		(zio->io_prop.zp_dedup_verify || zio->io_prop.zp_encrypt))
+	{
 		zio->io_prop.zp_dedup = zio->io_prop.zp_dedup_verify = B_FALSE;
 	}
 
@@ -1164,20 +1210,19 @@ zio_write(zio_t *pio, spa_t *spa, uint64_t txg, blkptr_t *bp,
 
 zio_t *
 zio_rewrite(zio_t *pio, spa_t *spa, uint64_t txg, blkptr_t *bp, abd_t *data,
-    uint64_t size, zio_done_func_t *done, void *private,
-    zio_priority_t priority, enum zio_flag flags, zbookmark_phys_t *zb)
+			uint64_t size, zio_done_func_t *done, void *private,
+			zio_priority_t priority, enum zio_flag flags, zbookmark_phys_t *zb)
 {
 	zio_t *zio;
 
 	zio = zio_create(pio, spa, txg, bp, data, size, size, done, private,
-	    ZIO_TYPE_WRITE, priority, flags | ZIO_FLAG_IO_REWRITE, NULL, 0, zb,
-	    ZIO_STAGE_OPEN, ZIO_REWRITE_PIPELINE);
+					 ZIO_TYPE_WRITE, priority, flags | ZIO_FLAG_IO_REWRITE, NULL, 0, zb,
+					 ZIO_STAGE_OPEN, ZIO_REWRITE_PIPELINE);
 
 	return (zio);
 }
 
-void
-zio_write_override(zio_t *zio, blkptr_t *bp, int copies, boolean_t nopwrite)
+void zio_write_override(zio_t *zio, blkptr_t *bp, int copies, boolean_t nopwrite)
 {
 	ASSERT(zio->io_type == ZIO_TYPE_WRITE);
 	ASSERT(zio->io_child_type == ZIO_CHILD_LOGICAL);
@@ -1195,11 +1240,10 @@ zio_write_override(zio_t *zio, blkptr_t *bp, int copies, boolean_t nopwrite)
 	zio->io_bp_override = bp;
 }
 
-void
-zio_free(spa_t *spa, uint64_t txg, const blkptr_t *bp)
+void zio_free(spa_t *spa, uint64_t txg, const blkptr_t *bp)
 {
 
-	(void) zfs_blkptr_verify(spa, bp, B_FALSE, BLK_VERIFY_HALT);
+	(void)zfs_blkptr_verify(spa, bp, B_FALSE, BLK_VERIFY_HALT);
 
 	/*
 	 * The check for EMBEDDED is a performance optimization.  We
@@ -1221,12 +1265,15 @@ zio_free(spa_t *spa, uint64_t txg, const blkptr_t *bp)
 	 * in spa_sync_iterate_to_convergence()]
 	 */
 	if (BP_IS_GANG(bp) ||
-	    BP_GET_DEDUP(bp) ||
-	    txg != spa->spa_syncing_txg ||
-	    (spa_sync_pass(spa) >= zfs_sync_pass_deferred_free &&
-	    !spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP))) {
+		BP_GET_DEDUP(bp) ||
+		txg != spa->spa_syncing_txg ||
+		(spa_sync_pass(spa) >= zfs_sync_pass_deferred_free &&
+		 !spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP)))
+	{
 		bplist_append(&spa->spa_free_bplist[txg & TXG_MASK], bp);
-	} else {
+	}
+	else
+	{
 		VERIFY3P(zio_free_sync(NULL, spa, txg, bp, 0), ==, NULL);
 	}
 }
@@ -1238,7 +1285,7 @@ zio_free(spa_t *spa, uint64_t txg, const blkptr_t *bp)
  */
 zio_t *
 zio_free_sync(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
-    enum zio_flag flags)
+			  enum zio_flag flags)
 {
 	ASSERT(!BP_IS_HOLE(bp));
 	ASSERT(spa_syncing_txg(spa) == txg);
@@ -1250,20 +1297,23 @@ zio_free_sync(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
 	arc_freed(spa, bp);
 	dsl_scan_freed(spa, bp);
 
-	if (BP_IS_GANG(bp) || BP_GET_DEDUP(bp)) {
+	if (BP_IS_GANG(bp) || BP_GET_DEDUP(bp))
+	{
 		/*
 		 * GANG and DEDUP blocks can induce a read (for the gang block
 		 * header, or the DDT), so issue them asynchronously so that
 		 * this thread is not tied up.
 		 */
 		enum zio_stage stage =
-		    ZIO_FREE_PIPELINE | ZIO_STAGE_ISSUE_ASYNC;
+			ZIO_FREE_PIPELINE | ZIO_STAGE_ISSUE_ASYNC;
 
 		return (zio_create(pio, spa, txg, bp, NULL, BP_GET_PSIZE(bp),
-		    BP_GET_PSIZE(bp), NULL, NULL,
-		    ZIO_TYPE_FREE, ZIO_PRIORITY_NOW,
-		    flags, NULL, 0, NULL, ZIO_STAGE_OPEN, stage));
-	} else {
+						   BP_GET_PSIZE(bp), NULL, NULL,
+						   ZIO_TYPE_FREE, ZIO_PRIORITY_NOW,
+						   flags, NULL, 0, NULL, ZIO_STAGE_OPEN, stage));
+	}
+	else
+	{
 		metaslab_free(spa, bp, txg, B_FALSE);
 		return (NULL);
 	}
@@ -1271,12 +1321,12 @@ zio_free_sync(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
 
 zio_t *
 zio_claim(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
-    zio_done_func_t *done, void *private, enum zio_flag flags)
+		  zio_done_func_t *done, void *private, enum zio_flag flags)
 {
 	zio_t *zio;
 
-	(void) zfs_blkptr_verify(spa, bp, flags & ZIO_FLAG_CONFIG_WRITER,
-	    BLK_VERIFY_HALT);
+	(void)zfs_blkptr_verify(spa, bp, flags & ZIO_FLAG_CONFIG_WRITER,
+							BLK_VERIFY_HALT);
 
 	if (BP_IS_EMBEDDED(bp))
 		return (zio_null(pio, spa, NULL, NULL, NULL, 0));
@@ -1294,13 +1344,13 @@ zio_claim(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
 	 * If txg == 0 we just verify that the block is claimable.
 	 */
 	ASSERT3U(spa->spa_uberblock.ub_rootbp.blk_birth, <,
-	    spa_min_claim_txg(spa));
+			 spa_min_claim_txg(spa));
 	ASSERT(txg == spa_min_claim_txg(spa) || txg == 0);
-	ASSERT(!BP_GET_DEDUP(bp) || !spa_writeable(spa));	/* zdb(8) */
+	ASSERT(!BP_GET_DEDUP(bp) || !spa_writeable(spa)); /* zdb(8) */
 
 	zio = zio_create(pio, spa, txg, bp, NULL, BP_GET_PSIZE(bp),
-	    BP_GET_PSIZE(bp), done, private, ZIO_TYPE_CLAIM, ZIO_PRIORITY_NOW,
-	    flags, NULL, 0, NULL, ZIO_STAGE_OPEN, ZIO_CLAIM_PIPELINE);
+					 BP_GET_PSIZE(bp), done, private, ZIO_TYPE_CLAIM, ZIO_PRIORITY_NOW,
+					 flags, NULL, 0, NULL, ZIO_STAGE_OPEN, ZIO_CLAIM_PIPELINE);
 	ASSERT0(zio->io_queued_timestamp);
 
 	return (zio);
@@ -1308,23 +1358,32 @@ zio_claim(zio_t *pio, spa_t *spa, uint64_t txg, const blkptr_t *bp,
 
 zio_t *
 zio_ioctl(zio_t *pio, spa_t *spa, vdev_t *vd, int cmd,
-    zio_done_func_t *done, void *private, enum zio_flag flags)
+		  zio_done_func_t *done, void *private, enum zio_flag flags)
 {
 	zio_t *zio;
 	int c;
 
-	if (vd->vdev_children == 0) {
+	if (pio != NULL && pio->io_type == ZIO_TYPE_MLEC_WRITE_DATA) {
+		zio = zio_create(pio, spa, 0, NULL, NULL, 0, 0, done, private,
+						 ZIO_TYPE_MLEC_WRITE_DATA, ZIO_PRIORITY_REBUILD, flags, vd, 0, NULL,
+						 ZIO_STAGE_OPEN, ZIO_IOCTL_PIPELINE);
+
+		zio->io_cmd = cmd;
+	} else if (vd->vdev_children == 0)
+	{
 		zio = zio_create(pio, spa, 0, NULL, NULL, 0, 0, done, private,
-		    ZIO_TYPE_IOCTL, ZIO_PRIORITY_NOW, flags, vd, 0, NULL,
-		    ZIO_STAGE_OPEN, ZIO_IOCTL_PIPELINE);
+						 ZIO_TYPE_IOCTL, ZIO_PRIORITY_NOW, flags, vd, 0, NULL,
+						 ZIO_STAGE_OPEN, ZIO_IOCTL_PIPELINE);
 
 		zio->io_cmd = cmd;
-	} else {
+	}
+	else
+	{
 		zio = zio_null(pio, spa, NULL, NULL, NULL, flags);
 
 		for (c = 0; c < vd->vdev_children; c++)
 			zio_nowait(zio_ioctl(zio, spa, vd->vdev_child[c], cmd,
-			    done, private, flags));
+								 done, private, flags));
 	}
 
 	return (zio);
@@ -1332,8 +1391,8 @@ zio_ioctl(zio_t *pio, spa_t *spa, vdev_t *vd, int cmd,
 
 zio_t *
 zio_trim(zio_t *pio, vdev_t *vd, uint64_t offset, uint64_t size,
-    zio_done_func_t *done, void *private, zio_priority_t priority,
-    enum zio_flag flags, enum trim_flag trim_flags)
+		 zio_done_func_t *done, void *private, zio_priority_t priority,
+		 enum zio_flag flags, enum trim_flag trim_flags)
 {
 	zio_t *zio;
 
@@ -1343,8 +1402,8 @@ zio_trim(zio_t *pio, vdev_t *vd, uint64_t offset, uint64_t size,
 	ASSERT3U(size, !=, 0);
 
 	zio = zio_create(pio, vd->vdev_spa, 0, NULL, NULL, size, size, done,
-	    private, ZIO_TYPE_TRIM, priority, flags | ZIO_FLAG_PHYSICAL,
-	    vd, offset, NULL, ZIO_STAGE_OPEN, ZIO_TRIM_PIPELINE);
+					 private, ZIO_TYPE_TRIM, priority, flags | ZIO_FLAG_PHYSICAL,
+					 vd, offset, NULL, ZIO_STAGE_OPEN, ZIO_TRIM_PIPELINE);
 	zio->io_trim_flags = trim_flags;
 
 	return (zio);
@@ -1352,19 +1411,19 @@ zio_trim(zio_t *pio, vdev_t *vd, uint64_t offset, uint64_t size,
 
 zio_t *
 zio_read_phys(zio_t *pio, vdev_t *vd, uint64_t offset, uint64_t size,
-    abd_t *data, int checksum, zio_done_func_t *done, void *private,
-    zio_priority_t priority, enum zio_flag flags, boolean_t labels)
+			  abd_t *data, int checksum, zio_done_func_t *done, void *private,
+			  zio_priority_t priority, enum zio_flag flags, boolean_t labels)
 {
 	zio_t *zio;
 
 	ASSERT(vd->vdev_children == 0);
 	ASSERT(!labels || offset + size <= VDEV_LABEL_START_SIZE ||
-	    offset >= vd->vdev_psize - VDEV_LABEL_END_SIZE);
+		   offset >= vd->vdev_psize - VDEV_LABEL_END_SIZE);
 	ASSERT3U(offset + size, <=, vd->vdev_psize);
 
 	zio = zio_create(pio, vd->vdev_spa, 0, NULL, data, size, size, done,
-	    private, ZIO_TYPE_READ, priority, flags | ZIO_FLAG_PHYSICAL, vd,
-	    offset, NULL, ZIO_STAGE_OPEN, ZIO_READ_PHYS_PIPELINE);
+					 private, ZIO_TYPE_READ, priority, flags | ZIO_FLAG_PHYSICAL, vd,
+					 offset, NULL, ZIO_STAGE_OPEN, ZIO_READ_PHYS_PIPELINE);
 
 	zio->io_prop.zp_checksum = checksum;
 
@@ -1373,23 +1432,24 @@ zio_read_phys(zio_t *pio, vdev_t *vd, uint64_t offset, uint64_t size,
 
 zio_t *
 zio_write_phys(zio_t *pio, vdev_t *vd, uint64_t offset, uint64_t size,
-    abd_t *data, int checksum, zio_done_func_t *done, void *private,
-    zio_priority_t priority, enum zio_flag flags, boolean_t labels)
+			   abd_t *data, int checksum, zio_done_func_t *done, void *private,
+			   zio_priority_t priority, enum zio_flag flags, boolean_t labels)
 {
 	zio_t *zio;
 
 	ASSERT(vd->vdev_children == 0);
 	ASSERT(!labels || offset + size <= VDEV_LABEL_START_SIZE ||
-	    offset >= vd->vdev_psize - VDEV_LABEL_END_SIZE);
+		   offset >= vd->vdev_psize - VDEV_LABEL_END_SIZE);
 	ASSERT3U(offset + size, <=, vd->vdev_psize);
 
 	zio = zio_create(pio, vd->vdev_spa, 0, NULL, data, size, size, done,
-	    private, ZIO_TYPE_WRITE, priority, flags | ZIO_FLAG_PHYSICAL, vd,
-	    offset, NULL, ZIO_STAGE_OPEN, ZIO_WRITE_PHYS_PIPELINE);
+					 private, ZIO_TYPE_WRITE, priority, flags | ZIO_FLAG_PHYSICAL, vd,
+					 offset, NULL, ZIO_STAGE_OPEN, ZIO_WRITE_PHYS_PIPELINE);
 
 	zio->io_prop.zp_checksum = checksum;
 
-	if (zio_checksum_table[checksum].ci_flags & ZCHECKSUM_FLAG_EMBEDDED) {
+	if (zio_checksum_table[checksum].ci_flags & ZCHECKSUM_FLAG_EMBEDDED)
+	{
 		/*
 		 * zec checksums are necessarily destructive -- they modify
 		 * the end of the write buffer to hold the verifier/checksum.
@@ -1410,8 +1470,8 @@ zio_write_phys(zio_t *pio, vdev_t *vd, uint64_t offset, uint64_t size,
  */
 zio_t *
 zio_vdev_child_io(zio_t *pio, blkptr_t *bp, vdev_t *vd, uint64_t offset,
-    abd_t *data, uint64_t size, int type, zio_priority_t priority,
-    enum zio_flag flags, zio_done_func_t *done, void *private)
+				  abd_t *data, uint64_t size, int type, zio_priority_t priority,
+				  enum zio_flag flags, zio_done_func_t *done, void *private)
 {
 	enum zio_stage pipeline = ZIO_VDEV_CHILD_PIPELINE;
 	zio_t *zio;
@@ -1423,10 +1483,12 @@ zio_vdev_child_io(zio_t *pio, blkptr_t *bp, vdev_t *vd, uint64_t offset,
 	 * The only exceptions are i/os that we don't care about
 	 * (OPTIONAL or REPAIR).
 	 */
+	zfs_dbgmsg("zio flag is %d", flags);
 	ASSERT((flags & ZIO_FLAG_OPTIONAL) || (flags & ZIO_FLAG_IO_REPAIR) ||
-	    done != NULL);
+		   done != NULL);
 
-	if (type == ZIO_TYPE_READ && bp != NULL) {
+	if (type == ZIO_TYPE_READ && bp != NULL)
+	{
 		/*
 		 * If we have the bp, then the child should perform the
 		 * checksum and the parent need not.  This pushes error
@@ -1437,7 +1499,8 @@ zio_vdev_child_io(zio_t *pio, blkptr_t *bp, vdev_t *vd, uint64_t offset,
 		pio->io_pipeline &= ~ZIO_STAGE_CHECKSUM_VERIFY;
 	}
 
-	if (vd->vdev_ops->vdev_op_leaf) {
+	if (vd->vdev_ops->vdev_op_leaf)
+	{
 		ASSERT0(vd->vdev_children);
 		offset += VDEV_LABEL_START_SIZE;
 	}
@@ -1458,22 +1521,23 @@ zio_vdev_child_io(zio_t *pio, blkptr_t *bp, vdev_t *vd, uint64_t offset,
 	 * have already processed the original allocating I/O.
 	 */
 	if (flags & ZIO_FLAG_IO_ALLOCATING &&
-	    (vd != vd->vdev_top || (flags & ZIO_FLAG_IO_RETRY))) {
+		(vd != vd->vdev_top || (flags & ZIO_FLAG_IO_RETRY)))
+	{
 		ASSERT(pio->io_metaslab_class != NULL);
 		ASSERT(pio->io_metaslab_class->mc_alloc_throttle_enabled);
 		ASSERT(type == ZIO_TYPE_WRITE);
 		ASSERT(priority == ZIO_PRIORITY_ASYNC_WRITE);
 		ASSERT(!(flags & ZIO_FLAG_IO_REPAIR));
 		ASSERT(!(pio->io_flags & ZIO_FLAG_IO_REWRITE) ||
-		    pio->io_child_type == ZIO_CHILD_GANG);
+			   pio->io_child_type == ZIO_CHILD_GANG);
 
 		flags &= ~ZIO_FLAG_IO_ALLOCATING;
 	}
 
-
+	zfs_dbgmsg("Creating child zio of type %d", type);
 	zio = zio_create(pio, pio->io_spa, pio->io_txg, bp, data, size, size,
-	    done, private, type, priority, flags, vd, offset, &pio->io_bookmark,
-	    ZIO_STAGE_VDEV_IO_START >> 1, pipeline);
+					 done, private, type, priority, flags, vd, offset, &pio->io_bookmark,
+					 ZIO_STAGE_VDEV_IO_START >> 1, pipeline);
 	ASSERT3U(zio->io_child_type, ==, ZIO_CHILD_VDEV);
 
 	zio->io_physdone = pio->io_physdone;
@@ -1485,32 +1549,30 @@ zio_vdev_child_io(zio_t *pio, blkptr_t *bp, vdev_t *vd, uint64_t offset,
 
 zio_t *
 zio_vdev_delegated_io(vdev_t *vd, uint64_t offset, abd_t *data, uint64_t size,
-    zio_type_t type, zio_priority_t priority, enum zio_flag flags,
-    zio_done_func_t *done, void *private)
+					  zio_type_t type, zio_priority_t priority, enum zio_flag flags,
+					  zio_done_func_t *done, void *private)
 {
 	zio_t *zio;
 
 	ASSERT(vd->vdev_ops->vdev_op_leaf);
 
 	zio = zio_create(NULL, vd->vdev_spa, 0, NULL,
-	    data, size, size, done, private, type, priority,
-	    flags | ZIO_FLAG_CANFAIL | ZIO_FLAG_DONT_RETRY | ZIO_FLAG_DELEGATED,
-	    vd, offset, NULL,
-	    ZIO_STAGE_VDEV_IO_START >> 1, ZIO_VDEV_CHILD_PIPELINE);
+					 data, size, size, done, private, type, priority,
+					 flags | ZIO_FLAG_CANFAIL | ZIO_FLAG_DONT_RETRY | ZIO_FLAG_DELEGATED,
+					 vd, offset, NULL,
+					 ZIO_STAGE_VDEV_IO_START >> 1, ZIO_VDEV_CHILD_PIPELINE);
 
 	return (zio);
 }
 
-void
-zio_flush(zio_t *zio, vdev_t *vd)
+void zio_flush(zio_t *zio, vdev_t *vd)
 {
 	zio_nowait(zio_ioctl(zio, zio->io_spa, vd, DKIOCFLUSHWRITECACHE,
-	    NULL, NULL,
-	    ZIO_FLAG_CANFAIL | ZIO_FLAG_DONT_PROPAGATE | ZIO_FLAG_DONT_RETRY));
+						 NULL, NULL,
+						 ZIO_FLAG_CANFAIL | ZIO_FLAG_DONT_PROPAGATE | ZIO_FLAG_DONT_RETRY));
 }
 
-void
-zio_shrink(zio_t *zio, uint64_t size)
+void zio_shrink(zio_t *zio, uint64_t size)
 {
 	ASSERT3P(zio->io_executor, ==, NULL);
 	ASSERT3U(zio->io_orig_size, ==, zio->io_size);
@@ -1522,7 +1584,8 @@ zio_shrink(zio_t *zio, uint64_t size)
 	 * Note, BP_IS_RAIDZ() assumes no compression.
 	 */
 	ASSERT(BP_GET_COMPRESS(zio->io_bp) == ZIO_COMPRESS_OFF);
-	if (!BP_IS_RAIDZ(zio->io_bp)) {
+	if (!BP_IS_RAIDZ(zio->io_bp))
+	{
 		/* we are not doing a raw write */
 		ASSERT3U(zio->io_size, ==, zio->io_lsize);
 		zio->io_orig_size = zio->io_size = zio->io_lsize = size;
@@ -1540,32 +1603,37 @@ zio_read_bp_init(zio_t *zio)
 {
 	blkptr_t *bp = zio->io_bp;
 	uint64_t psize =
-	    BP_IS_EMBEDDED(bp) ? BPE_GET_PSIZE(bp) : BP_GET_PSIZE(bp);
+		BP_IS_EMBEDDED(bp) ? BPE_GET_PSIZE(bp) : BP_GET_PSIZE(bp);
 
 	ASSERT3P(zio->io_bp, ==, &zio->io_bp_copy);
 
 	if (BP_GET_COMPRESS(bp) != ZIO_COMPRESS_OFF &&
-	    zio->io_child_type == ZIO_CHILD_LOGICAL &&
-	    !(zio->io_flags & ZIO_FLAG_RAW_COMPRESS)) {
+		zio->io_child_type == ZIO_CHILD_LOGICAL &&
+		!(zio->io_flags & ZIO_FLAG_RAW_COMPRESS))
+	{
 		zio_push_transform(zio, abd_alloc_sametype(zio->io_abd, psize),
-		    psize, psize, zio_decompress);
+						   psize, psize, zio_decompress);
 	}
 
 	if (((BP_IS_PROTECTED(bp) && !(zio->io_flags & ZIO_FLAG_RAW_ENCRYPT)) ||
-	    BP_HAS_INDIRECT_MAC_CKSUM(bp)) &&
-	    zio->io_child_type == ZIO_CHILD_LOGICAL) {
+		 BP_HAS_INDIRECT_MAC_CKSUM(bp)) &&
+		zio->io_child_type == ZIO_CHILD_LOGICAL)
+	{
 		zio_push_transform(zio, abd_alloc_sametype(zio->io_abd, psize),
-		    psize, psize, zio_decrypt);
+						   psize, psize, zio_decrypt);
 	}
 
-	if (BP_IS_EMBEDDED(bp) && BPE_GET_ETYPE(bp) == BP_EMBEDDED_TYPE_DATA) {
+	if (BP_IS_EMBEDDED(bp) && BPE_GET_ETYPE(bp) == BP_EMBEDDED_TYPE_DATA)
+	{
 		int psize = BPE_GET_PSIZE(bp);
 		void *data = abd_borrow_buf(zio->io_abd, psize);
 
 		zio->io_pipeline = ZIO_INTERLOCK_PIPELINE;
 		decode_embedded_bp_compressed(bp, data);
 		abd_return_buf_copy(zio->io_abd, data, psize);
-	} else {
+	}
+	else
+	{
 		ASSERT(!BP_IS_EMBEDDED(bp));
 		ASSERT3P(zio->io_bp, ==, &zio->io_bp_copy);
 	}
@@ -1590,7 +1658,8 @@ zio_write_bp_init(zio_t *zio)
 
 	ASSERT(zio->io_child_type != ZIO_CHILD_DDT);
 
-	if (zio->io_bp_override) {
+	if (zio->io_bp_override)
+	{
 		blkptr_t *bp = zio->io_bp;
 		zio_prop_t *zp = &zio->io_prop;
 
@@ -1608,7 +1677,8 @@ zio_write_bp_init(zio_t *zio)
 		 * set the flag accordingly to indicate that a nopwrite
 		 * has already occurred.
 		 */
-		if (!BP_IS_HOLE(bp) && zp->zp_nopwrite) {
+		if (!BP_IS_HOLE(bp) && zp->zp_nopwrite)
+		{
 			ASSERT(!zp->zp_dedup);
 			ASSERT3U(BP_GET_CHECKSUM(bp), ==, zp->zp_checksum);
 			zio->io_flags |= ZIO_FLAG_NOPWRITE;
@@ -1621,10 +1691,12 @@ zio_write_bp_init(zio_t *zio)
 			return (zio);
 
 		ASSERT((zio_checksum_table[zp->zp_checksum].ci_flags &
-		    ZCHECKSUM_FLAG_DEDUP) || zp->zp_dedup_verify);
+				ZCHECKSUM_FLAG_DEDUP) ||
+			   zp->zp_dedup_verify);
 
 		if (BP_GET_CHECKSUM(bp) == zp->zp_checksum &&
-		    !zp->zp_encrypt) {
+			!zp->zp_encrypt)
+		{
 			BP_SET_DEDUP(bp, 1);
 			zio->io_pipeline |= ZIO_STAGE_DDT_WRITE;
 			return (zio);
@@ -1657,15 +1729,16 @@ zio_write_compress(zio_t *zio)
 	 * If our children haven't all reached the ready stage,
 	 * wait for them and then repeat this pipeline stage.
 	 */
-	if (zio_wait_for_children(zio, ZIO_CHILD_LOGICAL_BIT |
-	    ZIO_CHILD_GANG_BIT, ZIO_WAIT_READY)) {
+	if (zio_wait_for_children(zio, ZIO_CHILD_LOGICAL_BIT | ZIO_CHILD_GANG_BIT, ZIO_WAIT_READY))
+	{
 		return (NULL);
 	}
 
 	if (!IO_IS_ALLOCATING(zio))
 		return (zio);
 
-	if (zio->io_children_ready != NULL) {
+	if (zio->io_children_ready != NULL)
+	{
 		/*
 		 * Now that all our children are ready, run the callback
 		 * associated with this zio in case it wants to modify the
@@ -1678,7 +1751,8 @@ zio_write_compress(zio_t *zio)
 	ASSERT(zio->io_child_type != ZIO_CHILD_DDT);
 	ASSERT(zio->io_bp_override == NULL);
 
-	if (!BP_IS_HOLE(bp) && bp->blk_birth == zio->io_txg) {
+	if (!BP_IS_HOLE(bp) && bp->blk_birth == zio->io_txg)
+	{
 		/*
 		 * We're rewriting an existing block, which means we're
 		 * working on behalf of spa_sync().  For spa_sync() to
@@ -1699,24 +1773,28 @@ zio_write_compress(zio_t *zio)
 
 		/* Make sure someone doesn't change their mind on overwrites */
 		ASSERT(BP_IS_EMBEDDED(bp) || MIN(zp->zp_copies + BP_IS_GANG(bp),
-		    spa_max_replication(spa)) == BP_GET_NDVAS(bp));
+										 spa_max_replication(spa)) == BP_GET_NDVAS(bp));
 	}
 
 	/* If it's a compressed write that is not raw, compress the buffer. */
 	if (compress != ZIO_COMPRESS_OFF &&
-	    !(zio->io_flags & ZIO_FLAG_RAW_COMPRESS)) {
+		!(zio->io_flags & ZIO_FLAG_RAW_COMPRESS))
+	{
 		void *cbuf = zio_buf_alloc(lsize);
 		psize = zio_compress_data(compress, zio->io_abd, cbuf, lsize,
-		    zp->zp_complevel);
-		if (psize == 0 || psize >= lsize) {
+								  zp->zp_complevel);
+		if (psize == 0 || psize >= lsize)
+		{
 			compress = ZIO_COMPRESS_OFF;
 			zio_buf_free(cbuf, lsize);
-		} else if (!zp->zp_dedup && !zp->zp_encrypt &&
-		    psize <= BPE_PAYLOAD_SIZE &&
-		    zp->zp_level == 0 && !DMU_OT_HAS_FILL(zp->zp_type) &&
-		    spa_feature_is_enabled(spa, SPA_FEATURE_EMBEDDED_DATA)) {
+		}
+		else if (!zp->zp_dedup && !zp->zp_encrypt &&
+				 psize <= BPE_PAYLOAD_SIZE &&
+				 zp->zp_level == 0 && !DMU_OT_HAS_FILL(zp->zp_type) &&
+				 spa_feature_is_enabled(spa, SPA_FEATURE_EMBEDDED_DATA))
+		{
 			encode_embedded_bp_compressed(bp,
-			    cbuf, compress, lsize, psize);
+										  cbuf, compress, lsize, psize);
 			BPE_SET_ETYPE(bp, BP_EMBEDDED_TYPE_DATA);
 			BP_SET_TYPE(bp, zio->io_prop.zp_type);
 			BP_SET_LEVEL(bp, zio->io_prop.zp_level);
@@ -1724,9 +1802,11 @@ zio_write_compress(zio_t *zio)
 			bp->blk_birth = zio->io_txg;
 			zio->io_pipeline = ZIO_INTERLOCK_PIPELINE;
 			ASSERT(spa_feature_is_active(spa,
-			    SPA_FEATURE_EMBEDDED_DATA));
+										 SPA_FEATURE_EMBEDDED_DATA));
 			return (zio);
-		} else {
+		}
+		else
+		{
 			/*
 			 * Round compressed size up to the minimum allocation
 			 * size of the smallest-ashift device, and zero the
@@ -1737,18 +1817,21 @@ zio_write_compress(zio_t *zio)
 			 */
 			ASSERT3U(spa->spa_min_alloc, >=, SPA_MINBLOCKSHIFT);
 			size_t rounded = (size_t)roundup(psize,
-			    spa->spa_min_alloc);
-			if (rounded >= lsize) {
+											 spa->spa_min_alloc);
+			if (rounded >= lsize)
+			{
 				compress = ZIO_COMPRESS_OFF;
 				zio_buf_free(cbuf, lsize);
 				psize = lsize;
-			} else {
+			}
+			else
+			{
 				abd_t *cdata = abd_get_from_buf(cbuf, lsize);
 				abd_take_ownership_of_buf(cdata, B_TRUE);
 				abd_zero_off(cdata, psize, rounded - psize);
 				psize = rounded;
 				zio_push_transform(zio, cdata,
-				    psize, lsize, NULL);
+								   psize, lsize, NULL);
 			}
 		}
 
@@ -1759,9 +1842,10 @@ zio_write_compress(zio_t *zio)
 		zio->io_bp_override = NULL;
 		*bp = zio->io_bp_orig;
 		zio->io_pipeline = zio->io_orig_pipeline;
-
-	} else if ((zio->io_flags & ZIO_FLAG_RAW_ENCRYPT) != 0 &&
-	    zp->zp_type == DMU_OT_DNODE) {
+	}
+	else if ((zio->io_flags & ZIO_FLAG_RAW_ENCRYPT) != 0 &&
+			 zp->zp_type == DMU_OT_DNODE)
+	{
 		/*
 		 * The DMU actually relies on the zio layer's compression
 		 * to free metadnode blocks that have had all contained
@@ -1770,28 +1854,34 @@ zio_write_compress(zio_t *zio)
 		 * to a hole.
 		 */
 		psize = zio_compress_data(ZIO_COMPRESS_EMPTY,
-		    zio->io_abd, NULL, lsize, zp->zp_complevel);
+								  zio->io_abd, NULL, lsize, zp->zp_complevel);
 		if (psize == 0 || psize >= lsize)
 			compress = ZIO_COMPRESS_OFF;
-	} else if (zio->io_flags & ZIO_FLAG_RAW_COMPRESS &&
-	    !(zio->io_flags & ZIO_FLAG_RAW_ENCRYPT)) {
+	}
+	else if (zio->io_flags & ZIO_FLAG_RAW_COMPRESS &&
+			 !(zio->io_flags & ZIO_FLAG_RAW_ENCRYPT))
+	{
 		/*
 		 * If we are raw receiving an encrypted dataset we should not
 		 * take this codepath because it will change the on-disk block
 		 * and decryption will fail.
 		 */
 		size_t rounded = MIN((size_t)roundup(psize,
-		    spa->spa_min_alloc), lsize);
+											 spa->spa_min_alloc),
+							 lsize);
 
-		if (rounded != psize) {
+		if (rounded != psize)
+		{
 			abd_t *cdata = abd_alloc_linear(rounded, B_TRUE);
 			abd_zero_off(cdata, psize, rounded - psize);
 			abd_copy_off(cdata, zio->io_abd, 0, 0, psize);
 			psize = rounded;
 			zio_push_transform(zio, cdata,
-			    psize, rounded, NULL);
+							   psize, rounded, NULL);
 		}
-	} else {
+	}
+	else
+	{
 		ASSERT3U(psize, !=, 0);
 	}
 
@@ -1804,28 +1894,35 @@ zio_write_compress(zio_t *zio)
 	 * There should only be a handful of blocks after pass 1 in any case.
 	 */
 	if (!BP_IS_HOLE(bp) && bp->blk_birth == zio->io_txg &&
-	    BP_GET_PSIZE(bp) == psize &&
-	    pass >= zfs_sync_pass_rewrite) {
+		BP_GET_PSIZE(bp) == psize &&
+		pass >= zfs_sync_pass_rewrite)
+	{
 		VERIFY3U(psize, !=, 0);
 		enum zio_stage gang_stages = zio->io_pipeline & ZIO_GANG_STAGES;
 
 		zio->io_pipeline = ZIO_REWRITE_PIPELINE | gang_stages;
 		zio->io_flags |= ZIO_FLAG_IO_REWRITE;
-	} else {
+	}
+	else
+	{
 		BP_ZERO(bp);
 		zio->io_pipeline = ZIO_WRITE_PIPELINE;
 	}
 
-	if (psize == 0) {
+	if (psize == 0)
+	{
 		if (zio->io_bp_orig.blk_birth != 0 &&
-		    spa_feature_is_active(spa, SPA_FEATURE_HOLE_BIRTH)) {
+			spa_feature_is_active(spa, SPA_FEATURE_HOLE_BIRTH))
+		{
 			BP_SET_LSIZE(bp, lsize);
 			BP_SET_TYPE(bp, zp->zp_type);
 			BP_SET_LEVEL(bp, zp->zp_level);
 			BP_SET_BIRTH(bp, zio->io_txg, 0);
 		}
 		zio->io_pipeline = ZIO_INTERLOCK_PIPELINE;
-	} else {
+	}
+	else
+	{
 		ASSERT(zp->zp_checksum != ZIO_CHECKSUM_GANG_HEADER);
 		BP_SET_LSIZE(bp, lsize);
 		BP_SET_TYPE(bp, zp->zp_type);
@@ -1835,14 +1932,16 @@ zio_write_compress(zio_t *zio)
 		BP_SET_CHECKSUM(bp, zp->zp_checksum);
 		BP_SET_DEDUP(bp, zp->zp_dedup);
 		BP_SET_BYTEORDER(bp, ZFS_HOST_BYTEORDER);
-		if (zp->zp_dedup) {
+		if (zp->zp_dedup)
+		{
 			ASSERT(zio->io_child_type == ZIO_CHILD_LOGICAL);
 			ASSERT(!(zio->io_flags & ZIO_FLAG_IO_REWRITE));
 			ASSERT(!zp->zp_encrypt ||
-			    DMU_OT_IS_ENCRYPTED(zp->zp_type));
+				   DMU_OT_IS_ENCRYPTED(zp->zp_type));
 			zio->io_pipeline = ZIO_DDT_WRITE_PIPELINE;
 		}
-		if (zp->zp_nopwrite) {
+		if (zp->zp_nopwrite)
+		{
 			ASSERT(zio->io_child_type == ZIO_CHILD_LOGICAL);
 			ASSERT(!(zio->io_flags & ZIO_FLAG_IO_REWRITE));
 			zio->io_pipeline |= ZIO_STAGE_NOP_WRITE;
@@ -1856,7 +1955,8 @@ zio_free_bp_init(zio_t *zio)
 {
 	blkptr_t *bp = zio->io_bp;
 
-	if (zio->io_child_type == ZIO_CHILD_LOGICAL) {
+	if (zio->io_child_type == ZIO_CHILD_LOGICAL)
+	{
 		if (BP_GET_DEDUP(bp))
 			zio->io_pipeline = ZIO_DDT_FREE_PIPELINE;
 	}
@@ -1898,8 +1998,8 @@ zio_taskq_dispatch(zio_t *zio, zio_taskq_type_t q, boolean_t cutinline)
 	 * available.
 	 */
 	if ((zio->io_priority == ZIO_PRIORITY_NOW ||
-	    zio->io_priority == ZIO_PRIORITY_SYNC_WRITE) &&
-	    spa->spa_zio_taskq[t][q + 1].stqs_count != 0)
+		 zio->io_priority == ZIO_PRIORITY_SYNC_WRITE) &&
+		spa->spa_zio_taskq[t][q + 1].stqs_count != 0)
 		q++;
 
 	ASSERT3U(q, <, ZIO_TASKQ_TYPES);
@@ -1911,7 +2011,7 @@ zio_taskq_dispatch(zio_t *zio, zio_taskq_type_t q, boolean_t cutinline)
 	 */
 	ASSERT(taskq_empty_ent(&zio->io_tqent));
 	spa_taskq_dispatch_ent(spa, t, q, zio_execute, zio, flags,
-	    &zio->io_tqent);
+						   &zio->io_tqent);
 }
 
 static boolean_t
@@ -1921,10 +2021,12 @@ zio_taskq_member(zio_t *zio, zio_taskq_type_t q)
 
 	taskq_t *tq = taskq_of_curthread();
 
-	for (zio_type_t t = 0; t < ZIO_TYPES; t++) {
+	for (zio_type_t t = 0; t < ZIO_TYPES; t++)
+	{
 		spa_taskqs_t *tqs = &spa->spa_zio_taskq[t][q];
 		uint_t i;
-		for (i = 0; i < tqs->stqs_count; i++) {
+		for (i = 0; i < tqs->stqs_count; i++)
+		{
 			if (tqs->stqs_taskq[i] == tq)
 				return (B_TRUE);
 		}
@@ -1941,14 +2043,12 @@ zio_issue_async(zio_t *zio)
 	return (NULL);
 }
 
-void
-zio_interrupt(void *zio)
+void zio_interrupt(void *zio)
 {
 	zio_taskq_dispatch(zio, ZIO_TASKQ_INTERRUPT, B_FALSE);
 }
 
-void
-zio_delay_interrupt(zio_t *zio)
+void zio_delay_interrupt(zio_t *zio)
 {
 	/*
 	 * The timeout_generic() function isn't defined in userspace, so
@@ -1962,10 +2062,12 @@ zio_delay_interrupt(zio_t *zio)
 	 * for this IO, thus jump to the end of this function and "skip" the
 	 * delay; issuing it directly to the zio layer.
 	 */
-	if (zio->io_target_timestamp != 0) {
+	if (zio->io_target_timestamp != 0)
+	{
 		hrtime_t now = gethrtime();
 
-		if (now >= zio->io_target_timestamp) {
+		if (now >= zio->io_target_timestamp)
+		{
 			/*
 			 * This IO has already taken longer than the target
 			 * delay to complete, so we don't want to delay it
@@ -1978,31 +2080,37 @@ zio_delay_interrupt(zio_t *zio)
 			 */
 
 			DTRACE_PROBE2(zio__delay__miss, zio_t *, zio,
-			    hrtime_t, now);
+						  hrtime_t, now);
 
 			zio_interrupt(zio);
-		} else {
+		}
+		else
+		{
 			taskqid_t tid;
 			hrtime_t diff = zio->io_target_timestamp - now;
 			clock_t expire_at_tick = ddi_get_lbolt() +
-			    NSEC_TO_TICK(diff);
+									 NSEC_TO_TICK(diff);
 
 			DTRACE_PROBE3(zio__delay__hit, zio_t *, zio,
-			    hrtime_t, now, hrtime_t, diff);
+						  hrtime_t, now, hrtime_t, diff);
 
-			if (NSEC_TO_TICK(diff) == 0) {
+			if (NSEC_TO_TICK(diff) == 0)
+			{
 				/* Our delay is less than a jiffy - just spin */
 				zfs_sleep_until(zio->io_target_timestamp);
 				zio_interrupt(zio);
-			} else {
+			}
+			else
+			{
 				/*
 				 * Use taskq_dispatch_delay() in the place of
 				 * OpenZFS's timeout_generic().
 				 */
 				tid = taskq_dispatch_delay(system_taskq,
-				    zio_interrupt, zio, TQ_NOSLEEP,
-				    expire_at_tick);
-				if (tid == TASKQID_INVALID) {
+										   zio_interrupt, zio, TQ_NOSLEEP,
+										   expire_at_tick);
+				if (tid == TASKQID_INVALID)
+				{
 					/*
 					 * Couldn't allocate a task.  Just
 					 * finish the zio without a delay.
@@ -2025,43 +2133,46 @@ zio_deadman_impl(zio_t *pio, int ziodepth)
 	zio_link_t *zl = NULL;
 	vdev_t *vd = pio->io_vd;
 
-	if (zio_deadman_log_all || (vd != NULL && vd->vdev_ops->vdev_op_leaf)) {
+	if (zio_deadman_log_all || (vd != NULL && vd->vdev_ops->vdev_op_leaf))
+	{
 		vdev_queue_t *vq = vd ? &vd->vdev_queue : NULL;
 		zbookmark_phys_t *zb = &pio->io_bookmark;
 		uint64_t delta = gethrtime() - pio->io_timestamp;
 		uint64_t failmode = spa_get_deadman_failmode(pio->io_spa);
 
 		zfs_dbgmsg("slow zio[%d]: zio=%px timestamp=%llu "
-		    "delta=%llu queued=%llu io=%llu "
-		    "path=%s "
-		    "last=%llu type=%d "
-		    "priority=%d flags=0x%x stage=0x%x "
-		    "pipeline=0x%x pipeline-trace=0x%x "
-		    "objset=%llu object=%llu "
-		    "level=%llu blkid=%llu "
-		    "offset=%llu size=%llu "
-		    "error=%d",
-		    ziodepth, pio, pio->io_timestamp,
-		    (u_longlong_t)delta, pio->io_delta, pio->io_delay,
-		    vd ? vd->vdev_path : "NULL",
-		    vq ? vq->vq_io_complete_ts : 0, pio->io_type,
-		    pio->io_priority, pio->io_flags, pio->io_stage,
-		    pio->io_pipeline, pio->io_pipeline_trace,
-		    (u_longlong_t)zb->zb_objset, (u_longlong_t)zb->zb_object,
-		    (u_longlong_t)zb->zb_level, (u_longlong_t)zb->zb_blkid,
-		    (u_longlong_t)pio->io_offset, (u_longlong_t)pio->io_size,
-		    pio->io_error);
-		(void) zfs_ereport_post(FM_EREPORT_ZFS_DEADMAN,
-		    pio->io_spa, vd, zb, pio, 0);
+				   "delta=%llu queued=%llu io=%llu "
+				   "path=%s "
+				   "last=%llu type=%d "
+				   "priority=%d flags=0x%x stage=0x%x "
+				   "pipeline=0x%x pipeline-trace=0x%x "
+				   "objset=%llu object=%llu "
+				   "level=%llu blkid=%llu "
+				   "offset=%llu size=%llu "
+				   "error=%d",
+				   ziodepth, pio, pio->io_timestamp,
+				   (u_longlong_t)delta, pio->io_delta, pio->io_delay,
+				   vd ? vd->vdev_path : "NULL",
+				   vq ? vq->vq_io_complete_ts : 0, pio->io_type,
+				   pio->io_priority, pio->io_flags, pio->io_stage,
+				   pio->io_pipeline, pio->io_pipeline_trace,
+				   (u_longlong_t)zb->zb_objset, (u_longlong_t)zb->zb_object,
+				   (u_longlong_t)zb->zb_level, (u_longlong_t)zb->zb_blkid,
+				   (u_longlong_t)pio->io_offset, (u_longlong_t)pio->io_size,
+				   pio->io_error);
+		(void)zfs_ereport_post(FM_EREPORT_ZFS_DEADMAN,
+							   pio->io_spa, vd, zb, pio, 0);
 
 		if (failmode == ZIO_FAILURE_MODE_CONTINUE &&
-		    taskq_empty_ent(&pio->io_tqent)) {
+			taskq_empty_ent(&pio->io_tqent))
+		{
 			zio_interrupt(pio);
 		}
 	}
 
 	mutex_enter(&pio->io_lock);
-	for (cio = zio_walk_children(pio, &zl); cio != NULL; cio = cio_next) {
+	for (cio = zio_walk_children(pio, &zl); cio != NULL; cio = cio_next)
+	{
 		cio_next = zio_walk_children(pio, &zl);
 		zio_deadman_impl(cio, ziodepth + 1);
 	}
@@ -2072,8 +2183,7 @@ zio_deadman_impl(zio_t *pio, int ziodepth)
  * Log the critical information describing this zio and all of its children
  * using the zfs_dbgmsg() interface then post deadman event for the ZED.
  */
-void
-zio_deadman(zio_t *pio, char *tag)
+void zio_deadman(zio_t *pio, char *tag)
 {
 	spa_t *spa = pio->io_spa;
 	char *name = spa_name(spa);
@@ -2083,7 +2193,8 @@ zio_deadman(zio_t *pio, char *tag)
 
 	zio_deadman_impl(pio, 0);
 
-	switch (spa_get_deadman_failmode(spa)) {
+	switch (spa_get_deadman_failmode(spa))
+	{
 	case ZIO_FAILURE_MODE_WAIT:
 		zfs_dbgmsg("%s waiting for hung I/O to pool '%s'", tag, name);
 		break;
@@ -2121,8 +2232,7 @@ static zio_pipe_stage_t *zio_pipeline[];
  * code paths.  zio_execute() itself cannot be inlined because
  * it is externally visible.
  */
-void
-zio_execute(void *zio)
+void zio_execute(void *zio)
 {
 	fstrans_cookie_t cookie;
 
@@ -2148,23 +2258,25 @@ zio_execute_stack_check(zio_t *zio)
 
 	/* Pool initialization outside of zio_taskq context. */
 	if (dp && spa_is_initializing(dp->dp_spa) &&
-	    !zio_taskq_member(zio, ZIO_TASKQ_ISSUE) &&
-	    !zio_taskq_member(zio, ZIO_TASKQ_ISSUE_HIGH))
+		!zio_taskq_member(zio, ZIO_TASKQ_ISSUE) &&
+		!zio_taskq_member(zio, ZIO_TASKQ_ISSUE_HIGH))
 		return (B_TRUE);
 #else
-	(void) zio;
+	(void)zio;
 #endif /* HAVE_LARGE_STACKS */
 
 	return (B_FALSE);
 }
 
-__attribute__((always_inline))
-static inline void
+__attribute__((always_inline)) static inline void
 __zio_execute(zio_t *zio)
 {
+	// zfs_dbgmsg("__zio_execute callled");
 	ASSERT3U(zio->io_queued_timestamp, >, 0);
 
-	while (zio->io_stage < ZIO_STAGE_DONE) {
+	while (zio->io_stage < ZIO_STAGE_DONE)
+	{
+		// zfs_dbgmsg("Current io stage %d", highbit64(zio->io_stage));
 		enum zio_stage pipeline = zio->io_pipeline;
 		enum zio_stage stage = zio->io_stage;
 
@@ -2174,12 +2286,14 @@ __zio_execute(zio_t *zio)
 		ASSERT(ISP2(stage));
 		ASSERT(zio->io_stall == NULL);
 
-		do {
+		do
+		{
 			stage <<= 1;
 		} while ((stage & pipeline) == 0);
 
-		ASSERT(stage <= ZIO_STAGE_DONE);
+		// zfs_dbgmsg("After while loop, stage is %d", highbit64(stage));
 
+		ASSERT(stage <= ZIO_STAGE_DONE);
 		/*
 		 * If we are in interrupt context and this pipeline stage
 		 * will grab a config lock that is held across I/O,
@@ -2190,9 +2304,10 @@ __zio_execute(zio_t *zio)
 		 * be sent to disk promptly.
 		 */
 		if ((stage & ZIO_BLOCKING_STAGES) && zio->io_vd == NULL &&
-		    zio_taskq_member(zio, ZIO_TASKQ_INTERRUPT)) {
-			boolean_t cut = (stage == ZIO_STAGE_VDEV_IO_START) ?
-			    zio_requeue_io_start_cut_in_line : B_FALSE;
+			zio_taskq_member(zio, ZIO_TASKQ_INTERRUPT))
+		{
+			// zfs_dbgmsg("We are cutting the line");
+			boolean_t cut = (stage == ZIO_STAGE_VDEV_IO_START) ? zio_requeue_io_start_cut_in_line : B_FALSE;
 			zio_taskq_dispatch(zio, ZIO_TASKQ_ISSUE, cut);
 			return;
 		}
@@ -2201,9 +2316,10 @@ __zio_execute(zio_t *zio)
 		 * If the current context doesn't have large enough stacks
 		 * the zio must be issued asynchronously to prevent overflow.
 		 */
-		if (zio_execute_stack_check(zio)) {
-			boolean_t cut = (stage == ZIO_STAGE_VDEV_IO_START) ?
-			    zio_requeue_io_start_cut_in_line : B_FALSE;
+		if (zio_execute_stack_check(zio))
+		{
+			zfs_dbgmsg("Async execution");
+			boolean_t cut = (stage == ZIO_STAGE_VDEV_IO_START) ? zio_requeue_io_start_cut_in_line : B_FALSE;
 			zio_taskq_dispatch(zio, ZIO_TASKQ_ISSUE, cut);
 			return;
 		}
@@ -2216,21 +2332,25 @@ __zio_execute(zio_t *zio)
 		 * (typically the same as this one), or NULL if we should
 		 * stop.
 		 */
+		// zfs_dbgmsg("Executing zio pipeline %d", highbit64(stage) - 1);
 		zio = zio_pipeline[highbit64(stage) - 1](zio);
 
 		if (zio == NULL)
+		{
+			// zfs_dbgmsg("zio became null!");
 			return;
+		}
 	}
-}
 
+	// zfs_dbgmsg("io pipeline done, stage at %d", zio->io_stage);
+}
 
 /*
  * ==========================================================================
  * Initiate I/O, either sync or async
  * ==========================================================================
  */
-int
-zio_wait(zio_t *zio)
+int zio_wait(zio_t *zio)
 {
 	/*
 	 * Some routines, like zio_free_sync(), may return a NULL zio
@@ -2251,16 +2371,20 @@ zio_wait(zio_t *zio)
 	ASSERT0(zio->io_queued_timestamp);
 	zio->io_queued_timestamp = gethrtime();
 
+	// zfs_dbgmsg("The num of parent zio %ld", zio->io_parent_count);
+
 	__zio_execute(zio);
 
 	mutex_enter(&zio->io_lock);
-	while (zio->io_executor != NULL) {
+	while (zio->io_executor != NULL)
+	{
 		error = cv_timedwait_io(&zio->io_cv, &zio->io_lock,
-		    ddi_get_lbolt() + timeout);
+								ddi_get_lbolt() + timeout);
 
 		if (zfs_deadman_enabled && error == -1 &&
-		    gethrtime() - zio->io_queued_timestamp >
-		    spa_deadman_ziotime(zio->io_spa)) {
+			gethrtime() - zio->io_queued_timestamp >
+				spa_deadman_ziotime(zio->io_spa))
+		{
 			mutex_exit(&zio->io_lock);
 			timeout = MSEC_TO_TICK(zfs_deadman_checktime_ms);
 			zio_deadman(zio, FTAG);
@@ -2275,8 +2399,7 @@ zio_wait(zio_t *zio)
 	return (error);
 }
 
-void
-zio_nowait(zio_t *zio)
+void zio_nowait(zio_t *zio)
 {
 	/*
 	 * See comment in zio_wait().
@@ -2287,7 +2410,8 @@ zio_nowait(zio_t *zio)
 	ASSERT3P(zio->io_executor, ==, NULL);
 
 	if (zio->io_child_type == ZIO_CHILD_LOGICAL &&
-	    list_is_empty(&zio->io_parent_list)) {
+		list_is_empty(&zio->io_parent_list))
+	{
 		zio_t *pio;
 
 		/*
@@ -2303,6 +2427,7 @@ zio_nowait(zio_t *zio)
 
 	ASSERT0(zio->io_queued_timestamp);
 	zio->io_queued_timestamp = gethrtime();
+	// zfs_dbgmsg("Calling __zio_execute()");
 	__zio_execute(zio);
 }
 
@@ -2347,7 +2472,8 @@ zio_reexecute(void *arg)
 	 */
 	zio_link_t *zl = NULL;
 	mutex_enter(&pio->io_lock);
-	for (cio = zio_walk_children(pio, &zl); cio != NULL; cio = cio_next) {
+	for (cio = zio_walk_children(pio, &zl); cio != NULL; cio = cio_next)
+	{
 		cio_next = zio_walk_children(pio, &zl);
 		for (int w = 0; w < ZIO_WAIT_TYPES; w++)
 			pio->io_children[cio->io_child_type][w]++;
@@ -2362,36 +2488,39 @@ zio_reexecute(void *arg)
 	 * We don't reexecute "The Godfather" I/O here as it's the
 	 * responsibility of the caller to wait on it.
 	 */
-	if (!(pio->io_flags & ZIO_FLAG_GODFATHER)) {
+	if (!(pio->io_flags & ZIO_FLAG_GODFATHER))
+	{
 		pio->io_queued_timestamp = gethrtime();
 		__zio_execute(pio);
 	}
 }
 
-void
-zio_suspend(spa_t *spa, zio_t *zio, zio_suspend_reason_t reason)
+void zio_suspend(spa_t *spa, zio_t *zio, zio_suspend_reason_t reason)
 {
 	if (spa_get_failmode(spa) == ZIO_FAILURE_MODE_PANIC)
 		fm_panic("Pool '%s' has encountered an uncorrectable I/O "
-		    "failure and the failure mode property for this pool "
-		    "is set to panic.", spa_name(spa));
+				 "failure and the failure mode property for this pool "
+				 "is set to panic.",
+				 spa_name(spa));
 
 	cmn_err(CE_WARN, "Pool '%s' has encountered an uncorrectable I/O "
-	    "failure and has been suspended.\n", spa_name(spa));
+					 "failure and has been suspended.\n",
+			spa_name(spa));
 
-	(void) zfs_ereport_post(FM_EREPORT_ZFS_IO_FAILURE, spa, NULL,
-	    NULL, NULL, 0);
+	(void)zfs_ereport_post(FM_EREPORT_ZFS_IO_FAILURE, spa, NULL,
+						   NULL, NULL, 0);
 
 	mutex_enter(&spa->spa_suspend_lock);
 
 	if (spa->spa_suspend_zio_root == NULL)
 		spa->spa_suspend_zio_root = zio_root(spa, NULL, NULL,
-		    ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE |
-		    ZIO_FLAG_GODFATHER);
+											 ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE |
+												 ZIO_FLAG_GODFATHER);
 
 	spa->spa_suspended = reason;
 
-	if (zio != NULL) {
+	if (zio != NULL)
+	{
 		ASSERT(!(zio->io_flags & ZIO_FLAG_GODFATHER));
 		ASSERT(zio != spa->spa_suspend_zio_root);
 		ASSERT(zio->io_child_type == ZIO_CHILD_LOGICAL);
@@ -2403,8 +2532,7 @@ zio_suspend(spa_t *spa, zio_t *zio, zio_suspend_reason_t reason)
 	mutex_exit(&spa->spa_suspend_lock);
 }
 
-int
-zio_resume(spa_t *spa)
+int zio_resume(spa_t *spa)
 {
 	zio_t *pio;
 
@@ -2425,8 +2553,7 @@ zio_resume(spa_t *spa)
 	return (zio_wait(pio));
 }
 
-void
-zio_resume_wait(spa_t *spa)
+void zio_resume_wait(spa_t *spa)
 {
 	mutex_enter(&spa->spa_suspend_lock);
 	while (spa_suspended(spa))
@@ -2508,30 +2635,31 @@ zio_gang_issue_func_done(zio_t *zio)
 
 static zio_t *
 zio_read_gang(zio_t *pio, blkptr_t *bp, zio_gang_node_t *gn, abd_t *data,
-    uint64_t offset)
+			  uint64_t offset)
 {
 	if (gn != NULL)
 		return (pio);
 
 	return (zio_read(pio, pio->io_spa, bp, abd_get_offset(data, offset),
-	    BP_GET_PSIZE(bp), zio_gang_issue_func_done,
-	    NULL, pio->io_priority, ZIO_GANG_CHILD_FLAGS(pio),
-	    &pio->io_bookmark));
+					 BP_GET_PSIZE(bp), zio_gang_issue_func_done,
+					 NULL, pio->io_priority, ZIO_GANG_CHILD_FLAGS(pio),
+					 &pio->io_bookmark));
 }
 
 static zio_t *
 zio_rewrite_gang(zio_t *pio, blkptr_t *bp, zio_gang_node_t *gn, abd_t *data,
-    uint64_t offset)
+				 uint64_t offset)
 {
 	zio_t *zio;
 
-	if (gn != NULL) {
+	if (gn != NULL)
+	{
 		abd_t *gbh_abd =
-		    abd_get_from_buf(gn->gn_gbh, SPA_GANGBLOCKSIZE);
+			abd_get_from_buf(gn->gn_gbh, SPA_GANGBLOCKSIZE);
 		zio = zio_rewrite(pio, pio->io_spa, pio->io_txg, bp,
-		    gbh_abd, SPA_GANGBLOCKSIZE, zio_gang_issue_func_done, NULL,
-		    pio->io_priority, ZIO_GANG_CHILD_FLAGS(pio),
-		    &pio->io_bookmark);
+						  gbh_abd, SPA_GANGBLOCKSIZE, zio_gang_issue_func_done, NULL,
+						  pio->io_priority, ZIO_GANG_CHILD_FLAGS(pio),
+						  &pio->io_bookmark);
 		/*
 		 * As we rewrite each gang header, the pipeline will compute
 		 * a new gang block header checksum for it; but no one will
@@ -2541,11 +2669,12 @@ zio_rewrite_gang(zio_t *pio, blkptr_t *bp, zio_gang_node_t *gn, abd_t *data,
 		 * (Presently, nothing actually uses interior data checksums;
 		 * this is just good hygiene.)
 		 */
-		if (gn != pio->io_gang_leader->io_gang_tree) {
+		if (gn != pio->io_gang_leader->io_gang_tree)
+		{
 			abd_t *buf = abd_get_offset(data, offset);
 
 			zio_checksum_compute(zio, BP_GET_CHECKSUM(bp),
-			    buf, BP_GET_PSIZE(bp));
+								 buf, BP_GET_PSIZE(bp));
 
 			abd_free(buf);
 		}
@@ -2555,11 +2684,13 @@ zio_rewrite_gang(zio_t *pio, blkptr_t *bp, zio_gang_node_t *gn, abd_t *data,
 		 */
 		if (pio->io_gang_leader->io_flags & ZIO_FLAG_INDUCE_DAMAGE)
 			zio->io_pipeline &= ~ZIO_VDEV_IO_STAGES;
-	} else {
+	}
+	else
+	{
 		zio = zio_rewrite(pio, pio->io_spa, pio->io_txg, bp,
-		    abd_get_offset(data, offset), BP_GET_PSIZE(bp),
-		    zio_gang_issue_func_done, NULL, pio->io_priority,
-		    ZIO_GANG_CHILD_FLAGS(pio), &pio->io_bookmark);
+						  abd_get_offset(data, offset), BP_GET_PSIZE(bp),
+						  zio_gang_issue_func_done, NULL, pio->io_priority,
+						  ZIO_GANG_CHILD_FLAGS(pio), &pio->io_bookmark);
 	}
 
 	return (zio);
@@ -2567,26 +2698,27 @@ zio_rewrite_gang(zio_t *pio, blkptr_t *bp, zio_gang_node_t *gn, abd_t *data,
 
 static zio_t *
 zio_free_gang(zio_t *pio, blkptr_t *bp, zio_gang_node_t *gn, abd_t *data,
-    uint64_t offset)
+			  uint64_t offset)
 {
-	(void) gn, (void) data, (void) offset;
+	(void)gn, (void)data, (void)offset;
 
 	zio_t *zio = zio_free_sync(pio, pio->io_spa, pio->io_txg, bp,
-	    ZIO_GANG_CHILD_FLAGS(pio));
-	if (zio == NULL) {
+							   ZIO_GANG_CHILD_FLAGS(pio));
+	if (zio == NULL)
+	{
 		zio = zio_null(pio, pio->io_spa,
-		    NULL, NULL, NULL, ZIO_GANG_CHILD_FLAGS(pio));
+					   NULL, NULL, NULL, ZIO_GANG_CHILD_FLAGS(pio));
 	}
 	return (zio);
 }
 
 static zio_t *
 zio_claim_gang(zio_t *pio, blkptr_t *bp, zio_gang_node_t *gn, abd_t *data,
-    uint64_t offset)
+			   uint64_t offset)
 {
-	(void) gn, (void) data, (void) offset;
+	(void)gn, (void)data, (void)offset;
 	return (zio_claim(pio, pio->io_spa, pio->io_txg, bp,
-	    NULL, NULL, ZIO_GANG_CHILD_FLAGS(pio)));
+					  NULL, NULL, ZIO_GANG_CHILD_FLAGS(pio)));
 }
 
 static zio_gang_issue_func_t *zio_gang_issue_func[ZIO_TYPES] = {
@@ -2595,8 +2727,8 @@ static zio_gang_issue_func_t *zio_gang_issue_func[ZIO_TYPES] = {
 	zio_rewrite_gang,
 	zio_free_gang,
 	zio_claim_gang,
-	NULL
-};
+	NULL,
+	NULL};
 
 static void zio_gang_tree_assemble_done(zio_t *zio);
 
@@ -2607,7 +2739,7 @@ zio_gang_node_alloc(zio_gang_node_t **gnpp)
 
 	ASSERT(*gnpp == NULL);
 
-	gn = kmem_zalloc(sizeof (*gn), KM_SLEEP);
+	gn = kmem_zalloc(sizeof(*gn), KM_SLEEP);
 	gn->gn_gbh = zio_buf_alloc(SPA_GANGBLOCKSIZE);
 	*gnpp = gn;
 
@@ -2623,7 +2755,7 @@ zio_gang_node_free(zio_gang_node_t **gnpp)
 		ASSERT(gn->gn_child[g] == NULL);
 
 	zio_buf_free(gn->gn_gbh, SPA_GANGBLOCKSIZE);
-	kmem_free(gn, sizeof (*gn));
+	kmem_free(gn, sizeof(*gn));
 	*gnpp = NULL;
 }
 
@@ -2651,8 +2783,8 @@ zio_gang_tree_assemble(zio_t *gio, blkptr_t *bp, zio_gang_node_t **gnpp)
 	ASSERT(BP_IS_GANG(bp));
 
 	zio_nowait(zio_read(gio, gio->io_spa, bp, gbh_abd, SPA_GANGBLOCKSIZE,
-	    zio_gang_tree_assemble_done, gn, gio->io_priority,
-	    ZIO_GANG_CHILD_FLAGS(gio), &gio->io_bookmark));
+						zio_gang_tree_assemble_done, gn, gio->io_priority,
+						ZIO_GANG_CHILD_FLAGS(gio), &gio->io_bookmark));
 }
 
 static void
@@ -2678,7 +2810,8 @@ zio_gang_tree_assemble_done(zio_t *zio)
 
 	abd_free(zio->io_abd);
 
-	for (int g = 0; g < SPA_GBH_NBLKPTRS; g++) {
+	for (int g = 0; g < SPA_GBH_NBLKPTRS; g++)
+	{
 		blkptr_t *gbp = &gn->gn_gbh->zg_blkptr[g];
 		if (!BP_IS_GANG(gbp))
 			continue;
@@ -2688,7 +2821,7 @@ zio_gang_tree_assemble_done(zio_t *zio)
 
 static void
 zio_gang_tree_issue(zio_t *pio, zio_gang_node_t *gn, blkptr_t *bp, abd_t *data,
-    uint64_t offset)
+					uint64_t offset)
 {
 	zio_t *gio = pio->io_gang_leader;
 	zio_t *zio;
@@ -2703,15 +2836,17 @@ zio_gang_tree_issue(zio_t *pio, zio_gang_node_t *gn, blkptr_t *bp, abd_t *data,
 	 */
 	zio = zio_gang_issue_func[gio->io_type](pio, bp, gn, data, offset);
 
-	if (gn != NULL) {
+	if (gn != NULL)
+	{
 		ASSERT(gn->gn_gbh->zg_tail.zec_magic == ZEC_MAGIC);
 
-		for (int g = 0; g < SPA_GBH_NBLKPTRS; g++) {
+		for (int g = 0; g < SPA_GBH_NBLKPTRS; g++)
+		{
 			blkptr_t *gbp = &gn->gn_gbh->zg_blkptr[g];
 			if (BP_IS_HOLE(gbp))
 				continue;
 			zio_gang_tree_issue(zio, gn->gn_child[g], gbp, data,
-			    offset);
+								offset);
 			offset += BP_GET_PSIZE(gbp);
 		}
 	}
@@ -2743,7 +2878,8 @@ zio_gang_issue(zio_t *zio)
 {
 	blkptr_t *bp = zio->io_bp;
 
-	if (zio_wait_for_children(zio, ZIO_CHILD_GANG_BIT, ZIO_WAIT_DONE)) {
+	if (zio_wait_for_children(zio, ZIO_CHILD_GANG_BIT, ZIO_WAIT_DONE))
+	{
 		return (NULL);
 	}
 
@@ -2752,7 +2888,7 @@ zio_gang_issue(zio_t *zio)
 
 	if (zio->io_child_error[ZIO_CHILD_GANG] == 0)
 		zio_gang_tree_issue(zio, zio->io_gang_tree, bp, zio->io_abd,
-		    0);
+							0);
 	else
 		zio_gang_tree_free(&zio->io_gang_tree);
 
@@ -2782,7 +2918,8 @@ zio_write_gang_member_ready(zio_t *zio)
 	ASSERT3U(BP_GET_NDVAS(zio->io_bp), <=, BP_GET_NDVAS(pio->io_bp));
 
 	mutex_enter(&pio->io_lock);
-	for (int d = 0; d < BP_GET_NDVAS(zio->io_bp); d++) {
+	for (int d = 0; d < BP_GET_NDVAS(zio->io_bp); d++)
+	{
 		ASSERT(DVA_GET_GANG(&pdva[d]));
 		asize = DVA_GET_ASIZE(&pdva[d]);
 		asize += DVA_GET_ASIZE(&cdva[d]);
@@ -2831,13 +2968,13 @@ zio_write_gang_block(zio_t *pio, metaslab_class_t *mc)
 		gbh_copies = SPA_DVAS_PER_BP - 1;
 
 	int flags = METASLAB_HINTBP_FAVOR | METASLAB_GANG_HEADER;
-	if (pio->io_flags & ZIO_FLAG_IO_ALLOCATING) {
+	if (pio->io_flags & ZIO_FLAG_IO_ALLOCATING)
+	{
 		ASSERT(pio->io_priority == ZIO_PRIORITY_ASYNC_WRITE);
 		ASSERT(has_data);
 
 		flags |= METASLAB_ASYNC_ALLOC;
-		VERIFY(zfs_refcount_held(&mc->mc_allocator[pio->io_allocator].
-		    mca_alloc_slots, pio));
+		VERIFY(zfs_refcount_held(&mc->mc_allocator[pio->io_allocator].mca_alloc_slots, pio));
 
 		/*
 		 * The logical zio has already placed a reservation for
@@ -2848,14 +2985,16 @@ zio_write_gang_block(zio_t *pio, metaslab_class_t *mc)
 		 * additional reservations for gang blocks.
 		 */
 		VERIFY(metaslab_class_throttle_reserve(mc, gbh_copies - copies,
-		    pio->io_allocator, pio, flags));
+											   pio->io_allocator, pio, flags));
 	}
 
 	error = metaslab_alloc(spa, mc, SPA_GANGBLOCKSIZE,
-	    bp, gbh_copies, txg, pio == gio ? NULL : gio->io_bp, flags,
-	    &pio->io_alloc_list, pio, pio->io_allocator);
-	if (error) {
-		if (pio->io_flags & ZIO_FLAG_IO_ALLOCATING) {
+						   bp, gbh_copies, txg, pio == gio ? NULL : gio->io_bp, flags,
+						   &pio->io_alloc_list, pio, pio->io_allocator);
+	if (error)
+	{
+		if (pio->io_flags & ZIO_FLAG_IO_ALLOCATING)
+		{
 			ASSERT(pio->io_priority == ZIO_PRIORITY_ASYNC_WRITE);
 			ASSERT(has_data);
 
@@ -2867,16 +3006,19 @@ zio_write_gang_block(zio_t *pio, metaslab_class_t *mc)
 			 * stage.
 			 */
 			metaslab_class_throttle_unreserve(mc,
-			    gbh_copies - copies, pio->io_allocator, pio);
+											  gbh_copies - copies, pio->io_allocator, pio);
 		}
 
 		pio->io_error = error;
 		return (pio);
 	}
 
-	if (pio == gio) {
+	if (pio == gio)
+	{
 		gnpp = &gio->io_gang_tree;
-	} else {
+	}
+	else
+	{
 		gnpp = pio->io_private;
 		ASSERT(pio->io_ready == zio_write_gang_member_ready);
 	}
@@ -2890,15 +3032,16 @@ zio_write_gang_block(zio_t *pio, metaslab_class_t *mc)
 	 * Create the gang header.
 	 */
 	zio = zio_rewrite(pio, spa, txg, bp, gbh_abd, SPA_GANGBLOCKSIZE,
-	    zio_write_gang_done, NULL, pio->io_priority,
-	    ZIO_GANG_CHILD_FLAGS(pio), &pio->io_bookmark);
+					  zio_write_gang_done, NULL, pio->io_priority,
+					  ZIO_GANG_CHILD_FLAGS(pio), &pio->io_bookmark);
 
 	/*
 	 * Create and nowait the gang children.
 	 */
-	for (int g = 0; resid != 0; resid -= lsize, g++) {
+	for (int g = 0; resid != 0; resid -= lsize, g++)
+	{
 		lsize = P2ROUNDUP(resid / (SPA_GBH_NBLKPTRS - g),
-		    SPA_MINBLOCKSIZE);
+						  SPA_MINBLOCKSIZE);
 		ASSERT(lsize >= SPA_MINBLOCKSIZE && lsize <= resid);
 
 		zp.zp_checksum = gio->io_prop.zp_checksum;
@@ -2917,13 +3060,16 @@ zio_write_gang_block(zio_t *pio, metaslab_class_t *mc)
 		bzero(zp.zp_mac, ZIO_DATA_MAC_LEN);
 
 		zio_t *cio = zio_write(zio, spa, txg, &gbh->zg_blkptr[g],
-		    has_data ? abd_get_offset(pio->io_abd, pio->io_size -
-		    resid) : NULL, lsize, lsize, &zp,
-		    zio_write_gang_member_ready, NULL, NULL,
-		    zio_write_gang_done, &gn->gn_child[g], pio->io_priority,
-		    ZIO_GANG_CHILD_FLAGS(pio), &pio->io_bookmark);
-
-		if (pio->io_flags & ZIO_FLAG_IO_ALLOCATING) {
+							   has_data ? abd_get_offset(pio->io_abd, pio->io_size -
+																		  resid)
+										: NULL,
+							   lsize, lsize, &zp,
+							   zio_write_gang_member_ready, NULL, NULL,
+							   zio_write_gang_done, &gn->gn_child[g], pio->io_priority,
+							   ZIO_GANG_CHILD_FLAGS(pio), &pio->io_bookmark);
+
+		if (pio->io_flags & ZIO_FLAG_IO_ALLOCATING)
+		{
 			ASSERT(pio->io_priority == ZIO_PRIORITY_ASYNC_WRITE);
 			ASSERT(has_data);
 
@@ -2933,7 +3079,7 @@ zio_write_gang_block(zio_t *pio, metaslab_class_t *mc)
 			 * slot for them here.
 			 */
 			VERIFY(metaslab_class_throttle_reserve(mc,
-			    zp.zp_copies, cio->io_allocator, cio, flags));
+												   zp.zp_copies, cio->io_allocator, cio, flags));
 		}
 		zio_nowait(cio);
 	}
@@ -2992,27 +3138,28 @@ zio_nop_write(zio_t *zio)
 	 * allocate a new bp.
 	 */
 	if (BP_IS_HOLE(bp_orig) ||
-	    !(zio_checksum_table[BP_GET_CHECKSUM(bp)].ci_flags &
-	    ZCHECKSUM_FLAG_NOPWRITE) ||
-	    BP_IS_ENCRYPTED(bp) || BP_IS_ENCRYPTED(bp_orig) ||
-	    BP_GET_CHECKSUM(bp) != BP_GET_CHECKSUM(bp_orig) ||
-	    BP_GET_COMPRESS(bp) != BP_GET_COMPRESS(bp_orig) ||
-	    BP_GET_DEDUP(bp) != BP_GET_DEDUP(bp_orig) ||
-	    zp->zp_copies != BP_GET_NDVAS(bp_orig))
+		!(zio_checksum_table[BP_GET_CHECKSUM(bp)].ci_flags &
+		  ZCHECKSUM_FLAG_NOPWRITE) ||
+		BP_IS_ENCRYPTED(bp) || BP_IS_ENCRYPTED(bp_orig) ||
+		BP_GET_CHECKSUM(bp) != BP_GET_CHECKSUM(bp_orig) ||
+		BP_GET_COMPRESS(bp) != BP_GET_COMPRESS(bp_orig) ||
+		BP_GET_DEDUP(bp) != BP_GET_DEDUP(bp_orig) ||
+		zp->zp_copies != BP_GET_NDVAS(bp_orig))
 		return (zio);
 
 	/*
 	 * If the checksums match then reset the pipeline so that we
 	 * avoid allocating a new bp and issuing any I/O.
 	 */
-	if (ZIO_CHECKSUM_EQUAL(bp->blk_cksum, bp_orig->blk_cksum)) {
+	if (ZIO_CHECKSUM_EQUAL(bp->blk_cksum, bp_orig->blk_cksum))
+	{
 		ASSERT(zio_checksum_table[zp->zp_checksum].ci_flags &
-		    ZCHECKSUM_FLAG_NOPWRITE);
+			   ZCHECKSUM_FLAG_NOPWRITE);
 		ASSERT3U(BP_GET_PSIZE(bp), ==, BP_GET_PSIZE(bp_orig));
 		ASSERT3U(BP_GET_LSIZE(bp), ==, BP_GET_LSIZE(bp_orig));
 		ASSERT(zp->zp_compress != ZIO_COMPRESS_OFF);
 		ASSERT(bcmp(&bp->blk_prop, &bp_orig->blk_prop,
-		    sizeof (uint64_t)) == 0);
+					sizeof(uint64_t)) == 0);
 
 		/*
 		 * If we're overwriting a block that is currently on an
@@ -3021,8 +3168,9 @@ zio_nop_write(zio_t *zio)
 		 */
 		spa_config_enter(zio->io_spa, SCL_VDEV, FTAG, RW_READER);
 		vdev_t *tvd = vdev_lookup_top(zio->io_spa,
-		    DVA_GET_VDEV(&bp->blk_dva[0]));
-		if (tvd->vdev_ops == &vdev_indirect_ops) {
+									  DVA_GET_VDEV(&bp->blk_dva[0]));
+		if (tvd->vdev_ops == &vdev_indirect_ops)
+		{
 			spa_config_exit(zio->io_spa, SCL_VDEV, FTAG);
 			return (zio);
 		}
@@ -3052,7 +3200,7 @@ zio_ddt_child_read_done(zio_t *zio)
 	mutex_enter(&pio->io_lock);
 	ddp = ddt_phys_select(dde, bp);
 	if (zio->io_error == 0)
-		ddt_phys_clear(ddp);	/* this ddp doesn't need repair */
+		ddt_phys_clear(ddp); /* this ddp doesn't need repair */
 
 	if (zio->io_error == 0 && dde->dde_repair_abd == NULL)
 		dde->dde_repair_abd = zio->io_abd;
@@ -3070,7 +3218,8 @@ zio_ddt_read_start(zio_t *zio)
 	ASSERT(BP_GET_PSIZE(bp) == zio->io_size);
 	ASSERT(zio->io_child_type == ZIO_CHILD_LOGICAL);
 
-	if (zio->io_child_error[ZIO_CHILD_DDT]) {
+	if (zio->io_child_error[ZIO_CHILD_DDT])
+	{
 		ddt_t *ddt = ddt_select(zio->io_spa, bp);
 		ddt_entry_t *dde = ddt_repair_start(ddt, bp);
 		ddt_phys_t *ddp = dde->dde_phys;
@@ -3083,23 +3232,23 @@ zio_ddt_read_start(zio_t *zio)
 		if (ddp_self == NULL)
 			return (zio);
 
-		for (int p = 0; p < DDT_PHYS_TYPES; p++, ddp++) {
+		for (int p = 0; p < DDT_PHYS_TYPES; p++, ddp++)
+		{
 			if (ddp->ddp_phys_birth == 0 || ddp == ddp_self)
 				continue;
 			ddt_bp_create(ddt->ddt_checksum, &dde->dde_key, ddp,
-			    &blk);
+						  &blk);
 			zio_nowait(zio_read(zio, zio->io_spa, &blk,
-			    abd_alloc_for_io(zio->io_size, B_TRUE),
-			    zio->io_size, zio_ddt_child_read_done, dde,
-			    zio->io_priority, ZIO_DDT_CHILD_FLAGS(zio) |
-			    ZIO_FLAG_DONT_PROPAGATE, &zio->io_bookmark));
+								abd_alloc_for_io(zio->io_size, B_TRUE),
+								zio->io_size, zio_ddt_child_read_done, dde,
+								zio->io_priority, ZIO_DDT_CHILD_FLAGS(zio) | ZIO_FLAG_DONT_PROPAGATE, &zio->io_bookmark));
 		}
 		return (zio);
 	}
 
 	zio_nowait(zio_read(zio, zio->io_spa, bp,
-	    zio->io_abd, zio->io_size, NULL, NULL, zio->io_priority,
-	    ZIO_DDT_CHILD_FLAGS(zio), &zio->io_bookmark));
+						zio->io_abd, zio->io_size, NULL, NULL, zio->io_priority,
+						ZIO_DDT_CHILD_FLAGS(zio), &zio->io_bookmark));
 
 	return (zio);
 }
@@ -3109,7 +3258,8 @@ zio_ddt_read_done(zio_t *zio)
 {
 	blkptr_t *bp = zio->io_bp;
 
-	if (zio_wait_for_children(zio, ZIO_CHILD_DDT_BIT, ZIO_WAIT_DONE)) {
+	if (zio_wait_for_children(zio, ZIO_CHILD_DDT_BIT, ZIO_WAIT_DONE))
+	{
 		return (NULL);
 	}
 
@@ -3117,21 +3267,25 @@ zio_ddt_read_done(zio_t *zio)
 	ASSERT(BP_GET_PSIZE(bp) == zio->io_size);
 	ASSERT(zio->io_child_type == ZIO_CHILD_LOGICAL);
 
-	if (zio->io_child_error[ZIO_CHILD_DDT]) {
+	if (zio->io_child_error[ZIO_CHILD_DDT])
+	{
 		ddt_t *ddt = ddt_select(zio->io_spa, bp);
 		ddt_entry_t *dde = zio->io_vsd;
-		if (ddt == NULL) {
+		if (ddt == NULL)
+		{
 			ASSERT(spa_load_state(zio->io_spa) != SPA_LOAD_NONE);
 			return (zio);
 		}
-		if (dde == NULL) {
+		if (dde == NULL)
+		{
 			zio->io_stage = ZIO_STAGE_DDT_READ_START >> 1;
 			zio_taskq_dispatch(zio, ZIO_TASKQ_ISSUE, B_FALSE);
 			return (NULL);
 		}
-		if (dde->dde_repair_abd != NULL) {
+		if (dde->dde_repair_abd != NULL)
+		{
 			abd_copy(zio->io_abd, dde->dde_repair_abd,
-			    zio->io_size);
+					 zio->io_size);
 			zio->io_child_error[ZIO_CHILD_DDT] = 0;
 		}
 		ddt_repair_done(ddt, dde);
@@ -3163,22 +3317,28 @@ zio_ddt_collision(zio_t *zio, ddt_t *ddt, ddt_entry_t *dde)
 	 * loaded).
 	 */
 
-	for (int p = DDT_PHYS_SINGLE; p <= DDT_PHYS_TRIPLE; p++) {
+	for (int p = DDT_PHYS_SINGLE; p <= DDT_PHYS_TRIPLE; p++)
+	{
 		zio_t *lio = dde->dde_lead_zio[p];
 
-		if (lio != NULL && do_raw) {
+		if (lio != NULL && do_raw)
+		{
 			return (lio->io_size != zio->io_size ||
-			    abd_cmp(zio->io_abd, lio->io_abd) != 0);
-		} else if (lio != NULL) {
+					abd_cmp(zio->io_abd, lio->io_abd) != 0);
+		}
+		else if (lio != NULL)
+		{
 			return (lio->io_orig_size != zio->io_orig_size ||
-			    abd_cmp(zio->io_orig_abd, lio->io_orig_abd) != 0);
+					abd_cmp(zio->io_orig_abd, lio->io_orig_abd) != 0);
 		}
 	}
 
-	for (int p = DDT_PHYS_SINGLE; p <= DDT_PHYS_TRIPLE; p++) {
+	for (int p = DDT_PHYS_SINGLE; p <= DDT_PHYS_TRIPLE; p++)
+	{
 		ddt_phys_t *ddp = &dde->dde_phys[p];
 
-		if (ddp->ddp_phys_birth != 0 && do_raw) {
+		if (ddp->ddp_phys_birth != 0 && do_raw)
+		{
 			blkptr_t blk = *zio->io_bp;
 			uint64_t psize;
 			abd_t *tmpabd;
@@ -3195,11 +3355,13 @@ zio_ddt_collision(zio_t *zio, ddt_t *ddt, ddt_entry_t *dde)
 			tmpabd = abd_alloc_for_io(psize, B_TRUE);
 
 			error = zio_wait(zio_read(NULL, spa, &blk, tmpabd,
-			    psize, NULL, NULL, ZIO_PRIORITY_SYNC_READ,
-			    ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE |
-			    ZIO_FLAG_RAW, &zio->io_bookmark));
+									  psize, NULL, NULL, ZIO_PRIORITY_SYNC_READ,
+									  ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE |
+										  ZIO_FLAG_RAW,
+									  &zio->io_bookmark));
 
-			if (error == 0) {
+			if (error == 0)
+			{
 				if (abd_cmp(tmpabd, zio->io_abd) != 0)
 					error = SET_ERROR(ENOENT);
 			}
@@ -3207,7 +3369,9 @@ zio_ddt_collision(zio_t *zio, ddt_t *ddt, ddt_entry_t *dde)
 			abd_free(tmpabd);
 			ddt_enter(ddt);
 			return (error != 0);
-		} else if (ddp->ddp_phys_birth != 0) {
+		}
+		else if (ddp->ddp_phys_birth != 0)
+		{
 			arc_buf_t *abuf = NULL;
 			arc_flags_t aflags = ARC_FLAG_WAIT;
 			blkptr_t blk = *zio->io_bp;
@@ -3221,13 +3385,14 @@ zio_ddt_collision(zio_t *zio, ddt_t *ddt, ddt_entry_t *dde)
 			ddt_exit(ddt);
 
 			error = arc_read(NULL, spa, &blk,
-			    arc_getbuf_func, &abuf, ZIO_PRIORITY_SYNC_READ,
-			    ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE,
-			    &aflags, &zio->io_bookmark);
+							 arc_getbuf_func, &abuf, ZIO_PRIORITY_SYNC_READ,
+							 ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE,
+							 &aflags, &zio->io_bookmark);
 
-			if (error == 0) {
+			if (error == 0)
+			{
 				if (abd_cmp_buf(zio->io_orig_abd, abuf->b_data,
-				    zio->io_orig_size) != 0)
+								zio->io_orig_size) != 0)
 					error = SET_ERROR(ENOENT);
 				arc_buf_destroy(abuf, &abuf);
 			}
@@ -3279,11 +3444,14 @@ zio_ddt_child_write_done(zio_t *zio)
 	ASSERT(dde->dde_lead_zio[p] == zio);
 	dde->dde_lead_zio[p] = NULL;
 
-	if (zio->io_error == 0) {
+	if (zio->io_error == 0)
+	{
 		zio_link_t *zl = NULL;
 		while (zio_walk_parents(zio, &zl) != NULL)
 			ddt_phys_addref(ddp);
-	} else {
+	}
+	else
+	{
 		ddt_phys_clear(ddp);
 	}
 
@@ -3312,7 +3480,8 @@ zio_ddt_write(zio_t *zio)
 	dde = ddt_lookup(ddt, bp, B_TRUE);
 	ddp = &dde->dde_phys[p];
 
-	if (zp->zp_dedup_verify && zio_ddt_collision(zio, ddt, dde)) {
+	if (zp->zp_dedup_verify && zio_ddt_collision(zio, ddt, dde))
+	{
 		/*
 		 * If we're using a weak checksum, upgrade to a strong checksum
 		 * and try again.  If we're already using a strong checksum,
@@ -3320,12 +3489,15 @@ zio_ddt_write(zio_t *zio)
 		 * (And automatically e-mail a paper to Nature?)
 		 */
 		if (!(zio_checksum_table[zp->zp_checksum].ci_flags &
-		    ZCHECKSUM_FLAG_DEDUP)) {
+			  ZCHECKSUM_FLAG_DEDUP))
+		{
 			zp->zp_checksum = spa_dedup_checksum(spa);
 			zio_pop_transforms(zio);
 			zio->io_stage = ZIO_STAGE_OPEN;
 			BP_ZERO(bp);
-		} else {
+		}
+		else
+		{
 			zp->zp_dedup = B_FALSE;
 			BP_SET_DEDUP(bp, B_FALSE);
 		}
@@ -3335,24 +3507,29 @@ zio_ddt_write(zio_t *zio)
 		return (zio);
 	}
 
-	if (ddp->ddp_phys_birth != 0 || dde->dde_lead_zio[p] != NULL) {
+	if (ddp->ddp_phys_birth != 0 || dde->dde_lead_zio[p] != NULL)
+	{
 		if (ddp->ddp_phys_birth != 0)
 			ddt_bp_fill(ddp, bp, txg);
 		if (dde->dde_lead_zio[p] != NULL)
 			zio_add_child(zio, dde->dde_lead_zio[p]);
 		else
 			ddt_phys_addref(ddp);
-	} else if (zio->io_bp_override) {
+	}
+	else if (zio->io_bp_override)
+	{
 		ASSERT(bp->blk_birth == txg);
 		ASSERT(BP_EQUAL(bp, zio->io_bp_override));
 		ddt_phys_fill(ddp, bp);
 		ddt_phys_addref(ddp);
-	} else {
+	}
+	else
+	{
 		cio = zio_write(zio, spa, txg, bp, zio->io_orig_abd,
-		    zio->io_orig_size, zio->io_orig_size, zp,
-		    zio_ddt_child_write_ready, NULL, NULL,
-		    zio_ddt_child_write_done, dde, zio->io_priority,
-		    ZIO_DDT_CHILD_FLAGS(zio), &zio->io_bookmark);
+						zio->io_orig_size, zio->io_orig_size, zp,
+						zio_ddt_child_write_ready, NULL, NULL,
+						zio_ddt_child_write_done, dde, zio->io_priority,
+						ZIO_DDT_CHILD_FLAGS(zio), &zio->io_bookmark);
 
 		zio_push_transform(cio, zio->io_abd, zio->io_size, 0, NULL);
 		dde->dde_lead_zio[p] = cio;
@@ -3381,7 +3558,8 @@ zio_ddt_free(zio_t *zio)
 
 	ddt_enter(ddt);
 	freedde = dde = ddt_lookup(ddt, bp, B_TRUE);
-	if (dde) {
+	if (dde)
+	{
 		ddp = ddt_phys_select(dde, bp);
 		if (ddp)
 			ddt_phys_decref(ddp);
@@ -3416,7 +3594,8 @@ zio_io_to_allocate(spa_t *spa, int allocator)
 	 */
 	ASSERT3U(zio->io_allocator, ==, allocator);
 	if (!metaslab_class_throttle_reserve(zio->io_metaslab_class,
-	    zio->io_prop.zp_copies, allocator, zio, 0)) {
+										 zio->io_prop.zp_copies, allocator, zio, 0))
+	{
 		return (NULL);
 	}
 
@@ -3435,12 +3614,13 @@ zio_dva_throttle(zio_t *zio)
 
 	/* locate an appropriate allocation class */
 	mc = spa_preferred_class(spa, zio->io_size, zio->io_prop.zp_type,
-	    zio->io_prop.zp_level, zio->io_prop.zp_zpl_smallblk);
+							 zio->io_prop.zp_level, zio->io_prop.zp_zpl_smallblk);
 
 	if (zio->io_priority == ZIO_PRIORITY_SYNC_WRITE ||
-	    !mc->mc_alloc_throttle_enabled ||
-	    zio->io_child_type == ZIO_CHILD_GANG ||
-	    zio->io_flags & ZIO_FLAG_NODATA) {
+		!mc->mc_alloc_throttle_enabled ||
+		zio->io_child_type == ZIO_CHILD_GANG ||
+		zio->io_flags & ZIO_FLAG_NODATA)
+	{
 		return (zio);
 	}
 
@@ -3458,7 +3638,8 @@ zio_dva_throttle(zio_t *zio)
 	 * level, and region to accomplish both of these goals.
 	 */
 	int allocator = (uint_t)cityhash4(bm->zb_objset, bm->zb_object,
-	    bm->zb_level, bm->zb_blkid >> 20) % spa->spa_alloc_count;
+									  bm->zb_level, bm->zb_blkid >> 20) %
+					spa->spa_alloc_count;
 	zio->io_allocator = allocator;
 	zio->io_metaslab_class = mc;
 	mutex_enter(&spa->spa_allocs[allocator].spaa_lock);
@@ -3493,7 +3674,8 @@ zio_dva_allocate(zio_t *zio)
 	int error;
 	int flags = 0;
 
-	if (zio->io_gang_leader == NULL) {
+	if (zio->io_gang_leader == NULL)
+	{
 		ASSERT(zio->io_child_type > ZIO_CHILD_GANG);
 		zio->io_gang_leader = zio;
 	}
@@ -3516,10 +3698,11 @@ zio_dva_allocate(zio_t *zio)
 	 * if not already chosen, locate an appropriate allocation class
 	 */
 	mc = zio->io_metaslab_class;
-	if (mc == NULL) {
+	if (mc == NULL)
+	{
 		mc = spa_preferred_class(spa, zio->io_size,
-		    zio->io_prop.zp_type, zio->io_prop.zp_level,
-		    zio->io_prop.zp_zpl_smallblk);
+								 zio->io_prop.zp_type, zio->io_prop.zp_level,
+								 zio->io_prop.zp_zpl_smallblk);
 		zio->io_metaslab_class = mc;
 	}
 
@@ -3535,58 +3718,65 @@ zio_dva_allocate(zio_t *zio)
 	 * back to spa_sync() which is abysmal for performance.
 	 */
 	error = metaslab_alloc(spa, mc, zio->io_size, bp,
-	    zio->io_prop.zp_copies, zio->io_txg, NULL, flags,
-	    &zio->io_alloc_list, zio, zio->io_allocator);
+						   zio->io_prop.zp_copies, zio->io_txg, NULL, flags,
+						   &zio->io_alloc_list, zio, zio->io_allocator);
 
 	/*
 	 * Fallback to normal class when an alloc class is full
 	 */
-	if (error == ENOSPC && mc != spa_normal_class(spa)) {
+	if (error == ENOSPC && mc != spa_normal_class(spa))
+	{
 		/*
 		 * If throttling, transfer reservation over to normal class.
 		 * The io_allocator slot can remain the same even though we
 		 * are switching classes.
 		 */
 		if (mc->mc_alloc_throttle_enabled &&
-		    (zio->io_flags & ZIO_FLAG_IO_ALLOCATING)) {
+			(zio->io_flags & ZIO_FLAG_IO_ALLOCATING))
+		{
 			metaslab_class_throttle_unreserve(mc,
-			    zio->io_prop.zp_copies, zio->io_allocator, zio);
+											  zio->io_prop.zp_copies, zio->io_allocator, zio);
 			zio->io_flags &= ~ZIO_FLAG_IO_ALLOCATING;
 
 			VERIFY(metaslab_class_throttle_reserve(
-			    spa_normal_class(spa),
-			    zio->io_prop.zp_copies, zio->io_allocator, zio,
-			    flags | METASLAB_MUST_RESERVE));
+				spa_normal_class(spa),
+				zio->io_prop.zp_copies, zio->io_allocator, zio,
+				flags | METASLAB_MUST_RESERVE));
 		}
 		zio->io_metaslab_class = mc = spa_normal_class(spa);
-		if (zfs_flags & ZFS_DEBUG_METASLAB_ALLOC) {
+		if (zfs_flags & ZFS_DEBUG_METASLAB_ALLOC)
+		{
 			zfs_dbgmsg("%s: metaslab allocation failure, "
-			    "trying normal class: zio %px, size %llu, error %d",
-			    spa_name(spa), zio, (u_longlong_t)zio->io_size,
-			    error);
+					   "trying normal class: zio %px, size %llu, error %d",
+					   spa_name(spa), zio, (u_longlong_t)zio->io_size,
+					   error);
 		}
 
 		error = metaslab_alloc(spa, mc, zio->io_size, bp,
-		    zio->io_prop.zp_copies, zio->io_txg, NULL, flags,
-		    &zio->io_alloc_list, zio, zio->io_allocator);
+							   zio->io_prop.zp_copies, zio->io_txg, NULL, flags,
+							   &zio->io_alloc_list, zio, zio->io_allocator);
 	}
 
-	if (error == ENOSPC && zio->io_size > SPA_MINBLOCKSIZE) {
-		if (zfs_flags & ZFS_DEBUG_METASLAB_ALLOC) {
+	if (error == ENOSPC && zio->io_size > SPA_MINBLOCKSIZE)
+	{
+		if (zfs_flags & ZFS_DEBUG_METASLAB_ALLOC)
+		{
 			zfs_dbgmsg("%s: metaslab allocation failure, "
-			    "trying ganging: zio %px, size %llu, error %d",
-			    spa_name(spa), zio, (u_longlong_t)zio->io_size,
-			    error);
+					   "trying ganging: zio %px, size %llu, error %d",
+					   spa_name(spa), zio, (u_longlong_t)zio->io_size,
+					   error);
 		}
 		return (zio_write_gang_block(zio, mc));
 	}
-	if (error != 0) {
+	if (error != 0)
+	{
 		if (error != ENOSPC ||
-		    (zfs_flags & ZFS_DEBUG_METASLAB_ALLOC)) {
+			(zfs_flags & ZFS_DEBUG_METASLAB_ALLOC))
+		{
 			zfs_dbgmsg("%s: metaslab allocation failure: zio %px, "
-			    "size %llu, error %d",
-			    spa_name(spa), zio, (u_longlong_t)zio->io_size,
-			    error);
+					   "size %llu, error %d",
+					   spa_name(spa), zio, (u_longlong_t)zio->io_size,
+					   error);
 		}
 		zio->io_error = error;
 	}
@@ -3628,10 +3818,12 @@ zio_dva_unallocate(zio_t *zio, zio_gang_node_t *gn, blkptr_t *bp)
 	if (!BP_IS_HOLE(bp))
 		metaslab_free(zio->io_spa, bp, bp->blk_birth, B_TRUE);
 
-	if (gn != NULL) {
-		for (int g = 0; g < SPA_GBH_NBLKPTRS; g++) {
+	if (gn != NULL)
+	{
+		for (int g = 0; g < SPA_GBH_NBLKPTRS; g++)
+		{
 			zio_dva_unallocate(zio, gn->gn_child[g],
-			    &gn->gn_gbh->zg_blkptr[g]);
+							   &gn->gn_gbh->zg_blkptr[g]);
 		}
 	}
 }
@@ -3639,9 +3831,8 @@ zio_dva_unallocate(zio_t *zio, zio_gang_node_t *gn, blkptr_t *bp)
 /*
  * Try to allocate an intent log block.  Return 0 on success, errno on failure.
  */
-int
-zio_alloc_zil(spa_t *spa, objset_t *os, uint64_t txg, blkptr_t *new_bp,
-    uint64_t size, boolean_t *slog)
+int zio_alloc_zil(spa_t *spa, objset_t *os, uint64_t txg, blkptr_t *new_bp,
+				  uint64_t size, boolean_t *slog)
 {
 	int error = 1;
 	zio_alloc_list_t io_alloc_list;
@@ -3666,29 +3857,34 @@ zio_alloc_zil(spa_t *spa, objset_t *os, uint64_t txg, blkptr_t *new_bp,
 	 */
 	int flags = METASLAB_FASTWRITE | METASLAB_ZIL;
 	int allocator = (uint_t)cityhash4(0, 0, 0,
-	    os->os_dsl_dataset->ds_object) % spa->spa_alloc_count;
+									  os->os_dsl_dataset->ds_object) %
+					spa->spa_alloc_count;
 	error = metaslab_alloc(spa, spa_log_class(spa), size, new_bp, 1,
-	    txg, NULL, flags, &io_alloc_list, NULL, allocator);
+						   txg, NULL, flags, &io_alloc_list, NULL, allocator);
 	*slog = (error == 0);
-	if (error != 0) {
+	if (error != 0)
+	{
 		error = metaslab_alloc(spa, spa_embedded_log_class(spa), size,
-		    new_bp, 1, txg, NULL, flags,
-		    &io_alloc_list, NULL, allocator);
+							   new_bp, 1, txg, NULL, flags,
+							   &io_alloc_list, NULL, allocator);
 	}
-	if (error != 0) {
+	if (error != 0)
+	{
 		error = metaslab_alloc(spa, spa_normal_class(spa), size,
-		    new_bp, 1, txg, NULL, flags,
-		    &io_alloc_list, NULL, allocator);
+							   new_bp, 1, txg, NULL, flags,
+							   &io_alloc_list, NULL, allocator);
 	}
 	metaslab_trace_fini(&io_alloc_list);
 
-	if (error == 0) {
+	if (error == 0)
+	{
 		BP_SET_LSIZE(new_bp, size);
 		BP_SET_PSIZE(new_bp, size);
 		BP_SET_COMPRESS(new_bp, ZIO_COMPRESS_OFF);
 		BP_SET_CHECKSUM(new_bp,
-		    spa_version(spa) >= SPA_VERSION_SLIM_ZIL
-		    ? ZIO_CHECKSUM_ZILOG2 : ZIO_CHECKSUM_ZILOG);
+						spa_version(spa) >= SPA_VERSION_SLIM_ZIL
+							? ZIO_CHECKSUM_ZILOG2
+							: ZIO_CHECKSUM_ZILOG);
 		BP_SET_TYPE(new_bp, DMU_OT_INTENT_LOG);
 		BP_SET_LEVEL(new_bp, 0);
 		BP_SET_DEDUP(new_bp, 0);
@@ -3699,21 +3895,25 @@ zio_alloc_zil(spa_t *spa, objset_t *os, uint64_t txg, blkptr_t *new_bp,
 		 * these now since we will not be rewriting the bp at
 		 * rewrite time.
 		 */
-		if (os->os_encrypted) {
+		if (os->os_encrypted)
+		{
 			uint8_t iv[ZIO_DATA_IV_LEN];
 			uint8_t salt[ZIO_DATA_SALT_LEN];
 
 			BP_SET_CRYPT(new_bp, B_TRUE);
 			VERIFY0(spa_crypt_get_salt(spa,
-			    dmu_objset_id(os), salt));
+									   dmu_objset_id(os), salt));
 			VERIFY0(zio_crypt_generate_iv(iv));
 
 			zio_crypt_encode_params_bp(new_bp, salt, iv);
 		}
-	} else {
+	}
+	else
+	{
 		zfs_dbgmsg("%s: zil block allocation failure: "
-		    "size %llu, error %d", spa_name(spa), (u_longlong_t)size,
-		    error);
+				   "size %llu, error %d",
+				   spa_name(spa), (u_longlong_t)size,
+				   error);
 	}
 
 	return (error);
@@ -3738,6 +3938,7 @@ zio_alloc_zil(spa_t *spa, objset_t *os, uint64_t txg, blkptr_t *new_bp,
 static zio_t *
 zio_vdev_io_start(zio_t *zio)
 {
+	zfs_dbgmsg("zio_vdev_io_start called for type %d", zio->io_type);
 	vdev_t *vd = zio->io_vd;
 	uint64_t align;
 	spa_t *spa = zio->io_spa;
@@ -3747,7 +3948,9 @@ zio_vdev_io_start(zio_t *zio)
 	ASSERT(zio->io_error == 0);
 	ASSERT(zio->io_child_error[ZIO_CHILD_VDEV] == 0);
 
-	if (vd == NULL) {
+	if (vd == NULL)
+	{
+		zfs_dbgmsg("vd is null!");
 		if (!(zio->io_flags & ZIO_FLAG_CONFIG_WRITER))
 			spa_config_enter(spa, SCL_ZIO, zio, RW_READER);
 
@@ -3759,32 +3962,39 @@ zio_vdev_io_start(zio_t *zio)
 	}
 
 	ASSERT3P(zio->io_logical, !=, zio);
-	if (zio->io_type == ZIO_TYPE_WRITE) {
+	if (zio->io_type == ZIO_TYPE_WRITE)
+	{
 		ASSERT(spa->spa_trust_config);
 
 		/*
 		 * Note: the code can handle other kinds of writes,
 		 * but we don't expect them.
 		 */
-		if (zio->io_vd->vdev_removing) {
+		if (zio->io_vd->vdev_removing)
+		{
 			ASSERT(zio->io_flags &
-			    (ZIO_FLAG_PHYSICAL | ZIO_FLAG_SELF_HEAL |
-			    ZIO_FLAG_RESILVER | ZIO_FLAG_INDUCE_DAMAGE));
+				   (ZIO_FLAG_PHYSICAL | ZIO_FLAG_SELF_HEAL |
+					ZIO_FLAG_RESILVER | ZIO_FLAG_INDUCE_DAMAGE));
 		}
 	}
 
 	align = 1ULL << vd->vdev_top->vdev_ashift;
 
 	if (!(zio->io_flags & ZIO_FLAG_PHYSICAL) &&
-	    P2PHASE(zio->io_size, align) != 0) {
+		P2PHASE(zio->io_size, align) != 0)
+	{
 		/* Transform logical writes to be a full physical block size. */
 		uint64_t asize = P2ROUNDUP(zio->io_size, align);
 		abd_t *abuf = abd_alloc_sametype(zio->io_abd, asize);
 		ASSERT(vd == vd->vdev_top);
-		if (zio->io_type == ZIO_TYPE_WRITE) {
+
+		if (zio->io_type == ZIO_TYPE_WRITE || zio->io_type == ZIO_TYPE_MLEC_WRITE_DATA)
+		{
+			zfs_dbgmsg("Copying abd content in vdev_io_start to abuf, content %s", (char *) abd_to_buf(zio->io_abd));
 			abd_copy(abuf, zio->io_abd, zio->io_size);
 			abd_zero_off(abuf, zio->io_size, asize - zio->io_size);
 		}
+
 		zio_push_transform(zio, abuf, asize, asize, zio_subblock);
 	}
 
@@ -3792,10 +4002,13 @@ zio_vdev_io_start(zio_t *zio)
 	 * If this is not a physical io, make sure that it is properly aligned
 	 * before proceeding.
 	 */
-	if (!(zio->io_flags & ZIO_FLAG_PHYSICAL)) {
+	if (!(zio->io_flags & ZIO_FLAG_PHYSICAL))
+	{
 		ASSERT0(P2PHASE(zio->io_offset, align));
 		ASSERT0(P2PHASE(zio->io_size, align));
-	} else {
+	}
+	else
+	{
 		/*
 		 * For physical writes, we allow 512b aligned writes and assume
 		 * the device will perform a read-modify-write as necessary.
@@ -3847,34 +4060,43 @@ zio_vdev_io_start(zio_t *zio)
 	 * for correctness.
 	 */
 	if ((zio->io_flags & ZIO_FLAG_IO_REPAIR) &&
-	    !(zio->io_flags & ZIO_FLAG_SELF_HEAL) &&
-	    zio->io_txg != 0 &&	/* not a delegated i/o */
-	    vd->vdev_ops != &vdev_indirect_ops &&
-	    vd->vdev_top->vdev_ops != &vdev_draid_ops &&
-	    !vdev_dtl_contains(vd, DTL_PARTIAL, zio->io_txg, 1)) {
+		!(zio->io_flags & ZIO_FLAG_SELF_HEAL) &&
+		zio->io_txg != 0 && /* not a delegated i/o */
+		vd->vdev_ops != &vdev_indirect_ops &&
+		vd->vdev_top->vdev_ops != &vdev_draid_ops &&
+		!vdev_dtl_contains(vd, DTL_PARTIAL, zio->io_txg, 1))
+	{
+		zfs_dbgmsg("io bypass");
 		ASSERT(zio->io_type == ZIO_TYPE_WRITE);
 		zio_vdev_io_bypass(zio);
 		return (zio);
 	}
 
+
+	zfs_dbgmsg("zio_vdev_io_start type is %d", zio->io_type);
 	/*
 	 * Select the next best leaf I/O to process.  Distributed spares are
 	 * excluded since they dispatch the I/O directly to a leaf vdev after
 	 * applying the dRAID mapping.
 	 */
 	if (vd->vdev_ops->vdev_op_leaf &&
-	    vd->vdev_ops != &vdev_draid_spare_ops &&
-	    (zio->io_type == ZIO_TYPE_READ ||
-	    zio->io_type == ZIO_TYPE_WRITE ||
-	    zio->io_type == ZIO_TYPE_TRIM)) {
+		vd->vdev_ops != &vdev_draid_spare_ops &&
+		(zio->io_type == ZIO_TYPE_READ ||
+		 zio->io_type == ZIO_TYPE_WRITE ||
+		 zio->io_type == ZIO_TYPE_TRIM ||
+		 zio->io_type == ZIO_TYPE_MLEC_WRITE_DATA))
+	{
+		zfs_dbgmsg("vdev leave ops");
 
 		if (zio->io_type == ZIO_TYPE_READ && vdev_cache_read(zio))
 			return (zio);
 
+		zfs_dbgmsg("vdev ops is write");
 		if ((zio = vdev_queue_io(zio)) == NULL)
 			return (NULL);
 
-		if (!vdev_accessible(vd, zio)) {
+		if (!vdev_accessible(vd, zio))
+		{
 			zio->io_error = SET_ERROR(ENXIO);
 			zio_interrupt(zio);
 			return (NULL);
@@ -3882,7 +4104,11 @@ zio_vdev_io_start(zio_t *zio)
 		zio->io_delay = gethrtime();
 	}
 
+	zfs_dbgmsg("Calling vdev_op_io_start, vd ops type %s", vd->vdev_ops->vdev_op_type);
+	zfs_dbgmsg("%p", vd->vdev_ops->vdev_op_io_start);
 	vd->vdev_ops->vdev_op_io_start(zio);
+	zfs_dbgmsg("-------");
+	
 	return (NULL);
 }
 
@@ -3893,18 +4119,20 @@ zio_vdev_io_done(zio_t *zio)
 	vdev_ops_t *ops = vd ? vd->vdev_ops : &vdev_mirror_ops;
 	boolean_t unexpected_error = B_FALSE;
 
-	if (zio_wait_for_children(zio, ZIO_CHILD_VDEV_BIT, ZIO_WAIT_DONE)) {
+	if (zio_wait_for_children(zio, ZIO_CHILD_VDEV_BIT, ZIO_WAIT_DONE))
+	{
 		return (NULL);
 	}
 
 	ASSERT(zio->io_type == ZIO_TYPE_READ ||
-	    zio->io_type == ZIO_TYPE_WRITE || zio->io_type == ZIO_TYPE_TRIM);
+		   zio->io_type == ZIO_TYPE_WRITE || zio->io_type == ZIO_TYPE_TRIM || zio->io_type == ZIO_TYPE_MLEC_WRITE_DATA);
 
 	if (zio->io_delay)
 		zio->io_delay = gethrtime() - zio->io_delay;
 
 	if (vd != NULL && vd->vdev_ops->vdev_op_leaf &&
-	    vd->vdev_ops != &vdev_draid_spare_ops) {
+		vd->vdev_ops != &vdev_draid_spare_ops)
+	{
 		vdev_queue_io_done(zio);
 
 		if (zio->io_type == ZIO_TYPE_WRITE)
@@ -3912,15 +4140,19 @@ zio_vdev_io_done(zio_t *zio)
 
 		if (zio_injection_enabled && zio->io_error == 0)
 			zio->io_error = zio_handle_device_injections(vd, zio,
-			    EIO, EILSEQ);
+														 EIO, EILSEQ);
 
 		if (zio_injection_enabled && zio->io_error == 0)
 			zio->io_error = zio_handle_label_injection(zio, EIO);
 
-		if (zio->io_error && zio->io_type != ZIO_TYPE_TRIM) {
-			if (!vdev_accessible(vd, zio)) {
+		if (zio->io_error && zio->io_type != ZIO_TYPE_TRIM)
+		{
+			if (!vdev_accessible(vd, zio))
+			{
 				zio->io_error = SET_ERROR(ENXIO);
-			} else {
+			}
+			else
+			{
 				unexpected_error = B_TRUE;
 			}
 		}
@@ -3941,22 +4173,25 @@ zio_vdev_io_done(zio_t *zio)
  * as a scrub or async read IO. Otherwise, the high priority read request
  * would end up having to wait for the lower priority IO.
  */
-void
-zio_change_priority(zio_t *pio, zio_priority_t priority)
+void zio_change_priority(zio_t *pio, zio_priority_t priority)
 {
 	zio_t *cio, *cio_next;
 	zio_link_t *zl = NULL;
 
 	ASSERT3U(priority, <, ZIO_PRIORITY_NUM_QUEUEABLE);
 
-	if (pio->io_vd != NULL && pio->io_vd->vdev_ops->vdev_op_leaf) {
+	if (pio->io_vd != NULL && pio->io_vd->vdev_ops->vdev_op_leaf)
+	{
 		vdev_queue_change_io_priority(pio, priority);
-	} else {
+	}
+	else
+	{
 		pio->io_priority = priority;
 	}
 
 	mutex_enter(&pio->io_lock);
-	for (cio = zio_walk_children(pio, &zl); cio != NULL; cio = cio_next) {
+	for (cio = zio_walk_children(pio, &zl); cio != NULL; cio = cio_next)
+	{
 		cio_next = zio_walk_children(pio, &zl);
 		zio_change_priority(cio, priority);
 	}
@@ -3969,14 +4204,13 @@ zio_change_priority(zio_t *pio, zio_priority_t priority)
  */
 static void
 zio_vsd_default_cksum_finish(zio_cksum_report_t *zcr,
-    const abd_t *good_buf)
+							 const abd_t *good_buf)
 {
 	/* no processing needed */
 	zfs_ereport_finish_checksum(zcr, good_buf, zcr->zcr_cbdata, B_FALSE);
 }
 
-void
-zio_vsd_default_cksum_report(zio_t *zio, zio_cksum_report_t *zcr)
+void zio_vsd_default_cksum_report(zio_t *zio, zio_cksum_report_t *zcr)
 {
 	void *abd = abd_alloc_sametype(zio->io_abd, zio->io_size);
 
@@ -3993,14 +4227,16 @@ zio_vdev_io_assess(zio_t *zio)
 {
 	vdev_t *vd = zio->io_vd;
 
-	if (zio_wait_for_children(zio, ZIO_CHILD_VDEV_BIT, ZIO_WAIT_DONE)) {
+	if (zio_wait_for_children(zio, ZIO_CHILD_VDEV_BIT, ZIO_WAIT_DONE))
+	{
 		return (NULL);
 	}
 
 	if (vd == NULL && !(zio->io_flags & ZIO_FLAG_CONFIG_WRITER))
 		spa_config_exit(zio->io_spa, SCL_ZIO, zio);
 
-	if (zio->io_vsd != NULL) {
+	if (zio->io_vsd != NULL)
+	{
 		zio->io_vsd_ops->vsd_free(zio);
 		zio->io_vsd = NULL;
 	}
@@ -4015,15 +4251,16 @@ zio_vdev_io_assess(zio_t *zio)
 	 * compression/checksumming/etc. work to prevent our (cheap) IO reissue.
 	 */
 	if (zio->io_error && vd == NULL &&
-	    !(zio->io_flags & (ZIO_FLAG_DONT_RETRY | ZIO_FLAG_IO_RETRY))) {
-		ASSERT(!(zio->io_flags & ZIO_FLAG_DONT_QUEUE));	/* not a leaf */
+		!(zio->io_flags & (ZIO_FLAG_DONT_RETRY | ZIO_FLAG_IO_RETRY)))
+	{
+		ASSERT(!(zio->io_flags & ZIO_FLAG_DONT_QUEUE)); /* not a leaf */
 		ASSERT(!(zio->io_flags & ZIO_FLAG_IO_BYPASS));	/* not a leaf */
 		zio->io_error = 0;
 		zio->io_flags |= ZIO_FLAG_IO_RETRY |
-		    ZIO_FLAG_DONT_CACHE | ZIO_FLAG_DONT_AGGREGATE;
+						 ZIO_FLAG_DONT_CACHE | ZIO_FLAG_DONT_AGGREGATE;
 		zio->io_stage = ZIO_STAGE_VDEV_IO_START >> 1;
 		zio_taskq_dispatch(zio, ZIO_TASKQ_ISSUE,
-		    zio_requeue_io_start_cut_in_line);
+						   zio_requeue_io_start_cut_in_line);
 		return (NULL);
 	}
 
@@ -4032,7 +4269,7 @@ zio_vdev_io_assess(zio_t *zio)
 	 * if the device is not accessible at all.
 	 */
 	if (zio->io_error && vd != NULL && vd->vdev_ops->vdev_op_leaf &&
-	    !vdev_accessible(vd, zio))
+		!vdev_accessible(vd, zio))
 		zio->io_error = SET_ERROR(ENXIO);
 
 	/*
@@ -4040,10 +4277,11 @@ zio_vdev_io_assess(zio_t *zio)
 	 * set vdev_cant_write so that we stop trying to allocate from it.
 	 */
 	if (zio->io_error == ENXIO && zio->io_type == ZIO_TYPE_WRITE &&
-	    vd != NULL && !vd->vdev_ops->vdev_op_leaf) {
+		vd != NULL && !vd->vdev_ops->vdev_op_leaf)
+	{
 		vdev_dbgmsg(vd, "zio_vdev_io_assess(zio=%px) setting "
-		    "cant_write=TRUE due to write failure with ENXIO",
-		    zio);
+						"cant_write=TRUE due to write failure with ENXIO",
+					zio);
 		vd->vdev_cant_write = B_TRUE;
 	}
 
@@ -4053,15 +4291,16 @@ zio_vdev_io_assess(zio_t *zio)
 	 * boolean flag so that we don't bother with it in the future.
 	 */
 	if ((zio->io_error == ENOTSUP || zio->io_error == ENOTTY) &&
-	    zio->io_type == ZIO_TYPE_IOCTL &&
-	    zio->io_cmd == DKIOCFLUSHWRITECACHE && vd != NULL)
+		zio->io_type == ZIO_TYPE_IOCTL &&
+		zio->io_cmd == DKIOCFLUSHWRITECACHE && vd != NULL)
 		vd->vdev_nowritecache = B_TRUE;
 
 	if (zio->io_error)
 		zio->io_pipeline = ZIO_INTERLOCK_PIPELINE;
 
 	if (vd != NULL && vd->vdev_ops->vdev_op_leaf &&
-	    zio->io_physdone != NULL) {
+		zio->io_physdone != NULL)
+	{
 		ASSERT(!(zio->io_flags & ZIO_FLAG_DELEGATED));
 		ASSERT(zio->io_child_type == ZIO_CHILD_VDEV);
 		zio->io_physdone(zio->io_logical);
@@ -4070,8 +4309,7 @@ zio_vdev_io_assess(zio_t *zio)
 	return (zio);
 }
 
-void
-zio_vdev_io_reissue(zio_t *zio)
+void zio_vdev_io_reissue(zio_t *zio)
 {
 	ASSERT(zio->io_stage == ZIO_STAGE_VDEV_IO_START);
 	ASSERT(zio->io_error == 0);
@@ -4079,16 +4317,14 @@ zio_vdev_io_reissue(zio_t *zio)
 	zio->io_stage >>= 1;
 }
 
-void
-zio_vdev_io_redone(zio_t *zio)
+void zio_vdev_io_redone(zio_t *zio)
 {
 	ASSERT(zio->io_stage == ZIO_STAGE_VDEV_IO_DONE);
 
 	zio->io_stage >>= 1;
 }
 
-void
-zio_vdev_io_bypass(zio_t *zio)
+void zio_vdev_io_bypass(zio_t *zio)
 {
 	ASSERT(zio->io_stage == ZIO_STAGE_VDEV_IO_START);
 	ASSERT(zio->io_error == 0);
@@ -4103,7 +4339,6 @@ zio_vdev_io_bypass(zio_t *zio)
  * ==========================================================================
  */
 
-
 /*
  * This function is used for ZIO_STAGE_ENCRYPT. It is responsible for
  * managing the storage of encryption parameters and passing them to the
@@ -4133,13 +4368,15 @@ zio_encrypt(zio_t *zio)
 	if (!IO_IS_ALLOCATING(zio) && ot != DMU_OT_INTENT_LOG)
 		return (zio);
 
-	if (!(zp->zp_encrypt || BP_IS_ENCRYPTED(bp))) {
+	if (!(zp->zp_encrypt || BP_IS_ENCRYPTED(bp)))
+	{
 		BP_SET_CRYPT(bp, B_FALSE);
 		return (zio);
 	}
 
 	/* if we are doing raw encryption set the provided encryption params */
-	if (zio->io_flags & ZIO_FLAG_RAW_ENCRYPT) {
+	if (zio->io_flags & ZIO_FLAG_RAW_ENCRYPT)
+	{
 		ASSERT0(BP_GET_LEVEL(bp));
 		BP_SET_CRYPT(bp, B_TRUE);
 		BP_SET_BYTEORDER(bp, zp->zp_byteorder);
@@ -4148,14 +4385,15 @@ zio_encrypt(zio_t *zio)
 
 		/* dnode blocks must be written out in the provided byteorder */
 		if (zp->zp_byteorder != ZFS_HOST_BYTEORDER &&
-		    ot == DMU_OT_DNODE) {
+			ot == DMU_OT_DNODE)
+		{
 			void *bswap_buf = zio_buf_alloc(psize);
 			abd_t *babd = abd_get_from_buf(bswap_buf, psize);
 
 			ASSERT3U(BP_GET_COMPRESS(bp), ==, ZIO_COMPRESS_OFF);
 			abd_copy_to_buf(bswap_buf, zio->io_abd, psize);
 			dmu_ot_byteswap[DMU_OT_BYTESWAP(ot)].ob_func(bswap_buf,
-			    psize);
+														 psize);
 
 			abd_take_ownership_of_buf(babd, B_TRUE);
 			zio_push_transform(zio, babd, psize, psize, NULL);
@@ -4167,11 +4405,12 @@ zio_encrypt(zio_t *zio)
 	}
 
 	/* indirect blocks only maintain a cksum of the lower level MACs */
-	if (BP_GET_LEVEL(bp) > 0) {
+	if (BP_GET_LEVEL(bp) > 0)
+	{
 		BP_SET_CRYPT(bp, B_TRUE);
 		VERIFY0(zio_crypt_do_indirect_mac_checksum_abd(B_TRUE,
-		    zio->io_orig_abd, BP_GET_LSIZE(bp), BP_SHOULD_BYTESWAP(bp),
-		    mac));
+													   zio->io_orig_abd, BP_GET_LSIZE(bp), BP_SHOULD_BYTESWAP(bp),
+													   mac));
 		zio_crypt_encode_mac_bp(bp, mac);
 		return (zio);
 	}
@@ -4180,20 +4419,22 @@ zio_encrypt(zio_t *zio)
 	 * Objset blocks are a special case since they have 2 256-bit MACs
 	 * embedded within them.
 	 */
-	if (ot == DMU_OT_OBJSET) {
+	if (ot == DMU_OT_OBJSET)
+	{
 		ASSERT0(DMU_OT_IS_ENCRYPTED(ot));
 		ASSERT3U(BP_GET_COMPRESS(bp), ==, ZIO_COMPRESS_OFF);
 		BP_SET_CRYPT(bp, B_TRUE);
 		VERIFY0(spa_do_crypt_objset_mac_abd(B_TRUE, spa, dsobj,
-		    zio->io_abd, psize, BP_SHOULD_BYTESWAP(bp)));
+											zio->io_abd, psize, BP_SHOULD_BYTESWAP(bp)));
 		return (zio);
 	}
 
 	/* unencrypted object types are only authenticated with a MAC */
-	if (!DMU_OT_IS_ENCRYPTED(ot)) {
+	if (!DMU_OT_IS_ENCRYPTED(ot))
+	{
 		BP_SET_CRYPT(bp, B_TRUE);
 		VERIFY0(spa_do_crypt_mac_abd(B_TRUE, spa, dsobj,
-		    zio->io_abd, psize, mac));
+									 zio->io_abd, psize, mac));
 		zio_crypt_encode_mac_bp(bp, mac);
 		return (zio);
 	}
@@ -4220,34 +4461,43 @@ zio_encrypt(zio_t *zio)
 	 * For an explanation of what encryption parameters are stored
 	 * where, see the block comment in zio_crypt.c.
 	 */
-	if (ot == DMU_OT_INTENT_LOG) {
+	if (ot == DMU_OT_INTENT_LOG)
+	{
 		zio_crypt_decode_params_bp(bp, salt, iv);
-	} else {
+	}
+	else
+	{
 		BP_SET_CRYPT(bp, B_TRUE);
 	}
 
 	/* Perform the encryption. This should not fail */
 	VERIFY0(spa_do_crypt_abd(B_TRUE, spa, &zio->io_bookmark,
-	    BP_GET_TYPE(bp), BP_GET_DEDUP(bp), BP_SHOULD_BYTESWAP(bp),
-	    salt, iv, mac, psize, zio->io_abd, eabd, &no_crypt));
+							 BP_GET_TYPE(bp), BP_GET_DEDUP(bp), BP_SHOULD_BYTESWAP(bp),
+							 salt, iv, mac, psize, zio->io_abd, eabd, &no_crypt));
 
 	/* encode encryption metadata into the bp */
-	if (ot == DMU_OT_INTENT_LOG) {
+	if (ot == DMU_OT_INTENT_LOG)
+	{
 		/*
 		 * ZIL blocks store the MAC in the embedded checksum, so the
 		 * transform must always be applied.
 		 */
 		zio_crypt_encode_mac_zil(enc_buf, mac);
 		zio_push_transform(zio, eabd, psize, psize, NULL);
-	} else {
+	}
+	else
+	{
 		BP_SET_CRYPT(bp, B_TRUE);
 		zio_crypt_encode_params_bp(bp, salt, iv);
 		zio_crypt_encode_mac_bp(bp, mac);
 
-		if (no_crypt) {
+		if (no_crypt)
+		{
 			ASSERT3U(ot, ==, DMU_OT_DNODE);
 			abd_free(eabd);
-		} else {
+		}
+		else
+		{
 			zio_push_transform(zio, eabd, psize, psize, NULL);
 		}
 	}
@@ -4266,7 +4516,8 @@ zio_checksum_generate(zio_t *zio)
 	blkptr_t *bp = zio->io_bp;
 	enum zio_checksum checksum;
 
-	if (bp == NULL) {
+	if (bp == NULL)
+	{
 		/*
 		 * This is zio_write_phys().
 		 * We're either generating a label checksum, or none at all.
@@ -4277,11 +4528,16 @@ zio_checksum_generate(zio_t *zio)
 			return (zio);
 
 		ASSERT(checksum == ZIO_CHECKSUM_LABEL);
-	} else {
-		if (BP_IS_GANG(bp) && zio->io_child_type == ZIO_CHILD_GANG) {
+	}
+	else
+	{
+		if (BP_IS_GANG(bp) && zio->io_child_type == ZIO_CHILD_GANG)
+		{
 			ASSERT(!IO_IS_ALLOCATING(zio));
 			checksum = ZIO_CHECKSUM_GANG_HEADER;
-		} else {
+		}
+		else
+		{
 			checksum = BP_GET_CHECKSUM(bp);
 		}
 	}
@@ -4294,13 +4550,15 @@ zio_checksum_generate(zio_t *zio)
 static zio_t *
 zio_checksum_verify(zio_t *zio)
 {
+	zfs_dbgmsg("zio_checksum_verify() called");
 	zio_bad_cksum_t info;
 	blkptr_t *bp = zio->io_bp;
 	int error;
 
 	ASSERT(zio->io_vd != NULL);
 
-	if (bp == NULL) {
+	if (bp == NULL)
+	{
 		/*
 		 * This is zio_read_phys().
 		 * We're either verifying a label checksum, or nothing at all.
@@ -4311,13 +4569,15 @@ zio_checksum_verify(zio_t *zio)
 		ASSERT3U(zio->io_prop.zp_checksum, ==, ZIO_CHECKSUM_LABEL);
 	}
 
-	if ((error = zio_checksum_error(zio, &info)) != 0) {
+	if ((error = zio_checksum_error(zio, &info)) != 0)
+	{
 		zio->io_error = error;
 		if (error == ECKSUM &&
-		    !(zio->io_flags & ZIO_FLAG_SPECULATIVE)) {
-			(void) zfs_ereport_start_checksum(zio->io_spa,
-			    zio->io_vd, &zio->io_bookmark, zio,
-			    zio->io_offset, zio->io_size, &info);
+			!(zio->io_flags & ZIO_FLAG_SPECULATIVE))
+		{
+			(void)zfs_ereport_start_checksum(zio->io_spa,
+											 zio->io_vd, &zio->io_bookmark, zio,
+											 zio->io_offset, zio->io_size, &info);
 			mutex_enter(&zio->io_vd->vdev_stat_lock);
 			zio->io_vd->vdev_stat.vs_checksum_errors++;
 			mutex_exit(&zio->io_vd->vdev_stat_lock);
@@ -4330,8 +4590,7 @@ zio_checksum_verify(zio_t *zio)
 /*
  * Called by RAID-Z to ensure we don't compute the checksum twice.
  */
-void
-zio_checksum_verified(zio_t *zio)
+void zio_checksum_verified(zio_t *zio)
 {
 	zio->io_pipeline &= ~ZIO_STAGE_CHECKSUM_VERIFY;
 }
@@ -4345,17 +4604,16 @@ zio_checksum_verified(zio_t *zio)
  * Any other error is presumed to be worse because we weren't expecting it.
  * ==========================================================================
  */
-int
-zio_worst_error(int e1, int e2)
+int zio_worst_error(int e1, int e2)
 {
-	static int zio_error_rank[] = { 0, ENXIO, ECKSUM, EIO };
+	static int zio_error_rank[] = {0, ENXIO, ECKSUM, EIO};
 	int r1, r2;
 
-	for (r1 = 0; r1 < sizeof (zio_error_rank) / sizeof (int); r1++)
+	for (r1 = 0; r1 < sizeof(zio_error_rank) / sizeof(int); r1++)
 		if (e1 == zio_error_rank[r1])
 			break;
 
-	for (r2 = 0; r2 < sizeof (zio_error_rank) / sizeof (int); r2++)
+	for (r2 = 0; r2 < sizeof(zio_error_rank) / sizeof(int); r2++)
 		if (e2 == zio_error_rank[r2])
 			break;
 
@@ -4370,19 +4628,22 @@ zio_worst_error(int e1, int e2)
 static zio_t *
 zio_ready(zio_t *zio)
 {
+	// zfs_dbgmsg("zio_ready called");
 	blkptr_t *bp = zio->io_bp;
 	zio_t *pio, *pio_next;
 	zio_link_t *zl = NULL;
 
 	if (zio_wait_for_children(zio, ZIO_CHILD_GANG_BIT | ZIO_CHILD_DDT_BIT,
-	    ZIO_WAIT_READY)) {
+							  ZIO_WAIT_READY))
+	{
 		return (NULL);
 	}
 
-	if (zio->io_ready) {
+	if (zio->io_ready)
+	{
 		ASSERT(IO_IS_ALLOCATING(zio));
 		ASSERT(bp->blk_birth == zio->io_txg || BP_IS_HOLE(bp) ||
-		    (zio->io_flags & ZIO_FLAG_NOPWRITE));
+			   (zio->io_flags & ZIO_FLAG_NOPWRITE));
 		ASSERT(zio->io_children[ZIO_CHILD_GANG][ZIO_WAIT_READY] == 0);
 
 		zio->io_ready(zio);
@@ -4391,10 +4652,12 @@ zio_ready(zio_t *zio)
 	if (bp != NULL && bp != &zio->io_bp_copy)
 		zio->io_bp_copy = *bp;
 
-	if (zio->io_error != 0) {
+	if (zio->io_error != 0)
+	{
 		zio->io_pipeline = ZIO_INTERLOCK_PIPELINE;
 
-		if (zio->io_flags & ZIO_FLAG_IO_ALLOCATING) {
+		if (zio->io_flags & ZIO_FLAG_IO_ALLOCATING)
+		{
 			ASSERT(IO_IS_ALLOCATING(zio));
 			ASSERT(zio->io_priority == ZIO_PRIORITY_ASYNC_WRITE);
 			ASSERT(zio->io_metaslab_class != NULL);
@@ -4404,8 +4667,8 @@ zio_ready(zio_t *zio)
 			 * issue the next I/O to allocate.
 			 */
 			metaslab_class_throttle_unreserve(
-			    zio->io_metaslab_class, zio->io_prop.zp_copies,
-			    zio->io_allocator, zio);
+				zio->io_metaslab_class, zio->io_prop.zp_copies,
+				zio->io_allocator, zio);
 			zio_allocate_dispatch(zio->io_spa, zio->io_allocator);
 		}
 	}
@@ -4422,22 +4685,27 @@ zio_ready(zio_t *zio)
 	 * io_parent_list, from 'pio_next' onward, cannot change because
 	 * all parents must wait for us to be done before they can be done.
 	 */
-	for (; pio != NULL; pio = pio_next) {
+	for (; pio != NULL; pio = pio_next)
+	{
 		pio_next = zio_walk_parents(zio, &zl);
 		zio_notify_parent(pio, zio, ZIO_WAIT_READY, NULL);
 	}
 
-	if (zio->io_flags & ZIO_FLAG_NODATA) {
-		if (BP_IS_GANG(bp)) {
+	if (zio->io_flags & ZIO_FLAG_NODATA)
+	{
+		if (BP_IS_GANG(bp))
+		{
 			zio->io_flags &= ~ZIO_FLAG_NODATA;
-		} else {
+		}
+		else
+		{
 			ASSERT((uintptr_t)zio->io_abd < SPA_MAXBLOCKSIZE);
 			zio->io_pipeline &= ~ZIO_VDEV_IO_STAGES;
 		}
 	}
 
 	if (zio_injection_enabled &&
-	    zio->io_spa->spa_syncing_txg == zio->io_txg)
+		zio->io_spa->spa_syncing_txg == zio->io_txg)
 		zio_handle_ignored_writes(zio);
 
 	return (zio);
@@ -4473,7 +4741,8 @@ zio_dva_throttle_done(zio_t *zio)
 	 * throttle needs to know the allocating parent zio so we must find
 	 * it here.
 	 */
-	if (pio->io_child_type == ZIO_CHILD_GANG) {
+	if (pio->io_child_type == ZIO_CHILD_GANG)
+	{
 		/*
 		 * If our parent is a rewrite gang child then our grandparent
 		 * would have been the one that performed the allocation.
@@ -4492,11 +4761,11 @@ zio_dva_throttle_done(zio_t *zio)
 
 	mutex_enter(&pio->io_lock);
 	metaslab_group_alloc_decrement(zio->io_spa, vd->vdev_id, pio, flags,
-	    pio->io_allocator, B_TRUE);
+								   pio->io_allocator, B_TRUE);
 	mutex_exit(&pio->io_lock);
 
 	metaslab_class_throttle_unreserve(zio->io_metaslab_class, 1,
-	    pio->io_allocator, pio);
+									  pio->io_allocator, pio);
 
 	/*
 	 * Call into the pipeline to see if there is more work that
@@ -4509,6 +4778,7 @@ zio_dva_throttle_done(zio_t *zio)
 static zio_t *
 zio_done(zio_t *zio)
 {
+	// zfs_dbgmsg("zio_done() called");
 	/*
 	 * Always attempt to keep stack usage minimal here since
 	 * we can be called recursively up to 19 levels deep.
@@ -4521,10 +4791,15 @@ zio_done(zio_t *zio)
 	 * If our children haven't all completed,
 	 * wait for them and then repeat this pipeline stage.
 	 */
-	if (zio_wait_for_children(zio, ZIO_CHILD_ALL_BITS, ZIO_WAIT_DONE)) {
+	// zfs_dbgmsg("Waiting for children");
+	if (zio_wait_for_children(zio, ZIO_CHILD_ALL_BITS, ZIO_WAIT_DONE))
+	{
+		// zfs_dbgmsg("wait for children error");
 		return (NULL);
 	}
 
+	// zfs_dbgmsg("wait for children successful");
+
 	/*
 	 * If the allocation throttle is enabled, then update the accounting.
 	 * We only track child I/Os that are part of an allocating async
@@ -4532,7 +4807,8 @@ zio_done(zio_t *zio)
 	 * by the logical I/O but the actual write is done by child I/Os.
 	 */
 	if (zio->io_flags & ZIO_FLAG_IO_ALLOCATING &&
-	    zio->io_child_type == ZIO_CHILD_VDEV) {
+		zio->io_child_type == ZIO_CHILD_VDEV)
+	{
 		ASSERT(zio->io_metaslab_class != NULL);
 		ASSERT(zio->io_metaslab_class->mc_alloc_throttle_enabled);
 		zio_dva_throttle_done(zio);
@@ -4542,36 +4818,37 @@ zio_done(zio_t *zio)
 	 * If the allocation throttle is enabled, verify that
 	 * we have decremented the refcounts for every I/O that was throttled.
 	 */
-	if (zio->io_flags & ZIO_FLAG_IO_ALLOCATING) {
+	if (zio->io_flags & ZIO_FLAG_IO_ALLOCATING)
+	{
 		ASSERT(zio->io_type == ZIO_TYPE_WRITE);
 		ASSERT(zio->io_priority == ZIO_PRIORITY_ASYNC_WRITE);
 		ASSERT(zio->io_bp != NULL);
 
 		metaslab_group_alloc_verify(zio->io_spa, zio->io_bp, zio,
-		    zio->io_allocator);
-		VERIFY(zfs_refcount_not_held(&zio->io_metaslab_class->
-		    mc_allocator[zio->io_allocator].mca_alloc_slots, zio));
+									zio->io_allocator);
+		VERIFY(zfs_refcount_not_held(&zio->io_metaslab_class->mc_allocator[zio->io_allocator].mca_alloc_slots, zio));
 	}
 
-
 	for (int c = 0; c < ZIO_CHILD_TYPES; c++)
 		for (int w = 0; w < ZIO_WAIT_TYPES; w++)
 			ASSERT(zio->io_children[c][w] == 0);
 
-	if (zio->io_bp != NULL && !BP_IS_EMBEDDED(zio->io_bp)) {
+	if (zio->io_bp != NULL && !BP_IS_EMBEDDED(zio->io_bp))
+	{
 		ASSERT(zio->io_bp->blk_pad[0] == 0);
 		ASSERT(zio->io_bp->blk_pad[1] == 0);
 		ASSERT(bcmp(zio->io_bp, &zio->io_bp_copy,
-		    sizeof (blkptr_t)) == 0 ||
-		    (zio->io_bp == zio_unique_parent(zio)->io_bp));
+					sizeof(blkptr_t)) == 0 ||
+			   (zio->io_bp == zio_unique_parent(zio)->io_bp));
 		if (zio->io_type == ZIO_TYPE_WRITE && !BP_IS_HOLE(zio->io_bp) &&
-		    zio->io_bp_override == NULL &&
-		    !(zio->io_flags & ZIO_FLAG_IO_REPAIR)) {
+			zio->io_bp_override == NULL &&
+			!(zio->io_flags & ZIO_FLAG_IO_REPAIR))
+		{
 			ASSERT3U(zio->io_prop.zp_copies, <=,
-			    BP_GET_NDVAS(zio->io_bp));
+					 BP_GET_NDVAS(zio->io_bp));
 			ASSERT(BP_COUNT_GANG(zio->io_bp) == 0 ||
-			    (BP_COUNT_GANG(zio->io_bp) ==
-			    BP_GET_NDVAS(zio->io_bp)));
+				   (BP_COUNT_GANG(zio->io_bp) ==
+					BP_GET_NDVAS(zio->io_bp)));
 		}
 		if (zio->io_flags & ZIO_FLAG_NOPWRITE)
 			VERIFY(BP_EQUAL(zio->io_bp, &zio->io_bp_orig));
@@ -4588,14 +4865,17 @@ zio_done(zio_t *zio)
 	 * If the I/O on the transformed data was successful, generate any
 	 * checksum reports now while we still have the transformed data.
 	 */
-	if (zio->io_error == 0) {
-		while (zio->io_cksum_report != NULL) {
+	if (zio->io_error == 0)
+	{
+		while (zio->io_cksum_report != NULL)
+		{
 			zio_cksum_report_t *zcr = zio->io_cksum_report;
 			uint64_t align = zcr->zcr_align;
 			uint64_t asize = P2ROUNDUP(psize, align);
 			abd_t *adata = zio->io_abd;
 
-			if (adata != NULL && asize != psize) {
+			if (adata != NULL && asize != psize)
+			{
 				adata = abd_alloc(asize, B_TRUE);
 				abd_copy(adata, zio->io_abd, psize);
 				abd_zero_off(adata, psize, asize - psize);
@@ -4611,7 +4891,7 @@ zio_done(zio_t *zio)
 		}
 	}
 
-	zio_pop_transforms(zio);	/* note: may set zio->io_error */
+	zio_pop_transforms(zio); /* note: may set zio->io_error */
 
 	vdev_stat_update(zio, psize);
 
@@ -4620,8 +4900,10 @@ zio_done(zio_t *zio)
 	 * 30 seconds to complete, post an error described the I/O delay.
 	 * We ignore these errors if the device is currently unavailable.
 	 */
-	if (zio->io_delay >= MSEC2NSEC(zio_slow_io_ms)) {
-		if (zio->io_vd != NULL && !vdev_is_dead(zio->io_vd)) {
+	if (zio->io_delay >= MSEC2NSEC(zio_slow_io_ms))
+	{
+		if (zio->io_vd != NULL && !vdev_is_dead(zio->io_vd))
+		{
 			/*
 			 * We want to only increment our slow IO counters if
 			 * the IO is valid (i.e. not if the drive is removed).
@@ -4632,19 +4914,21 @@ zio_done(zio_t *zio)
 			 * of it.
 			 */
 			if (zfs_ereport_is_valid(FM_EREPORT_ZFS_DELAY,
-			    zio->io_spa, zio->io_vd, zio)) {
+									 zio->io_spa, zio->io_vd, zio))
+			{
 				mutex_enter(&zio->io_vd->vdev_stat_lock);
 				zio->io_vd->vdev_stat.vs_slow_ios++;
 				mutex_exit(&zio->io_vd->vdev_stat_lock);
 
-				(void) zfs_ereport_post(FM_EREPORT_ZFS_DELAY,
-				    zio->io_spa, zio->io_vd, &zio->io_bookmark,
-				    zio, 0);
+				(void)zfs_ereport_post(FM_EREPORT_ZFS_DELAY,
+									   zio->io_spa, zio->io_vd, &zio->io_bookmark,
+									   zio, 0);
 			}
 		}
 	}
 
-	if (zio->io_error) {
+	if (zio->io_error)
+	{
 		/*
 		 * If this I/O is attached to a particular vdev,
 		 * generate an error message describing the I/O failure
@@ -4652,10 +4936,12 @@ zio_done(zio_t *zio)
 		 * device is currently unavailable.
 		 */
 		if (zio->io_error != ECKSUM && zio->io_vd != NULL &&
-		    !vdev_is_dead(zio->io_vd)) {
+			!vdev_is_dead(zio->io_vd))
+		{
 			int ret = zfs_ereport_post(FM_EREPORT_ZFS_IO,
-			    zio->io_spa, zio->io_vd, &zio->io_bookmark, zio, 0);
-			if (ret != EALREADY) {
+									   zio->io_spa, zio->io_vd, &zio->io_bookmark, zio, 0);
+			if (ret != EALREADY)
+			{
 				mutex_enter(&zio->io_vd->vdev_stat_lock);
 				if (zio->io_type == ZIO_TYPE_READ)
 					zio->io_vd->vdev_stat.vs_read_errors++;
@@ -4666,19 +4952,21 @@ zio_done(zio_t *zio)
 		}
 
 		if ((zio->io_error == EIO || !(zio->io_flags &
-		    (ZIO_FLAG_SPECULATIVE | ZIO_FLAG_DONT_PROPAGATE))) &&
-		    zio == zio->io_logical) {
+									   (ZIO_FLAG_SPECULATIVE | ZIO_FLAG_DONT_PROPAGATE))) &&
+			zio == zio->io_logical)
+		{
 			/*
 			 * For logical I/O requests, tell the SPA to log the
 			 * error and generate a logical data ereport.
 			 */
 			spa_log_error(zio->io_spa, &zio->io_bookmark);
-			(void) zfs_ereport_post(FM_EREPORT_ZFS_DATA,
-			    zio->io_spa, NULL, &zio->io_bookmark, zio, 0);
+			(void)zfs_ereport_post(FM_EREPORT_ZFS_DATA,
+								   zio->io_spa, NULL, &zio->io_bookmark, zio, 0);
 		}
 	}
 
-	if (zio->io_error && zio == zio->io_logical) {
+	if (zio->io_error && zio == zio->io_logical)
+	{
 		/*
 		 * Determine whether zio should be reexecuted.  This will
 		 * propagate all the way to the root via zio_notify_parent().
@@ -4687,7 +4975,8 @@ zio_done(zio_t *zio)
 		ASSERT(zio->io_child_type == ZIO_CHILD_LOGICAL);
 
 		if (IO_IS_ALLOCATING(zio) &&
-		    !(zio->io_flags & ZIO_FLAG_CANFAIL)) {
+			!(zio->io_flags & ZIO_FLAG_CANFAIL))
+		{
 			if (zio->io_error != ENOSPC)
 				zio->io_reexecute |= ZIO_REEXECUTE_NOW;
 			else
@@ -4695,11 +4984,11 @@ zio_done(zio_t *zio)
 		}
 
 		if ((zio->io_type == ZIO_TYPE_READ ||
-		    zio->io_type == ZIO_TYPE_FREE) &&
-		    !(zio->io_flags & ZIO_FLAG_SCAN_THREAD) &&
-		    zio->io_error == ENXIO &&
-		    spa_load_state(zio->io_spa) == SPA_LOAD_NONE &&
-		    spa_get_failmode(zio->io_spa) != ZIO_FAILURE_MODE_CONTINUE)
+			 zio->io_type == ZIO_TYPE_FREE) &&
+			!(zio->io_flags & ZIO_FLAG_SCAN_THREAD) &&
+			zio->io_error == ENXIO &&
+			spa_load_state(zio->io_spa) == SPA_LOAD_NONE &&
+			spa_get_failmode(zio->io_spa) != ZIO_FAILURE_MODE_CONTINUE)
 			zio->io_reexecute |= ZIO_REEXECUTE_SUSPEND;
 
 		if (!(zio->io_flags & ZIO_FLAG_CANFAIL) && !zio->io_reexecute)
@@ -4723,8 +5012,8 @@ zio_done(zio_t *zio)
 	zio_inherit_child_errors(zio, ZIO_CHILD_LOGICAL);
 
 	if ((zio->io_error || zio->io_reexecute) &&
-	    IO_IS_ALLOCATING(zio) && zio->io_gang_leader == zio &&
-	    !(zio->io_flags & (ZIO_FLAG_IO_REWRITE | ZIO_FLAG_NOPWRITE)))
+		IO_IS_ALLOCATING(zio) && zio->io_gang_leader == zio &&
+		!(zio->io_flags & (ZIO_FLAG_IO_REWRITE | ZIO_FLAG_NOPWRITE)))
 		zio_dva_unallocate(zio, zio->io_gang_tree, zio->io_bp);
 
 	zio_gang_tree_free(&zio->io_gang_tree);
@@ -4733,10 +5022,11 @@ zio_done(zio_t *zio)
 	 * Godfather I/Os should never suspend.
 	 */
 	if ((zio->io_flags & ZIO_FLAG_GODFATHER) &&
-	    (zio->io_reexecute & ZIO_REEXECUTE_SUSPEND))
+		(zio->io_reexecute & ZIO_REEXECUTE_SUSPEND))
 		zio->io_reexecute &= ~ZIO_REEXECUTE_SUSPEND;
 
-	if (zio->io_reexecute) {
+	if (zio->io_reexecute)
+	{
 		/*
 		 * This is a logical I/O that wants to reexecute.
 		 *
@@ -4767,23 +5057,26 @@ zio_done(zio_t *zio)
 		 */
 		zl = NULL;
 		for (pio = zio_walk_parents(zio, &zl); pio != NULL;
-		    pio = pio_next) {
+			 pio = pio_next)
+		{
 			zio_link_t *remove_zl = zl;
 			pio_next = zio_walk_parents(zio, &zl);
 
 			if ((pio->io_flags & ZIO_FLAG_GODFATHER) &&
-			    (zio->io_reexecute & ZIO_REEXECUTE_SUSPEND)) {
+				(zio->io_reexecute & ZIO_REEXECUTE_SUSPEND))
+			{
 				zio_remove_child(pio, zio, remove_zl);
 				/*
 				 * This is a rare code path, so we don't
 				 * bother with "next_to_execute".
 				 */
 				zio_notify_parent(pio, zio, ZIO_WAIT_DONE,
-				    NULL);
+								  NULL);
 			}
 		}
 
-		if ((pio = zio_unique_parent(zio)) != NULL) {
+		if ((pio = zio_unique_parent(zio)) != NULL)
+		{
 			/*
 			 * We're not a root i/o, so there's nothing to do
 			 * but notify our parent.  Don't propagate errors
@@ -4795,22 +5088,28 @@ zio_done(zio_t *zio)
 			 * This is a rare code path, so we don't bother with
 			 * "next_to_execute".
 			 */
+			zfs_dbgmsg("Notifying parent that we are done");
 			zio_notify_parent(pio, zio, ZIO_WAIT_DONE, NULL);
-		} else if (zio->io_reexecute & ZIO_REEXECUTE_SUSPEND) {
+		}
+		else if (zio->io_reexecute & ZIO_REEXECUTE_SUSPEND)
+		{
 			/*
 			 * We'd fail again if we reexecuted now, so suspend
 			 * until conditions improve (e.g. device comes online).
 			 */
-			zio_suspend(zio->io_spa, zio, ZIO_SUSPEND_IOERR);
-		} else {
+			zfs_dbgmsg("Suspending spa and zio");
+			// zio_suspend(zio->io_spa, zio, ZIO_SUSPEND_IOERR);
+		}
+		else
+		{
 			/*
 			 * Reexecution is potentially a huge amount of work.
 			 * Hand it off to the otherwise-unused claim taskq.
 			 */
 			ASSERT(taskq_empty_ent(&zio->io_tqent));
 			spa_taskq_dispatch_ent(zio->io_spa,
-			    ZIO_TYPE_CLAIM, ZIO_TASKQ_ISSUE,
-			    zio_reexecute, zio, 0, &zio->io_tqent);
+								   ZIO_TYPE_CLAIM, ZIO_TASKQ_ISSUE,
+								   zio_reexecute, zio, 0, &zio->io_tqent);
 		}
 		return (NULL);
 	}
@@ -4822,7 +5121,8 @@ zio_done(zio_t *zio)
 	/*
 	 * Report any checksum errors, since the I/O is complete.
 	 */
-	while (zio->io_cksum_report != NULL) {
+	while (zio->io_cksum_report != NULL)
+	{
 		zio_cksum_report_t *zcr = zio->io_cksum_report;
 		zio->io_cksum_report = zcr->zcr_next;
 		zcr->zcr_next = NULL;
@@ -4831,8 +5131,9 @@ zio_done(zio_t *zio)
 	}
 
 	if (zio->io_flags & ZIO_FLAG_FASTWRITE && zio->io_bp &&
-	    !BP_IS_HOLE(zio->io_bp) && !BP_IS_EMBEDDED(zio->io_bp) &&
-	    !(zio->io_flags & ZIO_FLAG_NOPWRITE)) {
+		!BP_IS_HOLE(zio->io_bp) && !BP_IS_EMBEDDED(zio->io_bp) &&
+		!(zio->io_flags & ZIO_FLAG_NOPWRITE))
+	{
 		metaslab_fastwrite_unmark(zio->io_spa, zio->io_bp);
 	}
 
@@ -4854,19 +5155,23 @@ zio_done(zio_t *zio)
 	 */
 	zio_t *next_to_execute = NULL;
 	zl = NULL;
-	for (pio = zio_walk_parents(zio, &zl); pio != NULL; pio = pio_next) {
+	for (pio = zio_walk_parents(zio, &zl); pio != NULL; pio = pio_next)
+	{
 		zio_link_t *remove_zl = zl;
 		pio_next = zio_walk_parents(zio, &zl);
 		zio_remove_child(pio, zio, remove_zl);
 		zio_notify_parent(pio, zio, ZIO_WAIT_DONE, &next_to_execute);
 	}
 
-	if (zio->io_waiter != NULL) {
+	if (zio->io_waiter != NULL)
+	{
 		mutex_enter(&zio->io_lock);
 		zio->io_executor = NULL;
 		cv_broadcast(&zio->io_cv);
 		mutex_exit(&zio->io_lock);
-	} else {
+	}
+	else
+	{
 		zio_destroy(zio);
 	}
 
@@ -4880,35 +5185,32 @@ zio_done(zio_t *zio)
  */
 static zio_pipe_stage_t *zio_pipeline[] = {
 	NULL,
-	zio_read_bp_init,
-	zio_write_bp_init,
-	zio_free_bp_init,
-	zio_issue_async,
-	zio_write_compress,
-	zio_encrypt,
-	zio_checksum_generate,
-	zio_nop_write,
-	zio_ddt_read_start,
-	zio_ddt_read_done,
-	zio_ddt_write,
-	zio_ddt_free,
-	zio_gang_assemble,
-	zio_gang_issue,
-	zio_dva_throttle,
-	zio_dva_allocate,
-	zio_dva_free,
-	zio_dva_claim,
-	zio_ready,
-	zio_vdev_io_start,
-	zio_vdev_io_done,
-	zio_vdev_io_assess,
-	zio_checksum_verify,
-	zio_done
+	zio_read_bp_init,   // 1
+	zio_write_bp_init,  // 2
+	zio_free_bp_init,   // 3
+	zio_issue_async,    // 4
+	zio_write_compress, // 5
+	zio_encrypt,        // 6
+	zio_checksum_generate, // 7
+	zio_nop_write,      // 8
+	zio_ddt_read_start, // 9
+	zio_ddt_read_done, // 10
+	zio_ddt_write,     // 11
+	zio_ddt_free,      // 12
+	zio_gang_assemble, // 13
+	zio_gang_issue,    // 14
+	zio_dva_throttle,  // 15
+	zio_dva_allocate,  // 16
+	zio_dva_free,      // 17
+	zio_dva_claim,     // 18
+	zio_ready,		   // 19
+	zio_vdev_io_start, // 20
+	zio_vdev_io_done,  // 21
+	zio_vdev_io_assess, // 22
+	zio_checksum_verify, // 23
+	zio_done // 24
 };
 
-
-
-
 /*
  * Compare two zbookmark_phys_t's to see which we would reach first in a
  * pre-order traversal of the object tree.
@@ -4932,9 +5234,8 @@ static zio_pipe_stage_t *zio_pipeline[] = {
  * equivalent, compare appropriately to bookmarks in other objects, and to
  * compare appropriately to other bookmarks in the meta-dnode.
  */
-int
-zbookmark_compare(uint16_t dbss1, uint8_t ibs1, uint16_t dbss2, uint8_t ibs2,
-    const zbookmark_phys_t *zb1, const zbookmark_phys_t *zb2)
+int zbookmark_compare(uint16_t dbss1, uint8_t ibs1, uint16_t dbss2, uint8_t ibs2,
+					  const zbookmark_phys_t *zb1, const zbookmark_phys_t *zb2)
 {
 	/*
 	 * These variables represent the "equivalent" values for the zbookmark,
@@ -4946,8 +5247,8 @@ zbookmark_compare(uint16_t dbss1, uint8_t ibs1, uint16_t dbss2, uint8_t ibs2,
 	uint64_t zb1level, zb2level;
 
 	if (zb1->zb_object == zb2->zb_object &&
-	    zb1->zb_level == zb2->zb_level &&
-	    zb1->zb_blkid == zb2->zb_blkid)
+		zb1->zb_level == zb2->zb_level &&
+		zb1->zb_blkid == zb2->zb_blkid)
 		return (0);
 
 	IMPLY(zb1->zb_level > 0, ibs1 >= SPA_MINBLOCKSHIFT);
@@ -4959,20 +5260,26 @@ zbookmark_compare(uint16_t dbss1, uint8_t ibs1, uint16_t dbss2, uint8_t ibs2,
 	zb1L0 = (zb1->zb_blkid) * BP_SPANB(ibs1, zb1->zb_level);
 	zb2L0 = (zb2->zb_blkid) * BP_SPANB(ibs2, zb2->zb_level);
 
-	if (zb1->zb_object == DMU_META_DNODE_OBJECT) {
+	if (zb1->zb_object == DMU_META_DNODE_OBJECT)
+	{
 		zb1obj = zb1L0 * (dbss1 << (SPA_MINBLOCKSHIFT - DNODE_SHIFT));
 		zb1L0 = 0;
 		zb1level = zb1->zb_level + COMPARE_META_LEVEL;
-	} else {
+	}
+	else
+	{
 		zb1obj = zb1->zb_object;
 		zb1level = zb1->zb_level;
 	}
 
-	if (zb2->zb_object == DMU_META_DNODE_OBJECT) {
+	if (zb2->zb_object == DMU_META_DNODE_OBJECT)
+	{
 		zb2obj = zb2L0 * (dbss2 << (SPA_MINBLOCKSHIFT - DNODE_SHIFT));
 		zb2L0 = 0;
 		zb2level = zb2->zb_level + COMPARE_META_LEVEL;
-	} else {
+	}
+	else
+	{
 		zb2obj = zb2->zb_object;
 		zb2level = zb2->zb_level;
 	}
@@ -5004,7 +5311,7 @@ zbookmark_compare(uint16_t dbss1, uint8_t ibs1, uint16_t dbss2, uint8_t ibs2,
  */
 boolean_t
 zbookmark_subtree_completed(const dnode_phys_t *dnp,
-    const zbookmark_phys_t *subtree_root, const zbookmark_phys_t *last_block)
+							const zbookmark_phys_t *subtree_root, const zbookmark_phys_t *last_block)
 {
 	zbookmark_phys_t mod_zb = *subtree_root;
 	mod_zb.zb_blkid++;
@@ -5030,8 +5337,8 @@ zbookmark_subtree_completed(const dnode_phys_t *dnp,
 	 * to make sure that this code still works afterwards.
 	 */
 	return (zbookmark_compare(dnp->dn_datablkszsec, dnp->dn_indblkshift,
-	    1ULL << (DNODE_BLOCK_SHIFT - SPA_MINBLOCKSHIFT), 0, &mod_zb,
-	    last_block) <= 0);
+							  1ULL << (DNODE_BLOCK_SHIFT - SPA_MINBLOCKSHIFT), 0, &mod_zb,
+							  last_block) <= 0);
 }
 
 /*
@@ -5040,14 +5347,14 @@ zbookmark_subtree_completed(const dnode_phys_t *dnp,
  */
 boolean_t
 zbookmark_subtree_tbd(const dnode_phys_t *dnp,
-    const zbookmark_phys_t *subtree_root, const zbookmark_phys_t *last_block)
+					  const zbookmark_phys_t *subtree_root, const zbookmark_phys_t *last_block)
 {
 	ASSERT0(last_block->zb_level);
 	if (dnp == NULL)
 		return (B_FALSE);
 	return (zbookmark_compare(dnp->dn_datablkszsec, dnp->dn_indblkshift,
-	    1ULL << (DNODE_BLOCK_SHIFT - SPA_MINBLOCKSHIFT), 0, subtree_root,
-	    last_block) >= 0);
+							  1ULL << (DNODE_BLOCK_SHIFT - SPA_MINBLOCKSHIFT), 0, subtree_root,
+							  last_block) >= 0);
 }
 
 EXPORT_SYMBOL(zio_type_name);
@@ -5058,23 +5365,23 @@ EXPORT_SYMBOL(zio_data_buf_free);
 
 /* BEGIN CSTYLED */
 ZFS_MODULE_PARAM(zfs_zio, zio_, slow_io_ms, INT, ZMOD_RW,
-	"Max I/O completion time (milliseconds) before marking it as slow");
+				 "Max I/O completion time (milliseconds) before marking it as slow");
 
 ZFS_MODULE_PARAM(zfs_zio, zio_, requeue_io_start_cut_in_line, INT, ZMOD_RW,
-	"Prioritize requeued I/O");
+				 "Prioritize requeued I/O");
 
-ZFS_MODULE_PARAM(zfs, zfs_, sync_pass_deferred_free,  INT, ZMOD_RW,
-	"Defer frees starting in this pass");
+ZFS_MODULE_PARAM(zfs, zfs_, sync_pass_deferred_free, INT, ZMOD_RW,
+				 "Defer frees starting in this pass");
 
 ZFS_MODULE_PARAM(zfs, zfs_, sync_pass_dont_compress, INT, ZMOD_RW,
-	"Don't compress starting in this pass");
+				 "Don't compress starting in this pass");
 
 ZFS_MODULE_PARAM(zfs, zfs_, sync_pass_rewrite, INT, ZMOD_RW,
-	"Rewrite new bps starting in this pass");
+				 "Rewrite new bps starting in this pass");
 
 ZFS_MODULE_PARAM(zfs_zio, zio_, dva_throttle_enabled, INT, ZMOD_RW,
-	"Throttle block allocations in the ZIO pipeline");
+				 "Throttle block allocations in the ZIO pipeline");
 
 ZFS_MODULE_PARAM(zfs_zio, zio_, deadman_log_all, INT, ZMOD_RW,
-	"Log all slow ZIOs, not just those with vdevs");
+				 "Log all slow ZIOs, not just those with vdevs");
 /* END CSTYLED */
diff --git a/notes b/notes
new file mode 100644
index 000000000..ba8779588
--- /dev/null
+++ b/notes
@@ -0,0 +1,71 @@
+/* Notes for workflow of zfs. */
+
+main()                              /* cmd/zpool/zpool_main.c */
+zpool_do_attach_or_replace()        /* cmd/zpool/zpool_main.c */
+zpool_vdev_attach()                 /* lib/libzfs/libzfs_pool.c */
+zfs_ioctl()                         /* lib/libzfs/os/linux/libzfs_util_os.c */
+
+/* Switch to system call ioctl(), the reset will be printed in  
+    /proc/spl/kstat/zfs/dbgmsg via zfs_dbgmsg(). */
+
+zfs_ioc_vdev_attach()               /* module/zfs/zfs_ioctl.c */
+spa_vdev_attach()                   /* module/zfs/spa.c*/
+
+/* during rebuild, a new thread will be created for rebuild */
+
+
+/* where vdev_mirror_io_start start is called */
+vdev_op_io_start()      /* first mapped to this function*/
+zio_vdev_io_start()     /* Start the io to disks*/
+/* together, the form a list zio_pipeline. The function are then called 
+in __zio_execute()*/
+
+
+
+sturcts note:
+
+vdev_t    struct for a virtual device
+zio_t      io struct
+dmu_t      data management unit here 
+
+
+vdev_alloc()     in vdev.c , allocate a vdev struct, associate operations
+This is called by spa_config_parse() in spa.c. 
+
+vdev_open()      open the corresponding vdev 
+
+
+in attach, a new root vdev is created. 
+
+questions: What is the synchronization behind zio? (Need time to investigate)
+
+What connects zio_execute? Are there multiple threads open? 
+What to do next? 
+in spa.h : print rebuild
+check what's inside dsl_scan
+find whether zio_wait is being used. 
+
+
+mirror  call time: 2157
+zio_io_start : 1167
+vd = Null 438
+replacing 552 
+
+
+Questions: what happens when we have 3 leaf vdevs in a mirror? 
+
+
+
+Create workflow:
+
+zpool_do_create()     
+
+zpool_create()     This swithed to zfs kernel module
+
+zfs_ioc_pool_create()   
+
+spa_create()
+
+spa_config_parse()     Here the vdev tree is allocated recursively
+
+vdev_alloc()          We do individual allocation here
\ No newline at end of file
diff --git a/scrub_test.sh b/scrub_test.sh
new file mode 100755
index 000000000..7c5a7a3e0
--- /dev/null
+++ b/scrub_test.sh
@@ -0,0 +1,10 @@
+for i in {1..3}; do sudo truncate -s 4G /scratch/$i.img; done
+
+sudo zpool create zpool raidz /scratch/1.img /scratch/2.img /scratch/3.img -f
+sudo dd if=/dev/zero of=/scratch/2.img bs=4M count=1 2>/dev/null
+#sudo dd if=/dev/zero of=/scratch/3.img bs=4M count=1 2>/dev/null
+sudo zpool easyscrub zpool
+zpool status
+
+sudo cat /proc/spl/kstat/zfs/dbgmsg > log.txt
+sudo zpool destroy zpool 
diff --git a/test.md b/test.md
new file mode 100644
index 000000000..b78b23fe2
--- /dev/null
+++ b/test.md
@@ -0,0 +1,22 @@
+# Test command
+ 
+for i in {1..3}; do sudo truncate -s 2G /scratch/$i.img; done
+
+sudo zpool create zpool raidz1 /scratch/1.img /scratch/2.img /scratch/3.img
+sudo zpool create zpool mirror /scratch/1.img /scratch/2.img 
+
+
+# simulate disk failures
+sudo dd if=/dev/zero of=/scratch/2.img bs=4M count=1 2>/dev/null
+sudo zpool scrub zpool
+sudo truncate -s 2G /scratch/new.img
+sudo zpool replace zpool /scratch/2.img /scratch/new.img
+
+# change ownership 
+sudo chown cc:cc /zpool/data
+
+# check the log
+sudo cat /proc/spl/kstat/zfs/dbgmsg
+
+# delete the zpool
+sudo zpool destroy zpool
\ No newline at end of file
diff --git a/test_catastrophic_failure.sh b/test_catastrophic_failure.sh
new file mode 100755
index 000000000..597ddf7c3
--- /dev/null
+++ b/test_catastrophic_failure.sh
@@ -0,0 +1,34 @@
+#!/bin/bash
+# size=$1
+
+# sudo rm -rf /media/ramdisk*
+
+# for i in {1..4}; do
+#     sudo mkdir -p /media/ramdisk$i
+#     # sudo mount -t tmpfs -o size=${size} tmpfs /media/ramdisk$i/
+#     sudo truncate -s ${size} /media/ramdisk$i/disk.img
+
+#     # The following are for creating loopback for 
+#     # # We have already created the file image through truncate
+#     # # Partition the image file
+#     # sudo cfdisk /media/ramdisk$i/disk.img
+
+#     # # Create loop back (pad out the loop back index by 20 since the device might already have a few loop back
+#     # loopback_num=$(($i+20))
+#     # sudo losetup -P /dev/loop$loopback_num /media/ramdisk$i/disk.img
+
+#     # sudo mkfs.ext4 /dev/loop$loopback_num
+# done
+
+# Create the zpool
+sudo zfs set mountpoint=/mnt/zfs pool
+sudo chown -R aaronmao:aaronmao /mnt/zfs
+
+# Create the zfs test file, of 400MB
+for i in {1..6}; do
+    cd /mnt/zfs && sudo dd if=/dev/random of=/mnt/zfs/test$i bs=40M count=10
+done
+
+sudo dd if=/dev/zero of=/media/ramdisk1/disk.img bs=4M count=20 2>/dev/null
+sudo dd if=/dev/zero of=/media/ramdisk2/disk.img bs=4M count=20 2>/dev/null
+
diff --git a/zfs_full_install.sh b/zfs_full_install.sh
new file mode 100755
index 000000000..45ca21074
--- /dev/null
+++ b/zfs_full_install.sh
@@ -0,0 +1,13 @@
+# ligthweight script that install zfs
+
+sh autogen.sh
+ ./configure
+make -s -j$(nproc)
+
+sudo make install
+sudo ldconfig
+sudo depmod
+
+sudo ./scripts/zfs.sh 
+sudo systemctl enable zfs.target zfs-import.target \
+    zfs-mount.service zfs-import-cache.service zfs-import-scan.service
\ No newline at end of file
diff --git a/zfs_init.sh b/zfs_init.sh
new file mode 100755
index 000000000..3b70454a5
--- /dev/null
+++ b/zfs_init.sh
@@ -0,0 +1,12 @@
+# Script the initialize zfs
+
+#------------------------Set up environment and compile-------------------------
+sudo apt-get update
+sudo apt install build-essential autoconf automake libtool gawk alien fakeroot \
+    dkms libblkid-dev uuid-dev libudev-dev libssl-dev zlib1g-dev libaio-dev \
+    libattr1-dev libelf-dev linux-headers-generic python3 python3-dev \
+    python3-setuptools python3-cffi libffi-dev python3-packaging git \
+    libcurl4-openssl-dev debhelper-compat dh-python po-debconf python3-all-dev \
+    python3-sphinx
+
+sudo apt-get install linux-image-generic
diff --git a/zfs_install.sh b/zfs_install.sh
new file mode 100755
index 000000000..a5d67335b
--- /dev/null
+++ b/zfs_install.sh
@@ -0,0 +1,13 @@
+# ligthweight script that install zfs
+
+#sh autogen.sh
+# ./configure
+make -s -j$(nproc)
+
+sudo make install
+sudo ldconfig
+sudo depmod
+
+sudo ./scripts/zfs.sh 
+sudo systemctl enable zfs.target zfs-import.target \
+    zfs-mount.service zfs-import-cache.service zfs-import-scan.service
\ No newline at end of file
diff --git a/zfs_reinstall.sh b/zfs_reinstall.sh
new file mode 100755
index 000000000..f9d39e7c9
--- /dev/null
+++ b/zfs_reinstall.sh
@@ -0,0 +1,51 @@
+#!/bin/bash
+
+size=$1
+
+# A simple shell script that uninstall zfs 
+sudo service zfs-zed stop
+# Check if the ZFS module is loaded
+if lsmod | grep -q "^zfs"; then
+    echo "ZFS module is loaded."
+    sudo zpool destroy -f pool
+    set -e
+    sudo modprobe -r zfs
+    # Perform actions when the ZFS module is loaded
+else
+    set -e
+    echo "ZFS module is not loaded."
+    # Perform actions when the ZFS module is not loaded
+fi
+sudo make uninstall
+sudo ldconfig
+sudo depmod
+sudo rm -rf /usr/local/etc/zfs
+sudo rm -rf /usr/lib/python3/dist-packages/libzfs_core
+
+make -s -j$(nproc)
+
+sudo make install
+sudo ldconfig
+sudo depmod
+
+sudo ./scripts/zfs.sh -r
+sudo systemctl enable zfs.target zfs-import.target \
+    zfs-mount.service zfs-import-cache.service zfs-import-scan.service
+sudo modprobe zfs
+sudo service zfs-zed start
+
+sudo rm -rf /media/ramdisk*
+
+for i in {1..4}; do
+    sudo mkdir -p /media/ramdisk$i
+    # sudo mount -t tmpfs -o size=${size} tmpfs /media/ramdisk$i/
+    sudo truncate -s ${size} /media/ramdisk$i/disk.img
+done
+
+sudo zpool create pool raidz /media/ramdisk1/disk.img /media/ramdisk2/disk.img /media/ramdisk3/disk.img
+sudo zfs set mountpoint=/data/dataNode1 pool
+sudo zfs mount
+sudo chown -R aaronmao:aaronmao /data/dataNode1
+dd if=/dev/urandom of=/data/dataNode1/newfile bs=2M count=1
+
+printf "=======\nAll commands ran successfully\n=======" 
\ No newline at end of file
diff --git a/zfs_test.sh b/zfs_test.sh
new file mode 100755
index 000000000..8d0d77501
--- /dev/null
+++ b/zfs_test.sh
@@ -0,0 +1,13 @@
+# Test script to see what's going on
+
+for i in {1..3}; do sudo truncate -s 4G /scratch/$i.img; done
+
+sudo zpool create test raidz /scratch/1.img /scratch/2.img /scratch/3.img 
+sudo dd if=/dev/zero of=/scratch/2.img bs=4M count=1 2>/dev/null
+sudo zpool scrub zpool
+sudo truncate -s 2G /scratch/new.img
+sudo zpool replace zpool /scratch/2.img /scratch/new.img
+
+sudo cat /proc/spl/kstat/zfs/dbgmsg > log.txt
+
+sudo zpool destroy zpool
\ No newline at end of file
diff --git a/zfs_uninstall.sh b/zfs_uninstall.sh
new file mode 100755
index 000000000..12fc8860e
--- /dev/null
+++ b/zfs_uninstall.sh
@@ -0,0 +1,8 @@
+# A simple shell script that uninstall zfs 
+sudo service zed stop
+sudo modprobe -r zfs
+sudo make uninstall
+sudo ldconfig
+sudo depmod
+sudo rm -rf /usr/local/etc/zfs
+sudo rm -rf /usr/lib/python3/dist-packages/libzfs_core
\ No newline at end of file
